\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

%\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
 \usepackage[final]{nips_2017}

%\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% For citations
\usepackage{natbib}
\usepackage{amsthm}
\usepackage{makecell}
% For algorithms
%\usepackage[algcompatible]{algpseudocode}
%\usepackage{algorithm}
%\usepackage{algorithmicx}
%\usepackage[noend]{algcompatible}
\usepackage[vlined,linesnumbered,ruled]{algorithm2e}
\usepackage{bbm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsopn}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{soul}
\usepackage{commath}
%%%%%%%%%%%%%%%%%% toc
\usepackage{scrwfile}
\TOCclone[\contentsname~(\appendixname)]{toc}{atoc}
\newcommand\StartAppendixEntries{}
\AfterTOCHead[toc]{%
  \renewcommand\StartAppendixEntries{\value{tocdepth}=-10000\relax}%
}
\AfterTOCHead[atoc]{%
  \edef\maintocdepth{\the\value{tocdepth}}%
  \value{tocdepth}=-10000\relax%
  \renewcommand\StartAppendixEntries{\value{tocdepth}=\maintocdepth\relax}%
}
\newcommand*\appendixwithtoc{%
  \cleardoublepage
  \appendix
  \addtocontents{toc}{\protect\StartAppendixEntries}
  \listofatoc
}
\usepackage{blindtext}
\usepackage{color}
%%%%%%%%%%%%%%%%%% toc

%%%%%%%%%%%%%%%% HEADER
\newcommand{\EG}{\textsc{Epoch-Greedy}\xspace}
\newcommand{\EPG}{\textsc{$\epsilon$-Greedy}\xspace}
\newcommand{\minimonster}{\textsc{ILOVETOCONBANDITS}\xspace}
\newcommand{\ILTCB}{\textsc{ILTCB}\xspace}
\newcommand{\AdaEG}{\textsc{Ada-Greedy}\xspace}
\newcommand{\AdaILTCB}{\textsc{Ada-ILTCB}\xspace}
\newcommand{\AdaPE}{\textsc{Ada-PE}\xspace}
\newcommand{\corral}{\textsc{Corral}\xspace}
\newcommand{\bistro}{\textsc{BISTRO+}\xspace}
\newcommand{\base}[1]{{{\cal{B}}_{#1}}}
\newcommand{\scale}{\rho}
\newcommand{\FS}{\text{FS}}

\newcommand{\calA}{{\mathcal{A}}}
\newcommand{\calX}{{\mathcal{X}}}
\newcommand{\calS}{{\mathcal{S}}}
\newcommand{\calI}{{\mathcal{I}}}
\newcommand{\calJ}{{\mathcal{J}}}
\newcommand{\calK}{{\mathcal{K}}}
\newcommand{\calD}{{\mathcal{D}}}
\newcommand{\calE}{{\mathcal{E}}}
\newcommand{\calR}{{\mathcal{R}}}
\newcommand{\calT}{{\mathcal{T}}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\avgR}{\wh{\cal{R}}}
\newcommand{\calW}{{\mathcal{W}}}
\newcommand{\ips}{\wh{r}}
\newcommand{\whpi}{\wh{\pi}}
\newcommand{\whE}{\wh{\E}}
\newcommand{\whV}{\wh{V}}
\newcommand{\Reg}{\text{\rm Reg}}
\newcommand{\whReg}{\wh{\text{\rm Reg}}}
\newcommand{\flg}{\text{\rm flag}}
\newcommand{\one}{\boldsymbol{1}}
\newcommand{\var}{\Delta}
\newcommand{\p}{\prime}
\newcommand{\nb}{\nabla}
\newcommand{\e}{\mathbf{e}}

\DeclareMathOperator*{\sep}{sep}
\DeclareMathOperator*{\arginf}{arginf}
\DeclareMathOperator*{\argsup}{argsup}
\DeclareMathOperator*{\range}{range}
\DeclareMathOperator*{\mydet}{det_{+}}
%\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\bigabs{\big\lvert}{\big\rvert}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\bigceil{\big\lceil}{\big\rceil}
\DeclarePairedDelimiter\bigfloor{\big\lfloor}{\big\rfloor}

\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\fY}{\field{Y}}
\newcommand{\fX}{\field{X}}
\newcommand{\fH}{\field{H}}
\newcommand{\fR}{\field{R}}
\newcommand{\fN}{\field{N}}
\newcommand{\E}{\field{E}}
\newcommand{\err}{\text{err}}

\newcommand{\theset}[2]{ \left\{ {#1} \,:\, {#2} \right\} }
\newcommand{\inner}[1]{ \left\langle {#1} \right\rangle }
\newcommand{\Ind}[1]{ \field{I}_{\{{#1}\}} }
\newcommand{\eye}[1]{ \boldsymbol{I}_{#1} }
%\newcommand{\norm}[1]{\left\|{#1}\right\|}
%\newcommand{\trace}[1]{\text{tr}\left({#1}\right)}
\newcommand{\trace}[1]{\textsc{tr}({#1})}
\newcommand{\diag}[1]{\mathrm{diag}\!\left\{{#1}\right\}}

\newcommand{\defeq}{\stackrel{\rm def}{=}}
\newcommand{\sgn}{\mbox{\sc sgn}}
\newcommand{\scI}{\mathcal{I}}
\newcommand{\scO}{\mathcal{O}}
\newcommand{\scN}{\mathcal{N}}

\newcommand{\dt}{\displaystyle}
\renewcommand{\ss}{\subseteq}
\newcommand{\wh}{\widehat}
\newcommand{\wt}{\widetilde}
\newcommand{\ve}{\varepsilon}
\newcommand{\hlambda}{\wh{\lambda}}
\newcommand{\yhat}{\wh{y}}

\newcommand{\hDelta}{\wh{\Delta}}
\newcommand{\hdelta}{\wh{\delta}}
\newcommand{\spin}{\{-1,+1\}}
%\newcommand{\calS}{\mathcal{S}}
\newcommand{\calC}{\mathcal{C}}

\newcommand{\cz}[1]{\textcolor{blue}{Chicheng: #1}}

%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{assumption}{Assumption}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{fact}[theorem]{Fact}
\DeclareMathOperator{\val}{val}
\DeclareMathOperator{\spa}{sp}
\DeclareMathOperator{\solve}{solve}
\allowdisplaybreaks

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\title{Bandit Multiclass Classification}

%\author{}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\section{Preliminaries}
\begin{definition}
$S \subset (\calX \times [K])^*$ is said to be {\em linearly separable} with margin parameter $\gamma > 0$, if there exists an multiclass linear classifier $W \in \fR^{K \times d}$ ($\| W \|_F \leq 1$),
such that for every $(x,y)$ in $S$,
$W_y \cdot x \geq W_{y'} \cdot x + \gamma \max_{(x,y) \in S} \| x \|_2$ holds simultaneously for all $y' \neq y$.
\label{def:mc-margin}
\end{definition}

\begin{definition}
$S \subset (\calX \times [K])^*$ is said to be {\em one-versus-all (OvA) linearly separable} with margin parameter $\gamma > 0$, if there exists an multiclass linear classifier $W \in \fR^{k \times d}$ ($\| W \|_F \leq 1$),
such that for every $(x,y)$ in $S$,
$W_y \cdot x \geq \gamma \max_{(x,y) \in S} \| x \|_2 $, 
and 
$W_{y'} \cdot x \leq -\gamma \max_{(x,y) \in S} \| x \|_2$
holds simultaneously for all $y' \neq y$.
\label{def:mc-ova-margin}
\end{definition}

For every example $(x, y)$, denote by 
\[ b_y^i = \begin{cases} +1  & y_t = i \\ -1 & y_t \neq i \end{cases} \]
its one-versus-all label. The above definition is equivalent to: there exists a $W$ in $\fR^{K \times d}$ ($\|W\|_F \leq 1$),
for every $i$ in $[K]$,
for every $(x, y)$ in $S$,
\[ 
b_y^i W_i \cdot x \geq \gamma \max_{(x,y) \in S} \| x \|_2.
\]


\cz{The reason we are using this specific notion of linear separability is:
1. We need a direct comparison of $\gamma$'s before and after feature transformation, therefore a ``normalized'' notion of margin is used.  
2. Alternative notions such as $\frac{W_y \cdot x}{\| x \|_2} \geq \gamma$ requires normalizing each example, in order to get a $\frac{1}{\gamma^2}$ mistake bound. If kernel Perceptron is used, this extra normalization step is equivalent to using a new kernel $k'(x,x') = \frac{k(x,x')}{\sqrt{k(x,x) k(x',x')}}$ as opposed to using the original kernel $k$ - we would like to avoid this techinicality for ease of presentation.}

\section{Main algorithm}
Our key observation is that, we can reduce the problem of online bandit linear multiclass learning under linear separability to the same problem under OvA linear separability. Specifically, inspired by~\cite{klivans2004learning}, we define
a feature map $\phi: \fR^d \to \fR^{d'}$, such that if the original dataset $S$ is linearly separable with margin $\gamma > 0$, then the dataset tranformed by $\phi$, $S' := \cbr{(\phi(x), y): (x,y) \in S}$ is OvA linearly separable with margin $\gamma' > 0$. Built on this feature map, we propose an efficient algorithm that has a finite mistake bound in the linearly separable setting. Its key idea is to 
run an algorithm that has a finite mistake bound under OvA linear separability, upon the transformed examples $(\phi(x), y)$. 
%a Perceptron-based algorithm that achieves a finite mistake bound under strong linear separablilty, and show how it can be combined with the feature map $\phi$ to get a
%online learning intersection of halfspaces.


\begin{algorithm}[H]
\caption{Main Algorithm}
\label{alg:ova}
\SetKwInOut{Input}{Input} 
\Input{Feature mapping $\phi$}
Initialize $w_t^{(1)}=\ldots=w_t^{(K)}=\mathbf{0}\in \mathbb{R}^{d'}$\\
\For{$t=1$ to $T$}{
   \If{$\exists y$ such that $w_t^{(y)\top} \phi(x_t) \geq 0$}{
       Assign $\tilde{y}_t$ to any $y$ with $w_t^{(y)\top}  \phi(x_t) \geq 0$ \\
       Predict $\tilde{y}_t$ \\
       \lIf{$\tilde{y}_t\neq y_t$}{ update $w_t^{(\tilde{y}_t)} \leftarrow w_t^{(\tilde{y}_t)} - \phi(x_t)$ } \label{line:update1}
   }
   \Else{
       Pick $\tilde{y}_t$ randomly from $
       \text{unif}([K])$ \label{line:explore}\\
       Predict $\tilde{y}_t$ \\
       \lIf{$\tilde{y}_t = y_t$}{ update $w_t^{(\tilde{y}_t)} \leftarrow w_t^{(\tilde{y}_t)} + \phi(x_t)$ } \label{line:update2}
   }
}
\end{algorithm}

We have the following guarantees of the above algorithm.
\begin{theorem}
Suppose feature map $\phi$ is such that for \cbr{(\phi(x_t), y_t): t \in [T]} is OvA linearly separable by margin $\gamma'$ except for $N$ points. Then, 
the expected cumulative number of mistakes made by Algorithm~\ref{alg:ova} is at most $K \cdot ( (1 + \frac 1 {\gamma'}) N + \sqrt{ (1 + \frac 1 {\gamma'}) N \cdot \frac{1}{\gamma'^2} } + \frac{1}{\gamma'^2})$.
\end{theorem}

\begin{corollary}
Suppose feature map $\phi$ is such that \cbr{(\phi(x_t), y_t): t \in [T]} is OvA linearly separable by margin $\gamma'$, then the cumulative number of mistakes made by Algorithm~\ref{alg:ova} is $\frac{K}{\gamma'^2}$.
\end{corollary}

\begin{proof}
Denote by $U_{t,1}$ (resp. $U_{t,2}$) the indicator in the ``if'' branch (resp. ``else'' branch) at time $t$. In addition, denote by $U_t^y$ the indicator of $w_t^{(y)}$ gets updated at 
round $t$. It can be easily seen that $U_{t,1} + U_{t,2} = \sum_{y=1}^k U_t^y = U_t^{\tilde{y}_t}$, which is equal to the indicator of Algorithm~\ref{alg:ova} making an update at iteration $t$. Let $N_{t,2}$ be the indicator of Algorithm~\ref{alg:ova} encounters the ``else'' branch at time $t$. Throughout, we also use the shorthand that $b_{t,i} = b_{y_t}^i$.



% for every $y$ in $[K]$, we have the following bound on $\sum_{t=1}^T U_t^y$: for all $w_y \in \fR^d$ and all $\rho > 0$,
%\begin{eqnarray*}
%\sum_{t=1}^T U_t^y 
%\leq
%\sum_{t=1}^T U_t^y (1 - \frac{b_{t,y} w_y \phi(x_t)}{\rho})_+ 
%+ \sqrt{(\sum_{t=1}^T U_t^y (1 - \frac{b_{t,y} w_y \phi(x_t)}{\rho})_+) \cdot \frac{\|w_y\|^2 R^2}{\rho^2} } 
%+ \frac{\|w_y\|^2 R^2}{\rho^2}
%\end{eqnarray*}
%
%Summing over all $y$ in $[K]$ and doing some algebra, 

By standard analysis of the Perceptron algorithm, we have that for all $W \in \fR^{K \times d}$, and all $\rho > 0$,
\begin{equation}
\sum_{t=1}^T U_t^{\tilde{y}_t} 
\leq 
\sum_{t=1}^T  U_t^{\tilde{y}_t} (1 - \frac{b_{t,y} w_{\tilde{y}_t} \phi(x_t)}{\rho})_+ 
+ \sqrt{(\sum_{t=1}^T  U_t^{\tilde{y}_t} (1 - \frac{b_{t,y} w_{\tilde{y}_t} \phi(x_t)}{\rho})_+) \cdot \frac{\|W\|_F^2 R^2}{\rho^2} } 
+ \frac{\|W\|_F^2 R^2}{\rho^2},
\label{eqn:perceptron-mb}
\end{equation}
where $R = \max_{t=1}^T \| \phi(x_t)\|$.

Denote by $S_{\sep}$ the indices of examples in $\cbr{(x_t, y_t)}_{t=1}^T$ that is separable by a margin $\gamma$.
Take $W = W^*$, and $\rho = \gamma' \max_{t=1}^T \| \phi(x_t) \|_2$, we have that for all $t$ in $S_{\sep}$, for all $y$ in $[K]$, $(1 - \frac{b_{t,y} W^*_y \phi(x_t)}{\rho})_+ = 0$. For $t$ not in $S_{\sep}$,
$\sum_{y=1}^K U_t^y (1 - \frac{b_{t,y} W^*_y \phi(x_t)}{\rho})_+ \leq (1 + \frac{1}{\gamma'})$. 
This implies that $\sum_{t=1}^T  \sum_{y=1}^K U_t^y (1 - \frac{b_{t,y} w_y \phi(x_t)}{\rho})_+ \leq N(1 + \frac 1 {\gamma'})$.

Combining with Equation~\eqref{eqn:perceptron-mb}, we get 
\[
\sum_{t=1}^T U_t^{\tilde{y}_t}  \leq (1 + \frac 1 {\gamma'}) N + \sqrt{ (1 + \frac 1 {\gamma'}) N \cdot \frac{1}{\gamma'^2} } + \frac{1}{\gamma'^2}.
\]

Now, observe that $M_t$, the mistake indicator at time $t$ is at most $U_{t,1} + N_{t,2}$. Also, note that $\E[U_{t,2} | N_{t,2}] \geq \frac 1 K \cdot N_{t,2}$, as conditioned on $N_{t,2} = 1$, with probability $\frac 1 K$, $U_{t,2} = 1$ . This implies that
\[
\E[\sum_{t=1}^T M_t ] \leq \E[\sum_{t=1}^T U_{t,1} + N_{t,2} ] \leq K \E[\sum_{t=1}^T U_{t,1} + U_{t,2} ]  \leq K ((1 + \frac 1 {\gamma'}) N + \sqrt{ (1 + \frac 1 {\gamma'}) N \cdot \frac{1}{\gamma'^2} } + \frac{1}{\gamma'^2}).
\]
\end{proof}


%\section{Learning intersection of halfspaces with a margin}


\section{Feature mappings that induce OvA linear separability from linear separability}

How do we come up with a feature map that maps a linearly separable dataset to a OvA linearly separable dataset? Recall Definition~\ref{def:mc-ova-margin}, it suffices to give a feature map $\phi$ that for every $i$ in $K$, dataset $S_i = \cbr{(x, b_y^i): (x,y) \in S}$ is separable after the transformation. What property does dataset $S^i$ possess? Observe that for all examples $(x,y)$ in $S$ such that $b_y^i = +1$ (i.e. $y = i$), $x$ is such that $(w_i - w_{y'}) \cdot x \geq \gamma$ for all $y' \neq i$. For all examples $(x,y)$ in $S$ such that $b_y^i = -1$ (i.e. $y \neq i$), $x$ is such that 
$(w_y - w_i) \cdot x \geq \gamma$, specifically, $(w_i - w_{y'}) \cdot x \leq -\gamma$ for some $y' \neq i$. This motivates the following definition.

%Suppose we are given examples 
\begin{definition}
Suppose we are given dataset $S = \cbr{(x, y)}$, where $x$ is in $\cbr{x \in \fR^d: \| x \| \leq 1}$, and $y$ is in $\cbr{-1,+1}$.
$S$ is said to be separable by an intersection of halfspaces with margin parameter $\gamma > 0$, if there exists $t$ linear classifiers $v_1, \ldots, v_t$ (all of which are in $\cbr{v \in \fR^d: \| v \| \leq 1}$),
such that for every $(x,y)$ in $S$:
\begin{enumerate}
  \item If $y = +1$, then $v_i \cdot x \geq \gamma$ holds simultaneously for all $i$ in $[t]$.
  \item If $y = -1$, then there exists an $i$ in $[t]$, $v_i \cdot x \leq -\gamma$.
\end{enumerate}
\label{def:int-margin}
\end{definition}
\cz{Looking at~\cite{klivans2004learning} and its proof, it assumes that for all $(x,y)$ in $S$, $|v_i \cdot x| \geq \gamma$. In our subsequent proofs, we show that it can be in fact weakened to the condition above, which is also more suitable for online bandit multiclass linear classification.}

Given a set $S$ separable by an intersection of halfspaces with a margin $\gamma$, our goal is to find a feature map $\phi: \fR^d \to \fR^{d'}$, such that the dataset $S' = \cbr{(\phi(x), y): (x,y) \in S}$ is linearly separable by a margin $\gamma' > 0$.
\begin{definition}
Dataset $T$ is said to be separable by halfspace with margin parameter $\rho > 0$, if there exists a linear classifier $u$, such that,
\[ \min_{(x,y) \in T} y u \cdot x \geq \rho \|u\| \max_{(x,y) \in T} \| x \|. \]
\end{definition}

That is,
there exists a vector $u$ in $\fR^{d'}$, such that $\min_{(x,y) \in S} y u \cdot \phi(x) \geq \gamma' \|u\| \max_{(x,y) \in S} \| \phi(x) \|$.
In addition, we would like our feature map $\phi$ to correspond to a kernel function $k(x,x') = \inner{\phi(x), \phi(x')}$ that can be efficiently evaluated.


Our $\phi$'s in considerations are as follows:
\begin{enumerate}
%\sqrt{\frac{1}{\deg+1}}
\item $\phi_{s}(x) = ( x_1^{i_1} \ldots x_d^{i_d})_{i_1 + \ldots + i_d \leq s, i_1, \ldots, i_d \geq 0} = (x_S)_{S \in \fN^d, \|S\|_1 \leq s}$. This corresponds to the complete symmetric kernel $K_s$. Note that for all $x$ such that $\| x \|_2 \leq 1$,
$\inner{\phi_{s}(x), \phi_{s}(x)} \leq \sum_{i=0}^s \|x\|^{2i} \leq 1 + s$.

%\frac{1}{2^{\deg}}
\item $\phi_{s}'(x) = ( \sqrt{ {s \choose i_1+\ldots+i_d} {i_1+\ldots+i_d \choose i_1 \ldots i_d}}  x_1^{i_1} \ldots x_d^{i_d})_{i_1 + \ldots + i_d \leq s, i_1, \ldots, i_d \geq 0}$. This corresponds to the polynomial kernel
$K_s'(x,x') = (1 + \inner{x, x'})^s$. Note that for all $x$ such that $\| x \|_2 \leq 1$,
$\inner{\phi_s'(x), \phi_s'(x)} = K_s'(x,x) = (1 + \|x\|^2)^s \leq 2^s$.

\item $\phi_0(x) = ( \sqrt{ (\frac 1 2)^{i_1+\ldots+i_d} {i_1+\ldots+i_d \choose i_1 \ldots i_d}}  x_1^{i_1} \ldots x_d^{i_d})_{i_1, \ldots, i_d \geq 0}$. This corresponds to the kernel $K(x,x') = \frac{1}{1 - \frac 1 2 \inner{x,x'}}$.
Note that for all $x$ such that $\| x \|_2 \leq 1$,
$\inner{\phi_0(x), \phi_0(x)} = K(x,x) \leq 2$.
\end{enumerate}

For a polynomial $p = \sum_{S \in \fN^d} c_S x_S$, define $\|p\|$, the norm of $p$, as the $\ell_2$ norm of the vector that contains its coefficients on every monomial, that is $\|p\| = \| (c_S)_{S \in \fN^d} \| = \sqrt{\sum_{S \in \fN^d} c_S^2}$. We have the following simple result.
\begin{lemma}
Suppose polynomial $p$ has degree $\deg$, and has norm $\|p\|$. Then
\begin{enumerate}
\item For any degree $s \geq \deg$, $p$ can be written as $\inner{c, \phi_s(x)}$, where $\|c\| = \|p\|$.
\item For any degree $s \geq \deg$, $p$ can be written as $\inner{c', \phi_s'(x)}$, where $\|c'\| \leq \|p\|$.
\item $p$ can be written as $\inner{c_0, \phi_0(x)}$, where $\|c_0\| \leq 2^{\frac{\deg}{2}} \|p\|$.
\end{enumerate}
\end{lemma}
\label{lem:fm-convert}
\begin{proof}
Note that $p(x)$ can be written as $\sum_{i_1,\ldots,i_d \in \fN} c_{i_1,\ldots,i_d} x_1^{i_1} \ldots x_d^{i_d}$. As $p$ has degree $\deg$, $c_{i_1,\ldots,i_d} = 0$ for $i_1 + \ldots + i_d > \deg$.

The first item follows as $\phi_s(x)$ contains all monomials of degree at most $s$, which in turn contains all monomials in $p$.

For the second item, we define $(c')_{i_1,\ldots,i_d} = \frac{c_{i_1,\ldots,i_d}}{\sqrt{ {s \choose i_1+\ldots+i_d} {i_1+\ldots+i_d \choose i_1 \ldots i_d}}}$ for all $(i_1,\ldots,i_d) \in \fN$. It can be seen that $\inner{c', \phi_s'(x)} = \inner{c, \phi_s(x)}$, and
$\| c' \| \leq \| c \| = \| p \|$.

For the third item, we define $(c_0)_{i_1,\ldots,i_d} = \frac{c_{i_1,\ldots,i_d}}{\sqrt{ (\frac 1 2)^{i_1+\ldots+i_d} {i_1+\ldots+i_d \choose i_1 \ldots i_d}}}$ for all $(i_1,\ldots,i_d) \in \fN$. As $c_{i_1,\ldots,i_d} = 0$ for $i_1 + \ldots + i_d > \deg$, $|(c_0)_{i_1,\ldots,i_d}| \leq 2^{\frac{\deg}{2}} |c_{i_1,\ldots,i_d}|$.
This implies that
$\| c_0 \| \leq 2^{\frac{\deg}{2}} \| c \| = 2^{\frac{\deg}{2}} \| p \| $.
\end{proof}

%This is also the RKHS norm of $p$ wrt the complete symmetric kernel $K_{\deg}$, for $\deg \geq \deg(p)$.



\subsection{Large margin polynomial via rational functions}
\paragraph{The construction.} Define rational function
\[
  Q(x) = \sum_{i=1}^t S_{\ceil{\log \frac 1 \gamma}}^{2t}(\frac{v_i \cdot x}{\gamma}) - (t - \frac 1 2).
\]
where for $k,n$ in $\fN$, function
\[
S_n^k(x) = \frac{P_n(-x)^{h(k)} - P_n(x)^{h(k)}}{P_n(-x)^{h(k)} + P_n(x)^{h(k)}},
\]
$h(k)$ is the smallest odd integer greater than $\log(2k+1)$, and $P_n(x) = (x - 1) \cdot \prod_{i=1}^n (x - 2^i)^2$.

Now denote by $A_n^k(x) = P_n(x)^{h(k)} - P_n(-x)^{h(k)}$, $B_n^k(x) = -P_n(-x)^{h(k)} - P_n(x)^{h(k)}$.
Also, define $D_i(x) = S_{\ceil{\log \frac 1 \gamma}}^{2t}(\frac{v_i \cdot x}{\gamma})$.

\cz{Note the difference between this line and KS04 - we multiply by $B_{\ceil{\log \frac 1 \gamma}}^{2t}(\frac{v_i \cdot x}\gamma)$ as opposed to its square. The latter would also work, but it will make the equations more cumbersome.}
Define polynomial $p$ as
\begin{eqnarray*}
  p(x) &:=& \sum_{i=1}^t A_{\ceil{\log \frac 1 \gamma}}^{2t}(\frac{v_i \cdot x}{\gamma}) \cdot \prod_{j \neq i} B_{\ceil{\log \frac 1 \gamma}}^{2t}(\frac{v_i \cdot x}{\gamma}) - (t-\frac12) \prod_{j=1}^t B_{\ceil{\log \frac 1 \gamma}}^{2t}(\frac{v_i \cdot x}{\gamma})
\end{eqnarray*}

It can be seen that
$p(x) = \prod_{i=1}^t B_{\ceil{\log \frac 1 \gamma}}^{2t}(\frac{v_i \cdot x}\gamma) \cdot Q(x)$.

\begin{lemma}[\cite{klivans2004learning}]
For all $(x,y)$ in $S$ satisfying Definition~\ref{def:int-margin}, we have that $y Q(x) \geq \frac 1 2$.
As a consequence, $y p(x) \geq 2^{\ceil{\log \frac 1 \gamma}(\ceil{\log \frac 1 \gamma}-1)h(2t)t - 2}$.
\label{lem:rational-margin}
\end{lemma}
\begin{proof}
We consider two cases regarding the label of the example.
\begin{enumerate}
\item Suppose $y = 1$, then for all $i$ in $[t]$, $v_i \cdot x \in [\gamma, 1]$. This implies that $\frac{v_i \cdot x}{\gamma} \in [1, \frac 1 \gamma]$.
By item~\ref{item:sign-rt} of Fact~\ref{fact:newman}, we have that $D_i(x) \in [1, 1+ \frac 1 {2t}]$.
Hence, $Q(x) \geq t - (t - \frac 1 2) \geq \frac 1 2$.

\item Suppose $y = -1$. First observe that for all $i$ in $[t]$, $\frac{v_i \cdot x}{\gamma} \in [-\frac 1\gamma, \frac 1 \gamma]$. By items~\ref{item:sign-rt},
~\ref{item:sign-lt} and~\ref{item:sign-mid} of Fact~\ref{fact:newman}, we have that $S_{\ceil{\log \frac 1 \gamma}}^{2t}(\frac{v_i \cdot x}{\gamma}) \leq 1+ \frac 1 {2t}$.
In addition, by Definition~\ref{def:int-margin}, there exists an $i_0$ in $[t]$, such that $v_{i_0} \cdot x \leq -\gamma$.
By item~\ref{item:sign-lt} of Fact~\ref{fact:newman}, we have that
$D_{i_0}(x) \in [-1 - \frac 1 {2t}, -1]$.
Hence,
\[ Q(x) = D_{i_0}(x) + \sum_{i \neq i_0} D_i(x) - (t - \frac 1 2)
\leq -1 + (t-1)(1 + \frac 1 {2t}) - (t - \frac 1 2) \leq - \frac 1 2. \]
%\leq -1 + t-1 + \frac{t-1}{2t} - (t - \frac 1 2)
\end{enumerate}
To summarize, we have shown that for all $(x,y)$ in $S$, $y Q(x) \geq \frac 1 2$.
Therefore, by item~\ref{item:sign-denom} of Fact~\ref{fact:newman}, 
\[ y p(x) = y Q(x) \cdot \prod_{i=1}^t B_{\ceil{\log \frac 1 \gamma}}^{2t}(\frac{v_i \cdot x}\gamma) \geq \frac 1 2 ((1-\frac{1}{4t+1}) \cdot 2^{n(n-1)h(2t)})^t \geq 2^{n(n-1)h(2t)t - 2}. \]
\end{proof}


\begin{fact}
For $k, n$ in $\fN$, $S_n^k(\cdot)$ has the following properties:
\begin{enumerate}
\item If $x$ is in $[1, 2^n]$, then $S_n^k(x)$ is in $[1, 1 + \frac 1 k]$. \label{item:sign-rt}
\item If $x$ is in $[-2^{-n}, -1]$, then $S_n^k(x)$ is in $[-1 - \frac 1 k, -1]$. \label{item:sign-lt}
\item If $x$ is in $[-1, 1]$, then $|S_n^k(x)|$ is in $[-1, 1]$. \label{item:sign-mid}
\cz{This fact is based on discussions with Chen-Yu, which we found important but was omitted by~\cite{klivans2004learning}.}
\item For all $x$ in $[-2^{n}, 2^{n}]$, $B_n^k(x) \geq (1-\frac{1}{2k+1}) \cdot 2^{n(n-1)h(k)}$.
\cz{This is a refined lower bound of Lemma 6 of~\cite{klivans2004learning}; we got rid of the weird
requirement that ``the fractional part of $x$ is at least $2^{-n}$''.}
\label{item:sign-denom}
\end{enumerate}
\label{fact:newman}
\end{fact}
\begin{proof}
We prove all the items accordingly.
\begin{enumerate}
\item Observe that $S_n^k(x)$ can be written as:
\[ S_n^k(x) = \frac{1 + (-\frac{P_n(x)}{P_n(-x)})^{h(k)}}{ 1 - (-\frac{P_n(x)}{P_n(-x)})^{h(k)} }.  \]
Denote by $r = (-\frac{P_n(x)}{P_n(-x)})^{h(k)}$. By item~\ref{item:x-large} of Fact~\ref{fact:pn}, $r \in [0, \frac{1}{2k+1}]$. This implies that
$S_n^k(x) = \frac{1+r}{1-r} \in [1, 1 + \frac 1 k]$.

\item The item follows from item~\ref{item:sign-rt} and the fact that $S_n^k(\cdot)$ is an odd function.

\item Note that if $x$ is in $[0, 1]$, then by item~\ref{item:x-small} of Fact~\ref{fact:pn}, $r = (-\frac{P_n(x)}{P_n(-x)})^{h(k)} \in [-1, 0]$. This implies that $S_n^k(x) = \frac{1+r}{1-r} \in [0,1]$. As $S_n^k$ is an odd function, we also have that $S_n^k(x) \in [-1, 0]$ for all $x$ in $[-1, 0]$. This proves the item.

\item Noticing that $B_n^k(x)$ is an even function, without loss of generality, suppose $x \geq 0$.
First, note that if $x \geq 0$, then $\abs{P_n(-x)} \geq \prod_{i=1}^n 2^{2i} = 2^{n(n-1)}$.
If $x$ is in $[1,2^n]$, then $B_n^k(x) = (-P_n(-x))^{h(k)} (1 - r) \geq (-P_n(-x))^{h(k)} \cdot \frac{2k}{2k+1} \geq 2^{n(n-1)h(k)} \cdot \frac{2k}{2k+1}$.
If $x$ is in $[0,1]$, as both $P_n(-x)$ and $-P_n(x)$ are $\leq 0$, we have $B_n^k(x) \geq (-P_n(-x))^{h(k)} \geq 2^{n(n-1)h(k)}$. Therefore, for all $x \in [0,2^n]$,
$B_n^k(x) \geq (1-\frac{1}{2k+1}) 2^{n(n-1)h(k)}$.
\end{enumerate}
\end{proof}

\begin{fact}
The following are true:
\begin{enumerate}
\item If $x \in [0,1]$, then $0 \leq -P_n(x) \leq -P_n(-x)$. \label{item:x-small}
\item If $x \in [1, 2^n]$, then $0 \leq 4P_n(x) \leq -P_n(-x)$. \label{item:x-large}
\end{enumerate}
\label{fact:pn}
\end{fact}
\begin{proof}
First, observe that for any $x \geq 0$, for all $i$, $(x+2^i)^2 \geq (x - 2^i)^2$ holds.

For the first item, given that $x \in [0,1]$, $1+x \geq 1-x \geq 0$. Therefore,
\[ \frac{-P_n(-x)}{-P_n(x)} = \frac{1+x}{1-x} \cdot \prod_{i =1}^n \frac{(x+2^i)^2}{(x - 2^i)^2}  \geq 1. \]

For the second item, given that $x \geq 1$, $1+x \geq x-1 \geq 0$. In addition, as $x$ is in $[1,2^n]$, there exists an $i_0 \in [n]$ such that $2^{i_0-1} \leq x \leq 2^i_0$. Therefore,
$(x+2^i)^2 \geq 4(x - 2^i)^2$. Hence,
\[ \frac{-P_n(-x)}{P_n(x)} = \frac{1+x}{ x-1} \cdot \frac{(x+2^{i_0})^2}{(x - 2^{i_0})^2} \cdot \prod_{i \neq i_0} \frac{(x+2^i)^2}{(x - 2^i)^2} \geq 4. \]
\end{proof}

\paragraph{Norm bound.} We bound the norm of $p$ in the theorem below.
\begin{theorem}
$p(x)$ has degree $\deg = (2\ceil{\log \frac 1 \gamma}+1)h(2t)t$, and has norm $\|p\| \leq ((16\ceil{\log \frac 1 \gamma}+8) h(2t) t \cdot \frac 1 \gamma)^{(2\ceil{\log \frac 1 \gamma}+1)h(2t)t}$.
\label{thm:rational-norm}
\end{theorem}
\begin{proof}
The degree of $p$ follows from the
Let $n = \ceil{\log \frac 1 \gamma}$.

First, for all $i$ in $[t]$ and $l$ in $\cbr{0,1,\ldots,n}$,
\[ \| (\frac{v_i \cdot x}{\gamma} - 2^l) \|^2 \leq \frac{1}{\gamma^2} + 2^{2l} \leq 2 \cdot 2^{2n}. \]
This implies that
\[ \| P_n(\frac{v_i \cdot x}{\gamma}) \|^2 \leq ((4n+2) \cdot 2^{2n})^{2n+1}. \]
Therefore,
\[ \| P_n(\frac{v_i \cdot x}{\gamma})^{h(2t)} \|^2 \leq h(2t)^{(2n+1)h(2t)} \cdot ((4n+2) \cdot 2^{2n})^{(2n+1)h(2t)}. \]
%$A_n^k(x) = P_n(x)^{h(k)} - P_n(-x)^{h(k)}$, $B_n^k(x) = -P_n(-x)^{h(k)} - P_n(x)^{h(k)}$
By the definition of $A_n^k$ and $B_n^k$, we have
\[ \| A_n^{2t}(\frac{v_i \cdot x}{\gamma}) \|^2,  \| B_n^{2t}(\frac{v_i \cdot x}{\gamma}) \|^2 \leq ((8n+4) h(2t) \cdot 2^{2n})^{(2n+1)h(2t)}. \]

%\sum_{i=1}^t A_{\ceil{\log \frac 1 \gamma}}^{2t}(\frac{v_i \cdot x}{\gamma}) \cdot \prod_{j \neq i} B_{\ceil{\log \frac 1 \gamma}}^{2t}(\frac{v_i \cdot x}{\gamma}) - (t-\frac12) \prod_{j=1}^t B_{\ceil{\log \frac 1 \gamma}}^{2t}(\frac{v_i \cdot x}{\gamma})
Hence, for every $i$ in $[t]$,
\[
\| A_n^{2t}(\frac{v_i \cdot x}{\gamma}) \cdot \prod_{j \neq i} B_n^{2t}(\frac{v_j \cdot x}{\gamma}) \|^2
\leq
t^{(2n+1) h(2t) t} \cdot ((8n+4) h(2t) \cdot 2^{2n})^{(2n+1)h(2t)t}.
\]
Similarly,
\[
\| \prod_{i=1}^t B_n^{2t}(\frac{v_i \cdot x}{\gamma}) \|^2 \leq t^{(2n+1) h(2t) t} \cdot ((8n+4) h(2t) \cdot 2^{2n})^{(2n+1)h(2t)t}.
\]
Therefore,
\[
\| p(x) \|^2 \leq ((t - \frac 1 2)^2 + t) \cdot ((8n+4) h(2t) t \cdot 2^{2n})^{(2n+1)h(2t)t} \leq ((16n+8) h(2t) t \cdot 2^{2n})^{(2n+1)h(2t)t}.
\]
where the second inequality uses that $2^{(2n+1)h(2t)t} \geq t^2 + \frac 1 4$.
This implies that $p(x)$ can be written as $\inner{c, \phi_1(x)}$, where $\| c \|_2^2 \leq ((16n+8) h(2t) t \cdot 2^{2n})^{(2n+1)h(2t)t}$. Taking square roots on both sides, the theorem follows from algebra.
%Hence, $\| c \|_2 \leq ((16n+8) h(2t) t \cdot 2^n)^{(2n+1)h(2t)t}$.
\end{proof}



\subsection{Large margin polynomials via Chebyshev polynomials}
\paragraph{The construction.} Define
\[ p(x) = t + \frac 1 2 - \sum_{i=1}^t (T_{\ceil{\sqrt{\frac 1 \gamma}}}(1 - v_i \cdot x))^{\ceil{\log 2t}}, \]
where
$T_r(\cdot)$ is the $r$-th order Chebyshev polynomial. Denote by $C_i(x) = (T_{\ceil{\sqrt{\frac 1 \gamma}}}(1 - v_i \cdot x))^{\ceil{\log 2t}}$. $p$ can therefore be simplified to
\[ p(x) = t + \frac 1 2 - \sum_{i=1}^t C_i(x). \]

\begin{lemma}[\cite{klivans2004learning}]
For all $(x,y)$ in $S$ satisfying Definition~\ref{def:int-margin}, we have that $y p(x) \geq \frac 1 2$.
\label{lem:cheb-margin}
\end{lemma}
\begin{proof}
We consider the two cases regarding the label of the example.
\begin{enumerate}
\item Suppose $y = 1$, then for all $i$ in $[t]$, $v_i \cdot x \geq \gamma$. This implies that for all
$i$ in $[t]$, $1 - v_i \cdot x \in [0, 1-\gamma]$. By item~\ref{item:bound-in} of Fact~\ref{fact:cheb}, we have that for all $i$, $|C_i(x)| \leq 1$.
Therefore, $p(x) \geq t + \frac 1 2 - t \geq \frac 1 2$.

\item Suppose $y = -1$. First observe that for all $i$, $T_{\ceil{\sqrt{\frac 1 \gamma}}}(1 - v_i \cdot x) \geq -1$. This is because of items~\ref{item:bound-in} and~\ref{item:bound-out} of Fact~\ref{fact:cheb}, and that $1 - v_i \cdot x \geq 0$.
This implies that $C_i(x) \geq -1$.
On the other hand, by Definition~\ref{def:int-margin}, there exists an $i_0$ in $[t]$, such that $v_{i_0} \cdot x \leq -\gamma$. By item~\ref{item:bound-out} of Fact~\ref{fact:cheb}, we have that
$C_{i_0}(x) \geq (1 + \gamma \cdot \frac 1 \gamma)^{\ceil{\log 2t}} \geq 2t$. Therefore,
\[ p(x) = t + \frac 1 2 - C_{i_0}(x) - \sum_{i \neq i_0} C_i(x) \leq t + \frac 1 2 - 2t + (t-1) \leq -\frac 1 2. \]
\end{enumerate}
\end{proof}

\begin{fact}
$T_r(\cdot)$ has the following properties:
\begin{enumerate}
  \item If $|z| \leq 1$, then $|T_r(z)| \leq 1$. \label{item:bound-in}
  \item If $z > 1$, then $T_r(z) \geq 1 + r^2 (z-1)$. \label{item:bound-out}
  \item $T_r(z)$ can be written as $\sum_{l=0}^r c_l z^l$, where coefficients $c_l$ are bounded by $2^r$ in absolute value.
\end{enumerate}
\label{fact:cheb}
\end{fact}

\paragraph{Norm bound.} We bound the norm of $p$ in the theorem below.
\begin{theorem}
$p(x)$ has degree $\deg = \ceil{\sqrt{\frac1\gamma}} \cdot \ceil{\log(2t)}$, and has norm $\|p\| \leq 2 \cdot (256 \ceil{\sqrt{\frac1\gamma}} \ceil{\log 2t})^{\ceil{\sqrt{\frac1\gamma}} \ceil{\log 2t}}$.
\label{thm:cheb-norm}
\end{theorem}
\begin{proof}
Denote by $r = \ceil{\sqrt{\frac1\gamma}}$.
First, for all $i$, as $v_i$ has norm at most 1,
\[ \| (1 - v_i \cdot x) \|^2 \leq 2. \]
Therefore, by Lemma~\ref{lem:norm-oper},
\[ \| c_l (1 - v_i \cdot x)^l \|^2 = c_l^2 \| (1 - v_i \cdot x)^l \|^2 \leq 2^{2r} l^l 2^l. \]
Hence,
\[ \| T_r(1 - v_i \cdot x) \|^2 = \| \sum_{l=0}^r c_l (1 - v_i \cdot x)^l \|^2 \leq (r+1) \cdot \sum_{l=0}^r 2^{2r} l^l 2^l \leq (r+1)^2 (8r)^r \leq (32r)^r. \]
where the last inequality uses the fact that $(r+1)^2 \leq 2^r$.
And by Lemma~\ref{lem:norm-oper},
\[ \| C_r(x) \|^2 \leq (\ceil{\log 2t})^{\ceil{\log 2t} r} \cdot ((32r)^r)^{\ceil{\log 2t}}. \]
Therefore,
\[ \| p(x) \|^2 \leq (t+1) \cdot ((t+\frac 1 2)^2 + t (32 r\ceil{\log 2t})^{\ceil{\log 2t} r}) \leq 2 \cdot (256 r \ceil{\log 2t})^{r \ceil{\log 2t}}  \]
where the last inequality uses the fact that $(t+1)((t+\frac12)^2+t) \leq 2 \cdot 8^{\ceil{\log 2t}}$ and algebra.
%$2^{2r \ceil{\log 2t}} \geq \max(2t^2, r^2)$.
\end{proof}

\section{Margin lower bounds on respective feature maps}
\paragraph{Margin bounds using non-adaptive feature maps.}
\begin{theorem}
Suppose we are given an integer $s$, and a set of examples $S$ separable by an intersection of halfspaces by margin $\gamma$. Define the mapped dataset $S'=\cbr{(\phi_s(x),y): (x,y) \in S}$. Then,
\begin{enumerate}
\item If $s \geq (2\ceil{\log \frac 1 \gamma}+1) h(2t) t$, then $S'$ is separable by a halfspace with margin
\[
\gamma' = \frac{1}{ ( (16 \ceil{\log \frac 1 \gamma} + 8 ) h(2t) t 2^{\ceil{\log \frac 1 \gamma}})^{(2\ceil{\log \frac 1 \gamma} + 1 ) h(2t) t} }.
\]
\item If $s \geq \ceil{\sqrt{\frac1\gamma}} \cdot \ceil{\log(2t)}$, then $S'$ is separable by a halfspace with margin
\[
\gamma' = \frac{1}{4 \cdot (256 \ceil{\sqrt{\frac1\gamma}} \ceil{\log 2t})^{\ceil{\sqrt{\frac1\gamma}} \ceil{\log 2t}}}.
\]
\end{enumerate}
\end{theorem}

\begin{proof}
\begin{enumerate}
\item From Lemma~\ref{lem:rational-margin} and Theorem~\ref{thm:rational-norm}, 
along with the fact that $s \geq (2\ceil{\log \frac 1 \gamma}+1) h(2t) t$,
we know that there exists some polynomial $p = \inner{c, \phi_s(x)}$, such that
$y \inner{c, \phi_s(x)} \geq  2^{\ceil{\log \frac 1 \gamma}(\ceil{\log \frac 1 \gamma}-1)h(2t)t - 2}$ and 
$\|c\| \leq ((16\ceil{\log \frac 1 \gamma}+8) h(2t) t \cdot \frac 1 \gamma)^{(2\ceil{\log \frac 1 \gamma}+1)h(2t)t}$. This implies that $S'$ is separable by a halfspace with the desired margin.

\item From Lemma~\ref{lem:cheb-margin} and Theorem~\ref{thm:cheb-norm},
along with the fact that $s \geq \ceil{\sqrt{\frac1\gamma}} \cdot \ceil{\log(2t)}$, we know that there exists some polynomial $p = \inner{c, \phi_s(x)}$, such that
$y \inner{c, \phi_s(x)} \geq \frac 1 2$ and
$\| c \| \leq 2 \cdot (256 \ceil{\sqrt{\frac1\gamma}} \ceil{\log 2t})^{\ceil{\sqrt{\frac1\gamma}} \ceil{\log 2t}}$. This implies that $S'$ is separable by a halfspace with the desired margin.
\end{enumerate}
\end{proof}


\paragraph{Margin bounds using adaptive-degree feature map.}
\begin{theorem}
Suppose we are given a set of examples $S$ separable by an intersection of halfspaces by margin $\gamma$. Define the mapped dataset $S'=\cbr{(\phi_0(x),y): (x,y) \in S}$. Then, $S'$ is separable by some halfspace with margin
\[
\gamma' =
\max(
\frac{1}{ \sqrt{2} ( (32 \ceil{\log \frac 1 \gamma} + 16 ) h(2t) t 2^{\ceil{\log \frac 1 \gamma}})^{(2\ceil{\log \frac 1 \gamma} + 1 ) h(2t) t} },
\frac{1}{4 \sqrt{2} \cdot (512 \ceil{\sqrt{\frac1\gamma}} \ceil{\log 2t})^{\ceil{\sqrt{\frac1\gamma}} \ceil{\log 2t}}}
)
\]
\end{theorem}

\begin{proof}
%The proof the essentially the same as the proof of 
\begin{enumerate}
\item From Lemma~\ref{lem:rational-margin} and Theorem~\ref{thm:rational-norm}, 
we know that there exists some polynomial $p(x)$ of degree at most $(2\ceil{\log \frac 1 \gamma}+1) h(2t) t$, such that
$y p(x) \geq  2^{\ceil{\log \frac 1 \gamma}(\ceil{\log \frac 1 \gamma}-1)h(2t)t - 2}$ and 
$\|p\| \leq ((16\ceil{\log \frac 1 \gamma}+8) h(2t) t \cdot \frac 1 \gamma)^{(2\ceil{\log \frac 1 \gamma}+1)h(2t)t}$. 
By Lemma~\ref{lem:fm-convert}, there is some vector $c$, such that $y \inner{c, \phi_0(x)} = y p(x)$ for all $(x,y)$ in $S$	, and $\| c \| \leq 2^{\frac 1 2 \cdot (2\ceil{\log \frac 1 \gamma}+1) h(2t) t} \| p \|$.
This implies that $S'$ is separable by a halfspace with the desired margin.

\item From Lemma~\ref{lem:cheb-margin} and Theorem~\ref{thm:cheb-norm}, we know that there exists some polynomial $p(x)$ of degree $\ceil{\sqrt{\frac1\gamma}} \cdot \ceil{\log(2t)}$ such that
$y p(x) \geq \frac 1 2$ and
$\| p \| \leq 2 \cdot (256 \ceil{\sqrt{\frac1\gamma}} \ceil{\log 2t})^{\ceil{\sqrt{\frac1\gamma}} \ceil{\log 2t}}$. 
By Lemma~\ref{lem:fm-convert}, there is some vector $c$, such that $y \inner{c, \phi_0(x)} = y p(x)$ for all $(x,y)$ in $S,$ and $\| c \| \leq 2^{\frac 1 2 \cdot \ceil{\sqrt{\frac1\gamma}} \cdot \ceil{\log(2t)}} \| p \|$.	
This implies that $S'$ is separable by a halfspace with the desired margin.
\end{enumerate}


\end{proof}

%\ceil{\log \frac 1 \gamma} + 1 ) h(2t) t
%2^\ceil{\log \frac 1 \gamma} )^{ (2  }

%and has norm $\|p\| \leq ((16\ceil{\log \frac 1 \gamma}+8) h(2t) t \cdot \frac 1 \gamma)^{(2\ceil{\log \frac 1 \gamma}+1)h(2t)t}$.


\section{Auxiliary lemmas}
\begin{lemma}
   For $i=1,\ldots, \ell$ let $q^{(i)}(x)=\sum_S c^{(i)}_{S} x_S $ be a polynomial over $x_1, \ldots, x_d$. Then
   \begin{enumerate}
     \item if $q^{(1)}\ldots q^{(\ell)}$ has degree at most $\deg$, we have $\|\prod_{i=1}^{\ell} q^{(i)}\|^2 \leq \ell^{\deg} \prod_{i=1}^{\ell} \|q^{(i)}\|^2$. Specifically, for a polynomial $p$ of degree $s$, $\|p^\ell \|^2 \leq  \ell^{\ell s} (\|p\|^2)^\ell$.
     \item $\|\sum_{i=1}^{\ell} q^{(i)}\|^2 \leq \ell(\sum_{i=1}^{\ell} \|q^{(i)}\|^2)$.
   \end{enumerate}
   \label{lem:norm-oper}
\end{lemma}



\newpage
\bibliography{adaptivity}
\bibliographystyle{plain}

\end{document}
