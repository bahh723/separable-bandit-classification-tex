\section{From linear separability to strong linear separability}
\label{section:from-linear-separability-to-strong-linear-separability}

In this section, we show how to construct a mapping $\phi$ from the unit ball of
$\R^d$ into a high dimensional inner product space that has the property that if
a set of labeled examples in the unit ball is linearly separable with a margin
$\gamma$, applying the mapping $\phi$ makes the examples \emph{strongly}
linearly separable with a margin $\gamma'$ and their norms are bounded by $R'$.
The parameters $\gamma'$ and $R'$ are functions of the old margin $\gamma$ and
the number of classes $K$, and the are specified in the theorems below.

Equipped with the mapping $\phi$, we can utilize
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} and
\autoref{theorem:strongly-separable-example-mistake-upper-bound} from the
previous section and we obtain an algorithm for linearly separable examples and
an upper bound on its number of mistakes. As a computational speed up, instead
of working with the mapped examples $\phi(x_1), \phi(x_2), \dots$ explicitly, we
can use the kernelized version of
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples}
that uses a kernel function $k(x,x') = \ip{\phi(x)}{\phi(x')}$.

We construct several different mappings $\phi$. The mappings differ in the
parameters $R'$, $\gamma'$, time complexity of evaluating $\phi(x)$, and time
complexity of evaluating $k(x,x')$. Some of the mappings depend the original
margin $\gamma$. In practice, the margin parameter $\gamma$ is not known, which
makes these mappings impractical. However, one of the mappings we construct
does \emph{not} depend on $\gamma$.

The idea behind all the mappings is polynomial approximation. According to the
well known Stone-Weierstrass theorem (see
e.g.~\citep[Section~10.10]{Davidson-Donsig-2010}), on a compact set,
multivariate polynomials uniformly approximate any continuous function.
Intuitively speaking, we use a multivariate polynomial to approximate, on the
unit ball of $\R^d$, the indicator function corresponding to the intersection of
$m=K-1$ halfspaces. Within margin $\gamma$ along the decision boundary, we allow
the polynomial to attain arbitrary value. The polynomial separates examples in
one class from examples in the other classes. To be able to quantify $R'$,
$\gamma'$ and time complexities of evaluating $\phi(x)$ and $k(x,x')$, we need
to quantify certain parameters of the approximating polynomial. We construct two
different polynomials with different parameters. The parameters are quantified
in
Theorems~\ref{theorem:polynomial-approximation-1}~and~\ref{theorem:polynomial-approximation-2}
stated below.

Before we state the theorems, recall that a polynomial of $d$ variables is a
function $p:\R^d \to \R$ of the form
$$
p(x) = p(x_1, x_2, \dots, x_d) = \sum_{\alpha_1, \alpha_2, \dots, \alpha_d} c_{\alpha_1, \alpha_2, \dots, \alpha_d} x_1^{\alpha_1} x_2^{\alpha_2} \dots x_d^{\alpha_d}
$$
where the sum ranges over a finite set of $d$-tuples $(\alpha_1, \alpha_2,
\dots, \alpha_d)$ of non-negative integers and $c_{\alpha_1, \alpha_2, \dots,
\alpha_d}$ is a real coefficient. The \emph{degree} of a polynomial $p$, denoted
by $\deg(p)$, is the largest value of $\alpha_1 + \alpha_2 + \dots + \alpha_d$
for which the coefficient $c_{\alpha_1, \alpha_2, \dots, \alpha_d}$ is non-zero.
The \emph{norm of a polynomial} $p$ is defined as
$$
\norm{p} = \sqrt{\sum_{\alpha_1, \alpha_2, \dots, \alpha_d} \left(c_{\alpha_1, \alpha_2, \dots, \alpha_d} \right)^2 } \; .
$$
It is easy see that this is indeed a norm, since we can interpret it as a
Euclidean norm of the vector of the coefficients of the polynomial.

\begin{theorem}[Polynomial approximation of intersection of halfspaces I]
\label{theorem:polynomial-approximation-1}
Let $v_1, v_2, \dots, v_m \in \R^d$ be vectors such that $\norm{v_1},
\norm{v_2}, \dots, \norm{v_m} \le 1$. Let $\gamma \in (0,1)$. There exists a
multivariate polynomial $p:\R^d \to \R$ such that
\begin{enumerate}
\item $p(x) \ge 1/2$ \\ for all $\displaystyle x \in \bigcap_{i=1}^m \left\{ x \in \R^d ~:~ \norm{x} \le 1, \ \ip{v_i}{x} \ge \gamma \right\}$
\item $p(x) \le -1/2$ \\ for all $\displaystyle x \in \bigcup_{i=1}^m \left\{ x \in \R^d ~:~ \norm{x} \le 1, \ \ip{v_i}{x} \le - \gamma \right\}$
\item $\displaystyle \deg(p) = \left\lceil \log_2(2m) \right\rceil \cdot \left\lceil \sqrt{\frac{1}{\gamma}} \right\rceil$
\item $\displaystyle \norm{p} \le \left( 188 \left\lceil \log_2(2m) \right\rceil \cdot \left\lceil \sqrt{\frac{1}{\gamma}} \right\rceil \right)^{\frac{1}{2} \left\lceil \log_2(2m) \right\rceil \cdot \left\lceil \sqrt{\frac{1}{\gamma}} \right\rceil}$
\end{enumerate}
\end{theorem}

\begin{theorem}[Polynomial approximation of intersection of halfspaces II]
\label{theorem:polynomial-approximation-2}
Let $v_1, v_2, \dots, v_m \in \R^d$ be vectors such that $\norm{v_1},
\norm{v_2}, \dots, \norm{v_m} \le 1$. Let $\gamma \in (0,1)$.
Define
$$
r = 2 \left\lceil \frac{1}{4} \log_2(4m + 1) \right\rceil + 1 \qquad \text{and} \qquad s = \left \lceil \log_2(1/\gamma) \right \rceil \; .
$$
Then, there exists a multivariate polynomial $p:\R^d \to \R$ such that
\begin{enumerate}
\item $\displaystyle p(x) \ge \frac{1}{4} \cdot 2^{s(s+1)rm}$ \\
for all $\displaystyle x \in \bigcap_{i=1}^m \left\{ x \in \R^d ~:~ \norm{x} \le 1, \ \ip{v_i}{x} \ge \gamma \right\}$

\item $\displaystyle p(x) \le - \frac{1}{4} \cdot 2^{s(s+1)rm}$ \\
for all $\displaystyle x \in \bigcup_{i=1}^m \left\{ x \in \R^d ~:~ \norm{x} \le 1, \ \ip{v_i}{x} \le - \gamma \right\}$

\item $\deg(p) \le (2s+1) rm$
\item $\norm{p} \le (2m-1/2) 2^m \cdot \left(2^{2s} rm (4s+2)^2 \right)^{(s+1/2)rm}$
\end{enumerate}
\end{theorem}


The proofs of the theorems can be found in
Section~\ref{section:proof-of-polynomial-approximation}. The geometric
interpretation of the two regions described in parts 1 and 2 of the theorems is
explained in Figure~\ref{figure:pizza-slice}. Similar but weaker results were
proved by~\cite{Klivans-Servedio-2008}. In particular, the bounds in parts
1, 2, 3, 4 of the theorems are independent of the dimension $d$.

\begin{figure}
\begin{center}
\input{figures/pizza-slice}
\end{center}
\caption[]{The figure shows the two regions used in parts 1 and 2 of
Theorems~\ref{theorem:polynomial-approximation-1}~and~\ref{theorem:polynomial-approximation-2}
for the case $m=d=2$ and a particular choice of vectors $v_1, v_2$ and margin
parameter $\gamma$. These regions are
$R^+ = \displaystyle \bigcap_{i=1}^m \left\{ x \in \R^d ~:~ \norm{x} \le 1, \ \ip{v_i}{x} \ge \gamma \right\}$,
$R^- = \displaystyle \bigcup_{i=1}^m \left\{ x \in \R^d ~:~ \norm{x} \le 1, \ \ip{v_i}{x} \le - \gamma \right\}$
The separating hyperplanes $\ip{v_1}{x} = 0$ and $\ip{v_2}{x} = 0$ are shown as dashed lines.}
\label{figure:pizza-slice}
\end{figure}

We construct the three different mappings $\phi_n, \psi_n, \rho$. The first two
are parameterized by a non-negative integer $n$. The domain of $\phi_n$ and
$\psi_n$ is $\R^d$. The domain of $\rho$ is the unit ball of $\R^d$. The
co-domain for $\phi_n$ and $\psi_n$ is $\R^p$ where $p = \sum_{i=0}^n
\binom{i+d-1}{d-1}$ equipped with the standard inner product $\ip{x}{x'} =
\sum_{i=1}^p x_i x'_i$. The co-domain is the classical separable Hilbert space
$\ell_2 = \{ x \in \R^\infty ~:~ \sum_{i=1}^\infty x_i^2 < + \infty \}$ equipped
with the standard inner product $\ip{x}{x'} = \sum_{i=1}^\infty x_i x'_i$. We
index the coordinates of the co-domain $\R^p$ by $d$-tuples $(\alpha_1,
\alpha_2, \dots, \alpha_d)$ of non-negative integers such that $\alpha_1 +
\alpha_2 + \dots + \alpha_d \le n$. Similarly, we index the coordinates of
$\ell_2$ by $d$-tuples $(\alpha_1, \alpha_2, \dots, \alpha_d)$ of non-negative
integers. The mappings are defined by
\begin{align*}
& \left(\phi_n(x_1, x_2, \dots, x_d)\right)_{(\alpha_1, \alpha_2, \dots, \alpha_d)}
= x_1^{\alpha_1} x_2^{\alpha_2} \dots x_d^{\alpha_d}
\\
& \left(\psi_n(x_1, x_2, \dots, x_d)\right)_{(\alpha_1, \alpha_2, \dots, \alpha_d)} = x_1^{\alpha_1} x_2^{\alpha_2} \dots x_d^{\alpha_d} \\
& \qquad \cdot \sqrt{\binom{n}{\alpha_1 + \alpha_2 + \dots + \alpha_d} \binom{\alpha_1 + \alpha_2 + \dots + \alpha_d}{\alpha_1, \alpha_2, \dots, \alpha_d}}
\\
& \left(\rho(x_1, x_2, \dots, x_d)\right)_{(\alpha_1, \alpha_2, \dots, \alpha_d)} = x_1^{\alpha_1} x_2^{\alpha_2} \dots x_d^{\alpha_d} \\
& \qquad \cdot \sqrt{2^{-(\alpha_1 + \alpha_2 + \dots + \alpha_d)} \binom{\alpha_1 + \alpha_2 + \dots + \alpha_d}{\alpha_1, \alpha_2, \dots, \alpha_d}}
\end{align*}
where $\binom{\alpha_1 + \alpha_2 + \dots + \alpha_d}{\alpha_1, \alpha_2, \dots, \alpha_d} = \frac{(\alpha_1 + \alpha_2 + \dots + \alpha_d)!}{\alpha_1! \alpha_2! \dots \alpha_d!}$
is the multinomial coefficient and $\binom{n}{k} = \frac{n!}{(n-k)!k!}$ is the binomial coefficient.

Mappings $\psi_n$ and $\rho$ have simple-to-compute kernels
\begin{align*}
K_{\psi_n}(x,x') & = \ip{\psi_n(x)}{\psi_n(x')}_{\R^p} = \left( 1 + \ip{x}{x'} \right)^n \; , \\
K_{\rho}(x,x') & = \ip{\rho(x)}{\rho(x')}_{\ell_2} = \frac{1}{1 - \frac{1}{2}\ip{x}{x'}} \; .
\end{align*}
In passing we note that the formula for $K_\rho$ implies the mapping $\rho$ is
well defined. Indeed, since $K_{\rho}(x,x) < +\infty$ for any $x$ in the unit
ball of $\R^d$, it means that $\rho(x)$ indeed lies in $\ell_2$.

TODO
