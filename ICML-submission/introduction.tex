\section{Introduction}
\label{section:introduction}

We study the problem of \textsc{Online Multiclass Linear Classification with
Bandit Feedback}~\citep{Kakade-Shalev-Shwartz-Tewari-2008}. The problem can be
viewed as a repeated game between the learner and the adversary. At each
timestep $t$, the adversary chooses a labeled example $(x_t, y_t)$ where the
feature vector $x_t \in \R^d$ is revealed to the learner and the correct label
$y_t$ is kept secret by the adversary. Upon receiving the feature vector $x_t$,
the learner is asked to make a prediction $\widehat{y}_t$. Immediately after
making its prediction, the learner receives feedback. In contrast with the
standard full-information setting (where the feedback given is the correct label
$y_t$), here the feedback is only a binary indicator of whether the prediction
was correct or not. The protocol of the problem is formally stated below.

\begin{protocol}[h]
\caption{\textsc{Online Multiclass Linear Classification with Bandit Feedback}
\label{algorithm:game-protocol}}
\begin{algorithmic}[1]
{
\REQUIRE{Number of classes $K$, number of rounds $T$.}
\FOR{$t=1,2,\dots,T$}
\STATE Adversary chooses example $(x_t, y_t)$, where $x_t$ is revealed to the learner.
\STATE{Predict class label $\widehat y_t \in \{1,2,\dots,K\}$.}
\STATE{Observe feedback $z_t \in \{0,1\}$, \\ \qquad where $z_t = \begin{cases}
0 & \text{if $\widehat y_t = y_t$,} \\
1 & \text{if $\widehat y_t \neq y_t$.}
\end{cases}$
}
\ENDFOR
}
\end{algorithmic}
\end{protocol}

The performance of the learner is measured by its cumulative number of
mistakes $\sum_{t=1}^T z_t = \sum_{t=1}^T \indicator{\widehat y_t \neq y_t}$.

In this paper, we focus on the special case when the examples chosen by the
adversary lie in $\R^d$ and are linearly separable with a margin. We introduce
two notions of linear separability, \emph{weak} and \emph{strong}, formally
stated in
Definitions~\ref{definition:weak-linear-separability}~and~\ref{definition:strong-linear-separability}.
The standard notion of multiclass linear
separability~\citep{Crammer-Singer-2003} corresponds to the weak linear
separability. For binary classification, the weak and strong separability are
identical. However, they differ for multiclass classification with $K \ge 3$
classes. For multiclass classification with $K$ classes, weak linear
separability means that the examples from each class lie in an intersection of
$K-1$ halfspaces and the examples outside of the class lie in the complement of
the intersection of the halfspaces. Strong linear separability means that
examples from each class are separated from the remaining examples by a
\emph{single} hyperplane.

In the full-information feedback setting, it is well known
\citep{Crammer-Singer-2003} that if all examples have norm at most $R$ and are
weakly linearly separable with a margin $\gamma$, then the \textsc{Multiclass
Perceptron} algorithm makes at most $\lfloor 2(R/\gamma)^2 \rfloor$ mistakes.
The guarantees provided by \textsc{Multiclass Perceptron} is very satisfying,
since it achieves an information-theoretically optimal mistake upper bound,
while having low time and memory complexities.~\footnote{These are
folklore results. For completeness, we present these results along
with their proofs in Appendix~\ref{section:multiclass-perceptron-proofs} in the
supplementary material.}

The bandit feedback setting, however, is much more challenging. For the case
when the examples are strongly linearly separable, to the best of our knowledge,
it is not known how to design an efficient algorithm with a finite mistake bound
before our work.~\footnote{Although~\cite{Chen-Chen-Zhang-Chen-Zhang-2009}
claimed that their Conservative OVA algorithm with PA-I update has a finite
mistake bound under the strong linear separability condition, their proof of
Theorem 2 is incorrect: their Lemma 4 cannot be directly applied to the bandit
setting, as the algorithm only makes updates on a subset of the $w_i$'s.} We
design a simple and efficient algorithm
(Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples})
that makes at most $O(K (R/\gamma)^2))$ mistakes in expectation. Its memory
complexity and per-round time complexity are both $O(dK)$. The algorithm can be
viewed as running $K$ copies of the \textsc{Binary Perceptron} algorithm, one
copy for each class. We prove that any (possibly randomized) algorithm must make
$\Omega(K (R/\gamma)^2))$ mistakes in the worst case. The extra $O(K)$
multiplicative factor the mistake bound, as compared to the full-information
setting, is the price we pay for the bandit feedback, or more precisely, the
lack of full-information feedback.

For the case when the examples are weakly linearly separable, we construct a
non-linear feature mapping $\phi$ and associated positive definite kernel
$k(x,x')$ that makes the examples \emph{strongly} linearly separable. We then
use the kernelized version of
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} for
the strongly separable case (Algorithm~\ref{algorithm:kernelized}). The kernel
$k(x,x')$ corresponding to the feature mapping $\phi$ has a simple explicit
formula and can be computed in $O(d)$ time, making
Algorithm~\ref{algorithm:kernelized} computationally efficient. For details on
kernel methods see e.g.~\citet{Scholkopf-Smola-2002} or
\citet{Shawe-Taylor-Cristianini-2004}.

The number of mistakes of the kernelized algorithm depends on the margin in the
corresponding feature space. We analyze how the mapping $\phi$ transforms the
margin parameter of weak separability in the original space $\R^d$ into a margin
parameter of strong separability in the new feature space. This problem is
related to the problem of learning intersection of halfspaces and has been
studied previously by \citet{Klivans-Servedio-2008}. As a side result, we
improve on the results of \citet{Klivans-Servedio-2008} by removing the
dependency on the original dimension $d$.

The resulting kernelized algorithm runs in time polynomial in the
original dimension of the feature vectors $d$, the number of classes $K$, and
the number of rounds $T$. We prove that if the examples lie in the unit ball of
$\R^d$ and are weakly linearly separable with margin $\gamma$,
Algorithm~\ref{algorithm:kernelized} makes at
most $2^{\widetilde{O}(\min(K \log^2 \frac{1}{\gamma}, \sqrt{\frac{1}{\gamma}}
\log K))}$ mistakes.
