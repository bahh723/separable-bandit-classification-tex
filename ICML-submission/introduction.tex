\section{Introduction}
\label{section:introduction}

We study the problem of \textsc{Online Multiclass Linear Classification with
Bandit Feedback}~\citep{Kakade-Shalev-Shwartz-Tewari-2008}. The problem can be
viewed as a repeated game between the learner and the adversary. At each
timestep $t$, the adversary chooses a labeled example $(x_t, y_t)$ where the
feature vector $x_t \in \R^d$ is revealed to the learner and the correct label
$y_t$ is kept secret by the adversary. Upon receiving the feature vector $x_t$,
the learner is asked to make a prediction $\widehat{y}_t$. Immediately after
making its prediction, the learner receives a feedback. In contrast with the
standard full information setting (where the feedback given is the correct label
$y_t$), here the feedback is only a binary indicator of whether the prediction
was correct or not. The protocol of the problem is formally stated below.

\begin{protocol}[h]
\caption{\textsc{Online Multiclass Classification with Bandit Feedback}
\label{algorithm:game-protocol}}
\begin{algorithmic}[1]
{
\REQUIRE{Number of classes $K$, number of rounds $T$.}
\FOR{$t=1,2,\dots,T$}
\STATE Adversary chooses example $(x_t, y_t)$, where $x_t$ is revealed to the learner.
\STATE{Predict class label $\widehat y_t \in \{1,2,\dots,K\}$}
\STATE{Observe feedback $z_t \in \{0,1\}$ \\ \qquad where $z_t = \begin{cases}
0 & \text{if $\widehat y_t = y_t$,} \\
1 & \text{if $\widehat y_t \neq y_t$.}
\end{cases}$
}
\ENDFOR
}
\end{algorithmic}
\end{protocol}

The performance of the learner is measured by its cumulative number of
mistakes $\sum_{t=1}^T z_t = \sum_{t=1}^T \indicator{\widehat y_t \neq y_t}$.
The bandit feedback is a natural feedback model motivated by applications
including online advertising and online content optimization.

In this paper, we focus on the special case when the examples chosen by the
adversary lie in $\R^d$ and are linearly separable with a margin. We introduce
two notions of linear separability, \emph{weak} and \emph{strong}. These are
formally stated in
Definitions~\ref{definition:weak-linear-separability}~and~\ref{definition:strong-linear-separability}.
The standard notion of multiclass linear separability corresponds to the weak
linear separability. For binary classification, the weak and strong separability
are identical. However, they differ for multiclass classification with $K \ge 3$
classes. For multiclass classification with $K$ classes, the standard notion of
linear separability means that the examples from each class lie in an
intersection of $K-1$ halfspaces and the examples outside of the class lie in
the complement of the intersection of the halfspaces. Strong linear separability
means that examples from each class are separated from the remaining examples by
a \emph{single} hyperplane.

In the full-information feedback setting, it is well known that if the examples
have norm at most $R$ and are weakly linearly separable with a margin $\gamma$
then the \textsc{Multiclass Perceptron} algorithm makes at most $\lfloor
2(R/\gamma)^2 \rfloor$ mistakes. This result is very satisfying since the upper
bound on the number of mistakes is information-theoretically optimal and at the
same time the \textsc{Multiclass Perceptron} algorithm has low time and memory
complexity.

The bandit feedback setting, however, is much more challenging. For the case
when the examples are strongly linearly separable, we design a simple and
efficient algorithm with an expected number of mistakes at most $(K-1) \lfloor
4(R/\gamma)^2 \rfloor$. The algorithm can be viewed as running $K$ copies of the
\textsc{Binary Perceptron} algorithm, one copy for each class. Intuitively
speaking, the factor $K-1$ is the price we pay for the bandit feedback, or more
precisely, the lack of full-information feedback. We also prove that any
(possibly randomized) algorithm makes $\Omega(K (R/\gamma)^2))$ mistakes in the
worst case.

For the case when examples are weakly linearly separable, we construct a feature
mapping $\phi$ such that makes them \emph{strongly} linearly separable. We then
use the kernelized version of the algorithm for the strongly separable case; for
more details on kernel methods see e.g.~\citet{Scholkopf-Smola-2002} and
\citet{Shawe-Taylor-Cristianini-2004}. The kernel $k(x,y)$ corresponding to the
feature mapping $\phi$ has a simple explicit formula and can be computed in
$O(d)$ time.

The number of mistakes of the kernelized algorithm depends on the margin in the
corresponding feature space. We analyze how the margin parameter of weak
separability in the original space $\R^d$ gets transformed into a margin
parameter of strong separability in the new feature space. This problem is
related to the problem of learning intersection of halfspaces and has been
studied previously by \citet{Klivans-Servedio-2008}. In fact, our analysis can
be viewed as a follow up on their work. We improve on the results of
\citet{Klivans-Servedio-2008} by removing the dependency on the original
dimension $d$.

The resulting kernelized algorithm runs in time that is polynomial in the
original dimension of the feature vectors $d$, the number of classes $K$, and
the number of rounds $T$. We prove that if the examples lie in unit ball of
$\R^d$ and are weakly separable with margin $\gamma$, the algorithm makes at
most $O(???)$ mistakes.
