\section{Introduction}
\label{section:introduction}

We study the problem of \textsc{Online Multiclass Linear Classification with
Bandit Feedback}~\citep{Kakade-Shalev-Shwartz-Tewari-2008}. The problem can be
viewed as a repeated game between a learner and an adversary. At each time step
$t$, the adversary chooses a labeled example $(x_t, y_t)$ and reveals $x_t\in
\R^d$ to the learner. Upon receiving $x_t$, the learner makes a prediction
$\widehat{y}_t$ and receives feedback. In contrast with the standard
full-information setting, where the feedback given is the correct label $y_t$,
here the feedback is only a binary indicator of whether the prediction was
correct or not. The protocol of the problem is formally stated below.

\begin{protocol}[h]
\caption{\textsc{Online Multiclass Linear Classification with Bandit Feedback}
\label{algorithm:game-protocol}}
\textbf{Require:} Number of classes $K$, number of rounds $T$. \\
\textbf{Require:} Inner product space $(V,\ip{\cdot}{\cdot})$. \\
\For{$t=1,2,\dots,T$}{
Adversary chooses example $(x_t, y_t) \in V \times \{1,2,\dots,K\}$ where $x_t$ is revealed to the learner.\\
Predict class label $\widehat y_t \in \{1,2,\dots,K\}$.\\
Observe feedback $z_t = \indicator{\widehat y_t \neq y_t} \in \{0,1\}$
}
\end{protocol}

The performance of the learner is measured by its cumulative number of
mistakes $\sum_{t=1}^T z_t = \sum_{t=1}^T \indicator{\widehat y_t \neq y_t}$,
where $\one$ denotes the indicator function.

In this paper, we focus on the special case when the examples chosen by the
adversary lie in $\R^d$ and are linearly separable with a margin. We introduce
two notions of linear separability, \emph{weak} and \emph{strong}, formally
stated in \autoref{definition:linear-separability}. The standard notion of
multiclass linear separability~\citep{Crammer-Singer-2003} corresponds to the
weak linear separability. For multiclass classification with $K$ classes, weak
linear separability requires that all examples from the same class lie in an
intersection of $K-1$ halfspaces and all other examples lie in the complement of
the intersection of the halfspaces. Strong linear separability means that
examples from each class are separated from the remaining examples by a
\emph{single} hyperplane.

In the full-information feedback setting, it is well known
\citep{Crammer-Singer-2003} that if all examples have norm at most $R$ and are
weakly linearly separable with a margin $\gamma$, then the \textsc{Multiclass
Perceptron} algorithm makes at most $\lfloor 2(R/\gamma)^2 \rfloor$ mistakes. It
is also known that any (possibly randomized) algorithm must make $\frac{1}{2}
\left\lfloor (R/\gamma)^2 \right \rfloor$ mistakes in the worst case. The
\textsc{Multiclass Perceptron} achieves an information-theoretically optimal
mistake bound, while being computationally and memory efficient.~\footnote{For
completeness, we present these folklore results along with their proofs in
Appendix~\ref{section:multiclass-perceptron-proofs} in the supplementary
material.}

The bandit feedback setting, however, is much more challenging. For the case
when the examples are strongly linearly separable, to the best of our knowledge,
it is not known how to design an efficient algorithm with a finite mistake bound
before our work.~\footnote{Although~\cite{Chen-Chen-Zhang-Chen-Zhang-2009}
claimed that their Conservative OVA algorithm with PA-I update has a finite
mistake bound under the strong linear separability condition, their
Theorem 2 is incorrect: first, their Lemma 1 (with $C = +\infty$) along with Theorem 1
implies a mistake upper bound of
$(\frac{R}{\gamma})^2$, which contradicts our lower
bound in Theorem~\ref{theorem:strongly-separable-examples-mistake-lower-bound};
second, their Lemma 1 cannot be directly applied to the bandit setting, as the
algorithm only makes updates on a subset of the $w_i$'s.} We design a simple and
efficient algorithm
(Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples})
that makes at most $O(K (R/\gamma)^2)$ mistakes in expectation. Its memory
complexity and per-round time complexity are both $O(dK)$. The algorithm can be
viewed as running $K$ copies of the \textsc{Binary Perceptron} algorithm, one
copy for each class. We prove that any (possibly randomized) algorithm must make
$\Omega(K (R/\gamma)^2)$ mistakes in the worst case. The extra $O(K)$
multiplicative factor in the mistake bound, as compared to the full-information
setting, is the price we pay for the bandit feedback, or more precisely, the
lack of full-information feedback.

For the case when the examples are weakly linearly separable, it was open for a
long time whether there exist \textit{efficient} algorithms with finite mistake
bound~\cite{Kakade-Shalev-Shwartz-Tewari-2008, Beygelzimer-Orabona-Zhang-2017}.
Furthermore, \citet{Kakade-Shalev-Shwartz-Tewari-2008} ask the question:
Is there \textit{any} algorithm with a finite mistake bound that has no explicit
dependence on the dimensionality of the feature vectors? We answer both
questions affirmatively by providing an efficient algorithm with finite
dimensionless mistake bound (Algorithm~\ref{algorithm:kernelized}).\footnote{An
in inefficient algorithm was given by~\cite{Daniely-Helbertal-2013}.}

The strategy used in Algorithm~\ref{algorithm:kernelized} is to construct a
non-linear feature mapping $\phi$ and associated positive definite kernel
$k(x,x')$ that makes the examples \emph{strongly} linearly separable in a
higher-dimensional space. We then use the kernelized version of
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} for
the strongly separable case. The kernel $k(x,x')$ corresponding to the feature
mapping $\phi$ has a simple explicit formula and can be computed in $O(d)$ time,
making Algorithm~\ref{algorithm:kernelized} computationally efficient. For
details on kernel methods see e.g.~\citet{Scholkopf-Smola-2002} or
\citet{Shawe-Taylor-Cristianini-2004}.

The number of mistakes of the kernelized algorithm depends on the margin in the
corresponding feature space. We analyze how the mapping $\phi$ transforms the
margin parameter of weak separability in the original space $\R^d$ into a margin
parameter of strong separability in the new feature space. This problem is
related to the problem of learning intersection of halfspaces and has been
studied previously by \citet{Klivans-Servedio-2008}. As a side result, we
improve on the results of \citet{Klivans-Servedio-2008} by removing the
dependency on the original dimension $d$.

The resulting kernelized algorithm runs in time polynomial in the
original dimension of the feature vectors $d$, the number of classes $K$, and
the number of rounds $T$. We prove that if the examples lie in the unit ball of
$\R^d$ and are weakly linearly separable with margin $\gamma$,
Algorithm~\ref{algorithm:kernelized} makes at
most $2^{\widetilde{O}(\min(K \log^2 (1/\gamma), \sqrt{1/\gamma}
\log K))}$ mistakes.

In Appendix~\ref{section:nearest-neighbor-algorithm}, we propose and analyze a
very different algorithm for weakly separable data. The algorithm is based on
the obvious idea that two points that are close enough must have the same label.
%\chicheng{I wonder if the mistake bound of this algorithm is too large to be included
%as ``our contributions'' - perhaps we should only mention it as a folklore/baseline?}

Finally, we study two questions related to the computational and
information-theoretic hardness of the problem. Any algorithm for the bandit
setting collects information in the form of so called \emph{strongly labeled}
and \emph{weakly labeled} examples. Strongly labeled examples are those for
which we know the class label. Weakly labeled example is an example for which we
know that class label can be anything except for a particular one class. In
Appendix~\ref{section:np-hardness-of-weak-labeling-problem}, we show that the
offline problem of finding a multiclass linear classifier consistent with a set
of strongly and weakly labeled examples is NP-hard. In
Appendix~\ref{section:mistake-lower-bound-for-ignorant-algorithms}, we prove a
lower bound on the number of mistakes of any algorithm that uses only
strongly-labeled examples and ignores weakly labeled examples.
