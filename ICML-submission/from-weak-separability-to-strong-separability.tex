\section{From weak separability to strong separability}
\label{section:from-weak-separability-to-strong-separability}

In this section, we construct and analyze a mapping $\phi$ from the unit ball of
$\R^d$ into a infinite-dimensional inner product space that has the property
that if a set of labeled examples in the unit ball is \emph{weakly} linearly
separable with a margin $\gamma$ after applying the mapping $\phi$ the examples
become \emph{strongly} linearly separable with a margin $\gamma'$ and their
norms are bounded by $2$. The parameter $\gamma'$ is a function of the old
margin $\gamma$ and the number of classes $K$, and is specified in the
\autoref{theorem:margin-transformation} below.

The domain of $\phi$ is the unit ball of $\R^d$. The co-domain of $\phi$ is the
classical real separable Hilbert space $\ell_2 = \{ x \in \R^\infty ~:~
\sum_{i=1}^\infty x_i^2 < + \infty \}$ equipped with the standard inner product
$\ip{x}{x'}_{\ell_2} = \sum_{i=1}^\infty x_i x'_i$. We index the coordinates of
$\ell_2$ by $d$-tuples $(\alpha_1, \alpha_2, \dots, \alpha_d)$ of non-negative
integers. The mapping is defined by
\begin{align*}
& \left(\phi(x_1, x_2, \dots, x_d)\right)_{(\alpha_1, \alpha_2, \dots, \alpha_d)} = x_1^{\alpha_1} x_2^{\alpha_2} \dots x_d^{\alpha_d} \\
& \qquad \cdot \sqrt{2^{-(\alpha_1 + \alpha_2 + \dots + \alpha_d)} \binom{\alpha_1 + \alpha_2 + \dots + \alpha_d}{\alpha_1, \alpha_2, \dots, \alpha_d}}
\end{align*}
where $\binom{\alpha_1 + \alpha_2 + \dots + \alpha_d}{\alpha_1, \alpha_2, \dots, \alpha_d} = \frac{(\alpha_1 + \alpha_2 + \dots + \alpha_d)!}{\alpha_1! \alpha_2! \dots \alpha_d!}$
is the multinomial coefficient. The corresponding kernel is
$$
k(x,x') = \ip{\phi(x)}{\phi(x')}_{\ell_2} = \frac{1}{1 - \frac{1}{2}\ip{x}{x'}_{\R^d}} \; .
$$
We note that the formula for $k$ implies that $k(x,x) < +\infty$ for any $x$ in
the unit ball of $\R^d$ and therefore $\phi(x)$ lies in $\ell_2$. We also note
that kernel function $k(x,x')$ can be evaluated in $O(d)$ time.

We derive a mistake bound for Algorithm~\ref{algorithm:kernelized} with kernel
function $k(x,x') = \ip{\phi(x)}{\phi(x')}_{\ell_2}$ by simply invoking
\autoref{theorem:strongly-separable-examples-mistake-upper-bound} with the
modified margin $\gamma'$ from \autoref{theorem:margin-transformation} below.
The mistake bound is stated as \autoref{corollary:weakly-separable-examples-mistake-upper-bound}.

The idea behind construction and analysis of the mapping $\phi$ is polynomial
approximation. According to the well known Stone-Weierstrass theorem (see
e.g.~\citet[Section~10.10]{Davidson-Donsig-2010}), on a compact set,
multivariate polynomials uniformly approximate any continuous function.
Intuitively speaking, we use a multivariate polynomial to approximate, on the
unit ball of $\R^d$, the indicator function corresponding to the intersection of
$m=K-1$ halfspaces. Within margin $\gamma$ along the decision boundary, we allow
the polynomial to attain arbitrary value. The polynomial separates examples in
one class from examples in the other classes. To be able to quantify $\gamma'$
we need to quantify certain parameters of the approximating polynomial. We
construct two different polynomials with different parameters. The parameters
are quantified in
Theorems~\ref{theorem:polynomial-approximation-1}~and~\ref{theorem:polynomial-approximation-2}
stated below.

Before we state the theorems, recall that a polynomial of $d$ variables is a
function $p:\R^d \to \R$ of the form
\begin{align*}
p(x)
& = p(x_1, x_2, \dots, x_d) \\
& = \sum_{\alpha_1, \alpha_2, \dots, \alpha_d} c_{\alpha_1, \alpha_2, \dots, \alpha_d} x_1^{\alpha_1} x_2^{\alpha_2} \dots x_d^{\alpha_d}
\end{align*}
where the sum ranges over a finite set of $d$-tuples $(\alpha_1, \alpha_2,
\dots, \alpha_d)$ of non-negative integers and $c_{\alpha_1, \alpha_2, \dots,
\alpha_d}$ is a real coefficient. The \emph{degree} of a polynomial $p$, denoted
by $\deg(p)$, is the largest value of $\alpha_1 + \alpha_2 + \dots + \alpha_d$
for which the coefficient $c_{\alpha_1, \alpha_2, \dots, \alpha_d}$ is non-zero.
The \emph{norm of a polynomial} $p$ is defined as
$$
\norm{p} = \sqrt{\sum_{\alpha_1, \alpha_2, \dots, \alpha_d} \left(c_{\alpha_1, \alpha_2, \dots, \alpha_d} \right)^2 } \; .
$$
It is easy see that this is indeed a norm, since we can interpret it as a
Euclidean norm of the vector of the coefficients of the polynomial.

\begin{theorem}[Polynomial approximation of intersection of halfspaces I]
\label{theorem:polynomial-approximation-1}
Let $v_1, v_2, \dots, v_m \in \R^d$ be vectors such that $\norm{v_1},
\norm{v_2}, \dots, \norm{v_m} \le 1$. Let $\gamma \in (0,1)$. There exists a
multivariate polynomial $p:\R^d \to \R$ such that
\begin{enumerate}
\item $p(x) \ge 1/2$ \\ for all $\displaystyle x \in \bigcap_{i=1}^m \left\{ x \in \R^d ~:~ \norm{x} \le 1, \ \ip{v_i}{x} \ge \gamma \right\}$
\item $p(x) \le -1/2$ \\ for all $\displaystyle x \in \bigcup_{i=1}^m \left\{ x \in \R^d ~:~ \norm{x} \le 1, \ \ip{v_i}{x} \le - \gamma \right\}$
\item $\displaystyle \deg(p) = \left\lceil \log_2(2m) \right\rceil \cdot \left\lceil \sqrt{\frac{1}{\gamma}} \right\rceil$
\item $\displaystyle \norm{p} \le \left( 188 \left\lceil \log_2(2m) \right\rceil \cdot \left\lceil \sqrt{\frac{1}{\gamma}} \right\rceil \right)^{\frac{1}{2} \left\lceil \log_2(2m) \right\rceil \cdot \left\lceil \sqrt{\frac{1}{\gamma}} \right\rceil}$
\end{enumerate}
\end{theorem}

\begin{theorem}[Polynomial approximation of intersection of halfspaces II]
\label{theorem:polynomial-approximation-2}
Let $v_1, v_2, \dots, v_m \in \R^d$ be vectors such that $\norm{v_1},
\norm{v_2}, \dots, \norm{v_m} \le 1$. Let $\gamma \in (0,1)$.
Define
$$
r = 2 \left\lceil \frac{1}{4} \log_2(4m + 1) \right\rceil + 1 \quad \text{and} \quad s = \left \lceil \log_2(1/\gamma) \right \rceil \; .
$$
Then, there exists a multivariate polynomial $p:\R^d \to \R$ such that
\begin{enumerate}
\item $\displaystyle p(x) \ge \frac{1}{4} \cdot 2^{s(s+1)rm}$ \\
for all $\displaystyle x \in \bigcap_{i=1}^m \left\{ x \in \R^d ~:~ \norm{x} \le 1, \ \ip{v_i}{x} \ge \gamma \right\}$

\item $\displaystyle p(x) \le - \frac{1}{4} \cdot 2^{s(s+1)rm}$ \\
for all $\displaystyle x \in \bigcup_{i=1}^m \left\{ x \in \R^d ~:~ \norm{x} \le 1, \ \ip{v_i}{x} \le - \gamma \right\}$

\item $\deg(p) \le (2s+1) rm$
\item $\norm{p} \le (2m-1/2) 2^m \cdot \left(2^{2s} rm (4s+2)^2 \right)^{(s+1/2)rm}$
\end{enumerate}
\end{theorem}

The proofs of the theorems can be found in
Section~\ref{section:proof-of-polynomial-approximation}. The geometric
interpretation of the two regions described in parts 1 and 2 of the theorems is
explained in Figure~\ref{figure:pizza-slice}. Similar but weaker results were
proved by~\citet{Klivans-Servedio-2008}. In particular, our bounds in parts 1,
2, 3, 4 of
Theorems~\ref{theorem:polynomial-approximation-1}~and~\ref{theorem:polynomial-approximation-2}
are independent of the dimension $d$.

\begin{figure}
\begin{center}
\input{figures/pizza-slice}
\end{center}
\caption[]{The figure shows the two regions used in parts 1 and 2 of
Theorems~\ref{theorem:polynomial-approximation-1}~and~\ref{theorem:polynomial-approximation-2}
for the case $m=d=2$ and a particular choice of vectors $v_1, v_2$ and margin
parameter $\gamma$. These regions are
$R^+ = \displaystyle \bigcap_{i=1}^m \left\{ x \in \R^d ~:~ \norm{x} \le 1, \ \ip{v_i}{x} \ge \gamma \right\}$,
$R^- = \displaystyle \bigcup_{i=1}^m \left\{ x \in \R^d ~:~ \norm{x} \le 1, \ \ip{v_i}{x} \le - \gamma \right\}$
The separating hyperplanes $\ip{v_1}{x} = 0$ and $\ip{v_2}{x} = 0$ are shown as dashed lines.}
\label{figure:pizza-slice}
\end{figure}



The following lemma expresses any multivariate polynomial in $\R^d$
as a linear function in $\ell_2$ and gives an upper bound its the norm.

\begin{lemma}[Norm bound]
\label{lemma:norm-bound}
Let $p:\R^d \to \R$ be a multivariate polynomial.
There exists $c \in \ell_2$ such that $p(x) = \ip{c}{\phi(x)}_{\ell_2}$
and $\norm{c}_{\ell_2} \le 2^{\deg(p)/2} \norm{p}$.
\end{lemma}

\begin{proof}
Note that the polynomial $p$ can be written as
$p(x) = \sum_{\alpha_1, \alpha_2, \dots, \alpha_d} c'_{\alpha_1, \alpha_2, \dots, \alpha_d} x_1^{\alpha_1} x_2^{\alpha_2} \dots x_d^{\alpha_d}$.
We define $c \in \ell_2$ using the multi-index notation as
$$
c_{\alpha_1, \alpha_2, \dots, \alpha_d}
= \frac{c'_{\alpha_1, \alpha_2, \dots, \alpha_d} 2^{(\alpha_1 + \alpha_2 + \dots + \alpha_d)/2}}{\sqrt{\binom{\alpha_1 + \alpha_2 + \dots + \alpha_d}{\alpha_1, \alpha_2, \dots, \alpha_d}}}
$$
for all tuples $(\alpha_1, \alpha_2, \dots, \alpha_d)$ such that $\alpha_1 + \alpha_2 + \dots + \alpha_d \le \deg(p)$.
Otherwise, we define $c_{\alpha_1, \alpha_2, \dots, \alpha_d} = 0$. Clearly,
$\ip{c}{\phi(x)}_{\ell_2} = p(x)$.

Since
\begin{align*}
|c_{\alpha_1, \alpha_2, \dots, \alpha_d}|
& \le 2^{(\alpha_1 + \alpha_2 + \dots + \alpha_d)/2} |c'_{\alpha_1, \alpha_2, \dots, \alpha_d}| \\
& \le 2^{\deg(p)/2} |c'_{\alpha_1, \alpha_2, \dots, \alpha_d}| \; ,
\end{align*}
we have
\begin{align*}
\norm{c}_{\ell_2}
& \le 2^{\deg(p)/2} \sqrt{\sum_{\alpha_1, \alpha_2, \dots, \alpha_d} (c'_{\alpha_1, \alpha_2, \dots, \alpha_d})^2} \\
& = 2^{\deg(p)/2} \norm{p} \; . \qquad \qedhere
\end{align*}
\end{proof}

Using the lemma and the polynomial approximation theorems, we can prove the
mapping $\phi$ maps any a weakly separable data to a strongly separable data
set.

\begin{theorem}[Margin transformation]
\label{theorem:margin-transformation}
Let $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T) \in \R^d \times \{1,2,\dots,K\}$
labeled examples that are weakly linearly separable with margin $\gamma > 0$
such that $\norm{x_1}, \norm{x_2}, \dots, \norm{x_T} \le 1$.
Let
\begin{align*}
r & = 2 \left\lceil \frac{1}{4} \log_2(4K-3) \right\rceil + 1 \quad \text{and} \quad s = \left \lceil \log_2(2/\gamma) \right \rceil \\
\gamma_1 & = \frac{1}{2\sqrt{K}}  \\
& \ \cdot \left(376 \lceil \log_2(2m) \rceil \cdot \left \lceil \sqrt{\frac{2}{\gamma}} \right \rceil \right)^{-\frac{1}{2} \lceil \log_2(2K-2) \rceil \cdot \left \lceil \sqrt{\frac{2}{\gamma}} \right \rceil} \; , \\
\gamma_2 & = \frac{2^{s(s+1)r(K-1)} }{4(2K-5/2) 2^{K-1}} \\
& \ \cdot \left(2^{2s+1} r(K-1) (4s+2)^2 \right)^{-(s+1/2)r(K-1)} \; .
\end{align*}
Then, the labeled examples $(\phi(x_1), y_1), (\phi(x_2), y_2), \dots,
(\phi(x_T), y_T)$ are strongly linearly separable with margin $\gamma' =
\max\{\gamma_1, \gamma_2\}$ and $\norm{\phi(x_1)}_{\ell_2},
\norm{\phi(x_2)}_{\ell_2}, \dots, \norm{\phi(x_T)}_{\ell_2} \le 2$.
\end{theorem}

\begin{proof}
First osberve that for any $t=1,2,\dots,T$,
$$
\norm{\phi(x_t)}_{\ell_2} = k(x_t,x_t) = \frac{1}{1 - \frac{1}{2} \norm{x_t}^2} \le 2 \; .
$$

Since the examples $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T)$ are weakly
linearly separable with margin $\gamma$ there are vectors $w_1, w_2, \dots, w_K$
satisfying \eqref{equation:weak-linear-separability-1} anf
\eqref{equation:weak-linear-separability-2}.

Fix any $i \in \{1,2,\dots,K\}$. Consider the $K-1$ vectors $(w_i - w_j)/2$ for
$j \in \{1,2,\dots,K\} \setminus \{i\}$. Note that the vectors have norm at most
$1$.
Theorems~\ref{theorem:polynomial-approximation-1}~and~\ref{theorem:polynomial-approximation-2}
imply that there exists multivariate polynomials $p_i:\R^d \to \R$ and $q_i:\R^d
\to \R$ such that
\begin{align*}
\deg(p_i) & = \lceil \log_2(2K-2) \rceil \cdot \left\lceil \sqrt{\frac{2}{\gamma}} \right\rceil \; , \\
\deg(q_i) & = (2s+1) r(K-1) \; .
\end{align*}
Futhermore, for all $t=1,2,\dots,T$, if $y_t = i$ then $p_i(x_t) \ge 1/2$,
$q_i(x_t) \ge \frac{1}{4} \cdot 2^{s(s+1)r(K-1)}$ and if $y_t \neq i$ then
$p_i(x_t) \le -1/2$, $q_i(x) \le - \frac{1}{4} \cdot 2^{s(s+1)r(K-1)}$ and
\begin{align*}
\norm{p_i} & \le \left(188 \lceil \log_2(2K-2) \rceil \cdot \left \lceil \sqrt{\frac{2}{\gamma}} \right \rceil \right)^{\frac{1}{2} \lceil \log_2(2K-2) \rceil
\cdot \left \lceil \sqrt{\frac{2}{\gamma}} \right \rceil} \; , \\
\norm{q_i} & \le (2K-5/2) 2^{K-1} \\
& \qquad \cdot \left(2^{2s} r(K-1) (4s+2)^2 \right)^{(s+1/2)r(K-1)} \; .
\end{align*}
By \autoref{lemma:norm-bound} there exists $c_i, c_i' \in \ell_2$ such that
$\ip{c_i}{\phi(x)} = p_i(x)$ and $\ip{c_i'}{\phi(x)} = q_i(x)$ and
\begin{align*}
\norm{c_i}_{\ell_2}
& \le \left(376 \lceil \log_2(2K-2) \rceil \cdot \left \lceil \sqrt{\frac{2}{\gamma}} \right \rceil \right)^{\frac{1}{2} \lceil \log_2(2K-2) \rceil
\cdot \left \lceil \sqrt{\frac{2}{\gamma}} \right \rceil} \\
\norm{c'_i}_{\ell_2} & \le (2K-5/2) 2^{K-1} \\
& \qquad \cdot \left(2^{2s+1} r(K-1) (4s+2)^2 \right)^{(s+1/2)r(K-1)} \; .
\end{align*}
Define vectors $u_i, u_i' \in \ell_2$ as
\begin{align*}
u_i & = \frac{c_i}{\sqrt{K} \left(376 \lceil \log_2(2K-2) \rceil \cdot \left \lceil \sqrt{\frac{2}{\gamma}} \right \rceil \right)^{\frac{1}{2} \lceil \log_2(2K-2) \rceil
\cdot \left \lceil \sqrt{\frac{2}{\gamma}} \right \rceil}} \; . \\
u_i' & = \frac{c_i'  \cdot \left(2^{2s+1} r(K-1) (4s+2)^2 \right)^{-(s+1/2)r(K-1)}}{(2K-5/2) 2^{K-1}} \; .
\end{align*}

Then, $\norm{u_1}^2 + \norm{u_2}^2 + \dots + \norm{u_K}^2 \le 1$ and
$\norm{u_1'}^2 + \norm{u_2'}^2 + \dots + \norm{u_K'}^2 \le 1$.
Futhermore, for all $t=1,2,\dots,T$, $\ip{u_{y_t}}{x_t} \ge \gamma_1$ and
$\ip{u'_{y_t}}{x_t} \ge \gamma_2$
and for all $j \in \{1,2,\dots,K\} \setminus \{y_t\}$,
$\ip{u_j}{x_t} \le - \gamma_1$ and $\ip{u'_j}{x_t} \le - \gamma_2$. In other words,
$(\phi(x_1), y_1), (\phi(x_2), y_2), \dots, (\phi(x_T), y_T)$ are
strongly linearly separable with margin $\gamma_1$ and also strongly linearly
separable with margin $\gamma_2$. Therefore, the examples are strongly separable
with margin $\gamma' = \max\{\gamma_1, \gamma_2\}$.
\end{proof}

As a corollary of \autoref{theorem:margin-transformation} and
\autoref{theorem:strongly-separable-examples-mistake-upper-bound}
we get the mistake bound for Algorithm~\ref{algorithm:kernelized}
under the weak separability condition.

\begin{corollary}[Mistake upper bound]
\label{corollary:weakly-separable-examples-mistake-upper-bound}
Let $K$ be a positive
integer, $\gamma$ be a positive real number, $R$ be a non-negative real number. If
$(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T)$ is a sequence of labeled examples in
$V \times \{1,2,\dots,K\}$ such that
\begin{enumerate}
  \item the examples are weakly linearly separable with margin $\gamma$ (Definition~\ref{definition:weak-linear-separability}),
  \item $\norm{x_1}, \norm{x_2}, \dots, \norm{x_T} \le 1$,
\end{enumerate}
then the expected number of mistakes
Algorithm~\ref{algorithm:kernelized}
makes is at most $2^{\widetilde{O}(\min(K \log^2
\frac{1}{\gamma}, \sqrt{\frac{1}{\gamma}} \log K))}$.
\end{corollary}
