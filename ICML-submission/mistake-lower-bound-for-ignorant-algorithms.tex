\section{Mistake lower bound for ignorant algorithms}
\label{section:mistake-lower-bound-for-ignorant-algorithms}

In this section, we prove a mistake lower bound for a family of algorithms
called \textit{ignorant algorithms}. Ignorant algorithms ignore the examples on
which they make mistakes. This assumption seems strong, but as we will explain
below, it is actually natural, and several recently proposed bandit
classification algorithms that achieve $\sqrt{T}$ regret bounds belong to
this family, e.g., SOBA~\citep{Beygelzimer-Orabona-Zhang-2017},
OBAMA~\citep{Foster-Kale-Luo-Mohri-Sridharan-2018}. Also,
\textsc{Nearest-Neighbor Algorithm} (Algorithm~\ref{algorithm:nearest-neighbor})
presented in Appendix~\ref{section:nearest-neighbor-algorithm} is an ignorant
algorithm.

Under the assumption that the examples lie in in the unit ball of $\R^d$ and are
weakly linearly separable with margin $\gamma$, we show that any ignorant
algorithm must make at least $\Omega \left( \left(\frac{1}{160 \gamma}\right)^{(d-2)/4} \right)$
mistakes in the worst case. In other words, an algorithm that achieves a better
mistake bound cannot ignore examples on which it makes a mistake and it must
make a meaningful update on such examples.

To formally define ignorant algorithms, we define the conditional distribution
from which an algorithm draws its predictions. Formally, given an algorithm
$\calA$ and an adversarial strategy, we define
\begin{multline*}
p_t(y|x) = \\
\Pr[y_t = y ~|~ (x_1, y_1), (x_2, y_2) \dots, (x_{t-1}, y_{t-1}), x_t = x] \; .
\end{multline*}
In other words, in any round $t$, conditioned on the past $t-1$ rounds, the
algorithm $\calA$ chooses $y_t$ from probability distribution $p_t(\cdot|x_t)$.
Formally, $p_t$ is a function $p:\{1,2,\dots,K\} \times \R^d \to [0,1]$
such that $\sum_{y=1}^K p_t(y|x) = 1$ for any $x \in \R^d$.

\begin{definition}[Ignorant algorithm]
An algorithm $\calA$ for \textsc{Online Multiclass Linear Classification with
Bandit Feedback} is called \emph{ignorant} if for every $t=1,2,\dots,T$,
$p_t$ is determined solely by the sequence
$(x_{a_1}, y_{a_1}), (x_{a_2}, y_{a_2}), \dots, (x_{a_n}, y_{a_n})$
in the rounds $1 \le a_1 < a_2 < \dots < a_n < t$ in which
the algorithm makes a correct prediction.
\end{definition}

An equivalent definition of an ignorant algorithm is that the memory state of
the algorithm does not change after it makes a mistake. Equivalently,
the memory state of an ignorant algorithm is completely determined
by the sequence of labeled examples on which it made correct prediction.

To explain the definition, consider an ignorant algorithm $\calA$. Suppose that
on a sequence of examples $(x_1, y_1), (x_2, y_2), \dots, (x_{t-1}, y_{t-1})$
generated by some adversary the algorthm $\calA$ makes correct predictions in
rounds $a_1, a_2, \dots, a_n$ where $1 \le a_1 < a_2 < \dots < a_n < t$ and
errors on rounds $\{1,2,\dots,t-1\} \setminus \{a_1, a_2, \dots, a_n\}$. Suppose
that on another sequence of examples $(x_1', y_1'), (x_2', y_2'), \dots,
(x_{s-1}', y_{s-1}')$ generated by another adversary the algorithm $\calA$ makes
correct predictions in rounds $b_1, b_2, \dots, b_n$ where $1 \le b_1 < b_2 <
\dots < b_n < s$ and errors on rounds $\{1,2,\dots,s-1\} \setminus \{b_1, b_2,
\dots, b_n\}$. Futhermore, suppose
\begin{align*}
(x_{a_1}, y_{a_1}) &= (x'_{b_1}, y'_{b_1}) \; , \\
(x_{a_2}, y_{a_2}) &= (x'_{b_2}, y'_{b_2}) \; , \\
\vdots \\
(x_{a_n}, y_{a_n}) &= (x'_{b_2}, y'_{b_n}) \; .
\end{align*}
Then, the definition
\begin{multline*}
\Pr[y_t = y ~|~ (x_1, y_1), (x_2, y_2) \dots, (x_{t-1}, y_{t-1}), x_t = x] = \\
\Pr[y_t' = y ~|~ (x_1', y_1'), (x_2', y_2') \dots, (x_{t-1}', y_{t-1}'), x_t' = x] \; .
\end{multline*}
Note that the sequences $(x_1, y_1)$, $(x_2, y_2)$, $\dots$, $(x_{t-1},
y_{t-1})$ and $(x_1', y_1')$, $(x_2', y_2')$, $\dots$, $(x_{s-1}', y_{s-1}')$
might have different lengths and and $\calA$ might error in different sets of
rounds. As a special case, if an ignorant algorithm makes a mistake in round $t$
then $p_{t+1}=p_t$.

Our main result is the following lower bound on the expected number of mistakes
for ignorant algorithms.

\begin{theorem}[Mistake lower bound for ignorant algorithms]
\label{theorem:ignorant-lower-bound}
Let $\gamma \in (0,1)$ and let $d$ be a positive integer. Suppose $\calA$ is an
ignorant algorithm for \textsc{Online Multiclass Linear Classification with
Bandit Feedback}. There exists $T$ and an adversary that sequentially chooses
labeled examples $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T) \in \R^d\times
\{1,2\}$ such that the examples are strongly linearly separable with magin
$\gamma$ and $\norm{x_1}, \norm{x_2}, \dots, \norm{x_T} \le 1$, and the expected
number of mistakes made by $\calA$ is at least
$$
\frac{1}{10} \left(\frac{1}{160\gamma}\right)^{\frac{d-2}{4}} \; .
$$
\end{theorem}

Before proving the theorem, we need the following lemma.

\begin{lemma}
\label{lemma:embed_d_gamma}
Let $\gamma \in (0,\frac{1}{160})$, let $d$ be a positive integer and let $N = (\frac{1}{2\sqrt{40\gamma}})^{d-2}$.
There exist vectors $u_1, u_2, \dots, u_N, v_1, v_2, \dots, v_N \in \R^d$ such that for all $i, j \in \{1,2,\dots,N\}$,
\begin{align*}
\norm{u_i} & \le 1 \; , \\
\norm{v_j} & \le 1 \; , \\
\ip{u_i}{v_j} & \ge \gamma & & \text{if $i=j$,} \\
\ip{u_i}{v_j} & \le -\gamma & & \text{if $i \neq j$.}
\end{align*}
\end{lemma}

\begin{proof}
By Lemma 6 of~\citet{Long-1995}, there exists vectors $z_1, z_2, \dots, z_N \in
\R^{d-1}$ such that $\norm{z_1} = \norm{z_2} = \dots = \norm{z_N} = 1$ and the
angle between the vectors is $\measuredangle(z_i, z_j) \ge \sqrt{40 \gamma}$ for
$i \neq j$, $i,j \in \{1,2,\dots,N\}$. Since $\cos\theta \le 1-\theta^2/5$ for
any $\theta \in [-\pi,\pi]$, this implies that
\begin{align*}
\ip{z_i}{z_j} &= 1 && \text{if $i = j$,} \\
\ip{z_i}{z_j} &\le 1 - 8\gamma && \text{if $i \neq j$.}
\end{align*}

Define $v_i = (\frac{1}{2} z_i, \frac{1}{2})$, and $u_i = (\frac{1}{2} z_i,
-\frac{1}{2}(1-4\gamma))$ for all $i \in \{1,2,\dots,N\}$. It can be easily
checked that for all $i$, $\norm{v_i} \le 1$ and $\norm{u_i} \le 1$.
Additionally,
$$
\ip{u_i}{v_j} = \frac{1}{4} \ip{z_i}{z_j} - \frac {1-4\gamma} 4 \; .
$$
Thus,
\begin{align*}
\ip{u_i}{v_j} &\ge \gamma && \text{if $i=j$,} \\
\ip{u_i}{v_j} &\le -\gamma && \text{if $i \neq j$.}
\end{align*}
\end{proof}

\begin{proof}
We consider the strategy for the adversary described in
Algorithm~\ref{algorithm:adversary-strategy}.

\begin{algorithm}
\caption{\textsc{Adversary's strategy}}
\label{algorithm:adversary-strategy}
\textbf{Define} $T=N$ and $v_1, v_2, \dots, v_N$ as in Lemma~\ref{lemma:embed_d_gamma}.\\
\textbf{Define} $q_0=\frac{1}{\sqrt{T}}$. \\
\textbf{Initialize} $\textsc{phase}= 1$. \\
\For{$t=1,2,\dots,T$}{
    \If{$\textsc{phase}=1$}{
       \If{$p_t(1|v_t)\ge 1-q_0$}{
          $(x_t, y_t)\leftarrow (v_t, 1)$
        }
       \Else{
          $(x_t, y_t)\leftarrow (v_t, 2)$ \\
          $\textsc{phase}\leftarrow 2$
       }
    }
    \Else{
         $(x_t, y_t)\leftarrow (x_{t-1}, y_{t-1})$
    }
}
\end{algorithm}

Define the indicators
\begin{align*}
A_t &= \indicator{\forall \tau\le t, p_\tau(1|x_\tau)<1-q_0} \\
B_t &= \indicator{\exists \tau\le t, p_\tau(1|x_\tau)\ge 1-q_0
 \text{\ and\ } \forall s\in[\tau,t), \widehat y_s \neq y_s}.
\end{align*}
Then we have
\begin{align}
& \mathbf{E}\left[\sum_{t=1}^{T} \indicator{\widehat y_t\neq y_t}\right] \nonumber \\
& \ge \mathbf{E}\left[\sum_{t=1}^T \indicator{\widehat y_t\neq y_t}A_t\right] \nonumber \\
& \qquad + \mathbf{E}\left[\sum_{t=1}^T \indicator{\widehat y_t\neq y_t}B_t\right] \nonumber \\
& = \mathbf{E}\left[\sum_{t=1}^T \mathbf{E}_t\left[\indicator{\widehat y_t\neq y_t}A_t\right]\right] + \mathbf{E}\left[\sum_{t=1}^T B_{t+1}\right] \nonumber \\
& \ge \mathbf{E}\left[\sum_{t=1}^T q_0 A_t\right] + \mathbf{E}\left[\sum_{t=1}^T B_{t+1}\right],
\label{equation:mistake_lower_bound_temp}
\end{align}
where in the first inequality we use the fact that $A_t$ and $B_t$ cannot be $1$
simultaneously; in the equality, we use $B_{t+1}=B_t\indicator{\widehat y_t\neq
y_t}$ by $B_t$'s definition; in the last inequality we use the fact that when
$t\le N$ and $A_t=1$, it must be $y_t=1$ and $\Pr[\widehat
y_t=y_t]=p_t(1|x_t)<1-q_0$. Now, denote $T_1 = \argmin_{\tau} \{
p_\tau(1|x_\tau) \ge 1-q_0 \}$ (if such $\tau$ does not exist or is larger than
$T$, we simply let $T_1=T+1$). Then $A_t=1$ for all $t\le
T_1-1$.

Note that $B_{T_1}=1$, and for $t \ge T_1$ the adversary will switch to
``$\textsc{phase}=2$'' and has $y_t=2$. Therefore,
$\Exp[B_{t+1}=0|B_{t}=1]=\Pr[\widehat y_t=y_t~|~B_t=1] = \Pr[\widehat
y_t=2|B_t=1].$ Note that when $B_t=1$, by definition there exists a $\tau\le t$
with $p_\tau(1|x_\tau)\ge 1-q_0$, and the algorithm all makes mistakes at $\tau,
\tau+1, \ldots, t$. Since the algorithm is ignorant, and when
$\textsc{phase}=2$, the features all remain the same, we have
$p_t(1|x_t)=p_{t-1}(1|x_{t-1})=\cdots=p_\tau(1|x_\tau)\ge 1-q_0$. Thus we can
further upper bound $\Exp[B_{t+1}=0|B_{t}=1]$ by $\Pr[\widehat y_t=2|B_t=1]\leq
q_0$. Thus the second term in \eqref{equation:mistake_lower_bound_temp} can be
calculated as

\begin{align*}
\mathbf{E}\left[\sum_{t=1}^T B_{t+1}\right]
& \ge \mathbf{E}\left[\sum_{t=T_1}^T (1-q_0)^{t-T_1+1} \right] \\
& = \frac{1-q_0}{q_0}\mathbf{E}\left[1-(1-q_0)^{T - T_1 + 1}\right] \; .
\end{align*}
Combining \eqref{equation:mistake_lower_bound_temp} and the above inequalities, we get
\begin{multline}
\label{equation:mistake_lower_bound_temp2}
\mathbf{E} \left[\sum_{t=1}^T \indicator{\widehat y_t\neq y_t}\right] \ge
\\ \mathbf{E}\left[q_0(T_1-1)+\frac{1-q_0}{q_0}\left(1-(1-q_0)^{T-T_1+1}\right)\right].
\end{multline}
In the case $T_1\ge \frac{1}{2}T + 1$, the right-hand side of
\eqref{equation:mistake_lower_bound_temp2} is lower bounded by
$\frac{1}{2}q_0 T = \frac{1}{2} \sqrt{T}$. In the case
$T_1< \frac{1}{2}T+1$, it is lower bounded by
\begin{align*}
& \frac{1-q_0}{q_0}\left(1-(1-q_0)^{\frac{1}{2}T}\right) \\
& = \frac{1-q_0}{q_0}\left(1-(1-q_0)^{\frac{1}{2q_0^2}}\right) \\
& \ge \frac{1-\frac{1}{\sqrt{2}}}{q_0}\left(1-\frac{1}{\sqrt{e}}\right) \\
& \ge \frac{1}{10} \sqrt{T} \; .
\end{align*}

Observe that in phase 1, the labels are equal to $1$ and in phase 2 the labels
are equal to $2$. Let $t$ be the round in which the switch happens. (If it never
happens, define $t=T+1$.) Note that $(x_t, y_t)=(x_{t+1}, y_{t+1})= \dots =
(x_T, y_T) = (v_t, 2)$. Consider the vectors $u_1, u_2, \dots, u_N$ as defined
in Lemma~\ref{lemma:embed_d_gamma} We claim that $w_1=-u_t/2$ and $w_2=u_t/2$
satisfy the conditions of strong linear separabaility.

Clearly $\norm{w_1}^2 + \norm{w_2}^2 \le (\norm{w_1} + \norm{w_2})^2 \le
(\frac{1}{2} + \frac{1}{2})^2 \le 1$. By Lemma~\ref{lemma:embed_d_gamma}, we
have $\ip{w_2/2}{x_s} = \ip{u_t/2}{v_t} \ge \gamma/2, \forall s \ge t$ and
$\ip{w_2/2}{x_s} = \ip{u_t/2}{v_s} \le - \gamma/2$ for all $s<t$. Similarly,
$\ip{w_1/2}{x_s} \le -\gamma/2$ for all $s \ge t$ and $\ip{w_1/2}{x_s} \ge
\gamma/2$ for all $s < t$. Thus, the examples are strongly separable with margin
$\gamma$.
\end{proof}
