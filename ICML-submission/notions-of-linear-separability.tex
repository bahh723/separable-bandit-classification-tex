\section{Notions of linear separability}
\label{section:notions-of-linear-separability}

We define two notions of linear separability for multiclass classification. The
first notion is the standard notion of linear separability used in the proof of
the mistake bound for the \textsc{Multiclass Perceptron} algorithm. The second
notion is stronger, i.e. more restrictive. However, it is more suitable for the
bandit setting, since it allows for a simple and efficient algorithm; see
Section~\ref{section:algorithm-for-strongly-linearly-separable-data}.

\begin{definition}[Weak linear separability]
\label{definition:weak-linear-separability}
Let $(V,\ip{\cdot}{\cdot})$ be an inner product space, $K$ be a positive
integer, and $\gamma$ be a positive real number. Let $(x_1, y_1), (x_2,
y_2), \dots, (x_T, y_T)$ be labeled examples in $V \times \{1,2,\dots,K\}$.

The examples are said to be \emph{weakly linearly separable with a
margin $\gamma$} if there exist vectors $w_1, w_2, \dots, w_K \in V$ such
that
\begin{align}
\label{equation:weak-linear-separability-1}
& \sum_{i=1}^K \norm{w_i}^2 \le 1 \; , \\
\label{equation:weak-linear-separability-2}
& \begin{gathered}
\forall t \in \{1,2,\dots,T\} \quad \forall i \in \{1,2,\dots, K\} \setminus \{y_t\} \\
\ip{x_t}{w_{y_t}} \ge \ip{x_t}{w_i} + \gamma \; .
\end{gathered}
\end{align}
\end{definition}

\begin{definition}[Strong linear separability]
\label{definition:strong-linear-separability}
Let $(V,\ip{\cdot}{\cdot})$ be an inner product space, $K$ be a positive
integer, and $\gamma$ be a positive real number. Let $(x_1, y_1), (x_2,
y_2), \dots, (x_T, y_T)$ be labeled examples in $V \times \{1,2,\dots,K\}$.

The examples are said to be \emph{strongly linearly separable with a
margin $\gamma$} if there exist vectors $w_1, w_2, \dots, w_K \in V$ such
that
\begin{align}
\label{equation:strong-linear-separability-1}
& \sum_{i=1}^K \norm{w_i}^2 \le 1 \; , \\
\label{equation:strong-linear-separability-2}
& \forall t \in \{1,2,\dots,T\} \qquad \qquad \ip{x_t}{w_{y_t}} \ge \frac \gamma 2 \; , \\
\label{equation:strong-linear-separability-3}
& \begin{gathered}
\forall t \in \{1,2,\dots,T\} \qquad \forall i \in \{1,2,\dots, K\} \setminus \{y_t\} \\
\ip{x_t}{w_i} \le - \frac \gamma 2 \; .
\end{gathered}
\end{align}
\end{definition}

The notion of weak linear separability with a margin is standard. It is used in
the full-information setting to upper bound the number of mistakes of the
\textsc{Multiclass Perceptron} algorithm. It is a folklore result that if a set
of labeled examples is separable with a margin $\gamma$ and the norm of the
examples is bounded by $R$, then \textsc{Multiclass Perceptron} algorithm makes
at most $\left\lfloor 2(R/\gamma)^2 \right \rfloor$ mistakes. Another folklore
result is that \textsc{Multiclass Perceptron} is essentially optimal in the
sense that any deterministic algorithm must make $\left\lfloor (R/\gamma)^2
\right \rfloor$ mistakes in the worst case. For completeness, we state the
\textsc{Multiclass Perceptron} algorithm and give proofs of both of these
results in Appendix~\ref{section:multiclass-perceptron-proofs}.

The notion of strong linear separability has appeared in the literature; see
e.g.~\citet{Chen-Chen-Zhang-Chen-Zhang-2009}.
Intuitively, strong linear
separability means that, for each class $i$, the set of examples belonging to
class $i$ and the set of example belonging to the remaining $K-1$ classes are
separated by a linear classifier $w_i$ by a margin $\frac \gamma 2$.

It is easy to see that if a set of labeled examples are strongly linearly
separable with margin $\gamma$, then they are weakly linearly separable with
the same margin. Indeed, if $w_1, w_2, \dots, w_K \in V$
satisfy \eqref{equation:strong-linear-separability-1},
\eqref{equation:strong-linear-separability-2},
\eqref{equation:strong-linear-separability-3} then they satisfy
\eqref{equation:weak-linear-separability-1} and
\eqref{equation:weak-linear-separability-2}.

In the special case of $K=2$, if a set of labeled examples are weakly linearly
separable with a margin $\gamma$, then they are strongly linearly separable with
the same margin. Indeed, if $w_1, w_2$ satisfy
\eqref{equation:weak-linear-separability-1} and
\eqref{equation:weak-linear-separability-2} then $w_1' = \frac{w_1 - w_2}{2}$,
$w_2' = \frac{w_2 - w_1}{2}$ satisfy
\eqref{equation:strong-linear-separability-1},
\eqref{equation:strong-linear-separability-2},
\eqref{equation:strong-linear-separability-3}. The first condition follows from
$\norm{w_i'}^2 \le (\frac{1}{2} \norm{w_1} + \frac{1}{2} \norm{w_2})^2 \le
\frac{1}{2}\norm{w_1}^2 + \frac{1}{2}\norm{w_2}^2 \le \frac{1}{2}$ for $i=1,2$.
The last two condition follows from the fact that $w_1' - w_2' = w_1 - w_2$.

However, for any $K \ge 3$ and any inner product space of dimension at least
$2$, there exists a set of labeled examples that is weakly linearly separable
with a positive margin $\gamma$ but it is not strongly linearly separable with
any positive margin $\gamma$. An example of such set of labeled examples is
shown in~\autoref{figure:weakly-linearly-separable-examples-with-margin}.

\begin{figure}
\begin{center}
\input{figures/weakly-linearly-separable-examples-with-margin}
\end{center}
\caption[]{A set of labeled examples in $\R^2$. The examples belong to
$K=3$ classes colored white, gray and black respectively. Each class lies in a
$120^\circ$ wedge. In other words, each class lies in an intersection of two
halfspaces.
\chicheng{Do we have an explicit formula for $W$ in this example? It might help
to provide it.}

While the examples are weakly linearly separable with a positive margin
$\gamma$, they are \emph{not} strongly linearly separable with any positive
margin $\gamma$. For instance, there do not exist linear separators that
separates the examples belonging to the gray class from the examples belonging
to the remaining two classes.
}
\label{figure:weakly-linearly-separable-examples-with-margin}
\end{figure}
