\section{Notions of linear separability}
\label{section:notions-of-linear-separability}

We define two notions of linear separability for multiclass classification. The
first notion is the standard notion of linear separability used in the proof of
the mistake bound for the \textsc{Multiclass Perceptron} algorithm. The second
notion is stronger, i.e. more restrictive. %However, it is more suitable for the
%bandit setting, since it allows for a simple and efficient algorithm; see
%Section~\ref{section:algorithm-for-strongly-linearly-separable-data}.

\begin{definition}
[Linear separability]
\label{definition:linear-separability}
Let $(V,\ip{\cdot}{\cdot})$ be an inner product space, $K$ be a positive
integer, and $\gamma$ be a positive real number.
We say that labeled examples $(x_1, y_1),
(x_2, y_2), \dots, (x_T, y_T) \in V \times \{1,2,\dots,K\}$ are
%said to be
\begin{enumerate}
\item
\emph{weakly linearly separable with a margin $\gamma$} if there exist vectors
$w_1, w_2, \dots, w_K \in V$ such that
\begin{align}
\label{equation:weak-linear-separability-1}
& \sum_{i=1}^K \norm{w_i}^2 \le 1 \; , \\
\label{equation:weak-linear-separability-2}
& \begin{gathered}
\forall t \in \{1,2,\dots,T\} \quad \forall i \in \{1,2,\dots, K\} \setminus \{y_t\} \\
\ip{x_t}{w_{y_t}} \ge \ip{x_t}{w_i} + \gamma \; ,
\end{gathered}
\end{align}
\item
\emph{strongly linearly separable with a margin $\gamma$} if there exist vectors
$w_1, w_2, \dots, w_K \in V$ such that
\begin{align}
\label{equation:strong-linear-separability-1}
& \sum_{i=1}^K \norm{w_i}^2 \le 1 \; , \\
\label{equation:strong-linear-separability-2}
& \forall t \in \{1,2,\dots,T\} \qquad \qquad \ip{x_t}{w_{y_t}} \ge \gamma/2 \; , \\
\label{equation:strong-linear-separability-3}
& \begin{gathered}
\forall t \in \{1,2,\dots,T\} \quad \forall i \in \{1,2,\dots, K\} \setminus \{y_t\} \\
\ip{x_t}{w_i} \le - \gamma/2 \; .
\end{gathered}
\end{align}
\end{enumerate}
\end{definition}

%The notion of weak linear separability is a standard assumption used in the
%full-information setting to upper bound the number of mistakes of the
%\textsc{Multiclass Perceptron} algorithm.
%%%%%
%(Recall our discussion on the topic in Section~\ref{section:introduction}.)
%Namely, \citet{Crammer-Singer-2003}
%prove that if the examples are weakly linearly separable with a margin $\gamma$
%and the norm of the examples is bounded by $R$, then \textsc{Multiclass
%Perceptron} algorithm makes at most $\left\lfloor 2(R/\gamma)^2 \right \rfloor$
%mistakes. It is a folklore result that \textsc{Multiclass Perceptron} is
%essentially optimal in the sense that any (possibly randomized) algorithm must
%make $\frac{1}{2} \left\lfloor (R/\gamma)^2 \right \rfloor$ mistakes in the
%worst case. For completeness, we state the \textsc{Multiclass Perceptron}
%algorithm, the upper and lower bound on the number of mistakes, and their proofs
%in Appendix~\ref{section:multiclass-perceptron-proofs} in the supplementary
%material.
%%%%%

The notion of strong linear separability has appeared in the literature; see
e.g.~\citet{Chen-Chen-Zhang-Chen-Zhang-2009}. Intuitively, strong linear
separability means that, for each class $i$, the set of examples belonging to
class $i$ and the set of examples belonging to the remaining $K-1$ classes are
separated by a linear classifier $w_i$ with margin $\frac{\gamma}{2}$.

It is easy to see that if a set of labeled examples is strongly linearly
separable with margin $\gamma$, then it is also weakly linearly separable with
the same margin (or larger). Indeed, if $w_1, w_2, \dots, w_K \in V$ satisfy
\eqref{equation:strong-linear-separability-1},
\eqref{equation:strong-linear-separability-2},
\eqref{equation:strong-linear-separability-3} then they satisfy
\eqref{equation:weak-linear-separability-1} and
\eqref{equation:weak-linear-separability-2}.

In the special case of $K=2$, if a set of labeled examples is weakly
linearly separable with a margin $\gamma$, then it is also strongly linearly
separable with the same margin. Indeed, if $w_1, w_2$ satisfy
\eqref{equation:weak-linear-separability-1} and
\eqref{equation:weak-linear-separability-2} then $w_1' = \frac{w_1 - w_2}{2}$,
$w_2' = \frac{w_2 - w_1}{2}$ satisfy
\eqref{equation:strong-linear-separability-1},
\eqref{equation:strong-linear-separability-2},
\eqref{equation:strong-linear-separability-3}.
Equation~\eqref{equation:strong-linear-separability-1} follows from
$\norm{w_i'}^2 \le (\frac{1}{2} \norm{w_1} + \frac{1}{2} \norm{w_2})^2 \le
\frac{1}{2}\norm{w_1}^2 + \frac{1}{2}\norm{w_2}^2 \le \frac{1}{2}$ for $i=1,2$.
Equations~\eqref{equation:strong-linear-separability-2}
and~\eqref{equation:strong-linear-separability-3} follow from the fact that
$w_1' - w_2' = w_1 - w_2$.

However, for any $K \ge 3$ and any inner product space of dimension at least
$2$, there exists a set of labeled examples that is weakly linearly separable
with a positive margin $\gamma$ but is not strongly linearly separable with
any positive margin.~\autoref{figure:weakly-linearly-separable-examples-with-margin}
shows one such set of labeled examples.

\begin{figure}
\begin{center}
 \scalebox{.8}{\input{figures/weakly-linearly-separable-examples-with-margin}}
\end{center}
\caption[]{A set of labeled examples in $\R^2$. The examples belong to
$K=3$ classes colored white, gray and black respectively. Each class lies in a
$120^\circ$ wedge. In other words, each class lies in an intersection of two
halfspaces. While the examples are weakly linearly separable with a positive margin
$\gamma$, they are \emph{not} strongly linearly separable with any positive
margin $\gamma$. For instance, there does \emph{not} exist a linear separator
that separates the examples belonging to the gray class from the examples
belonging to the remaining two classes.
}
\label{figure:weakly-linearly-separable-examples-with-margin}
\end{figure}
