\section{Algorithm for strongly linearly separable data}
\label{section:algorithm-for-strongly-linearly-separable-data}

In this section, we present an algorithm for \textsc{Online Multiclass
Classification with Bandit Feedback} and prove an upper bound on the number of
mistakes under the assumption that the examples are strongly linearly separable
with a margin. The algorithm is stated as
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} and
the mistake upper bound is stated as
\autoref{theorem:strongly-separable-example-mistake-upper-bound}.

The idea behind the algorithm is to use $K$ copies of the \textsc{Binary
Perceptron} algorithm, where each copy corresponds to a class.
In each round, copy $i$
makes a binary prediction, indicating whether the example belongs to class $i$ or not.
If at least one copy makes a positive prediction, the algorithm
chooses its prediction as one of the classes corresponding to a positive prediction.
If all copies make negative
predictions, the algorithms makes a prediction uniformly at random.

%accordingly. If multiple copies make positive predictions, the
%algorithm chooses one of them arbitrarily

\begin{algorithm}[h]
\caption{\textsc{Bandit Algorithm for Strongly Linearly Separable Examples}
\label{algorithm:algorithm-for-strongly-linearly-separable-examples}}
\begin{algorithmic}[1]
{
\REQUIRE{Number of classes $K$.}
\REQUIRE{Number of rounds $T$.}
\REQUIRE{Inner product space $(V,\ip{\cdot}{\cdot})$.}
\STATE{Initialize $w_1^{(1)} = w_2^{(1)} = \dots = w_K^{(1)} = 0$}
\FOR{$t=1,2,\dots,T$}
\STATE{Observe feature vector $x_t \in V$}
\STATE{Compute $S_t = \left\{ i ~:~ 1 \le i \le K, \ \ip{w_i^{(t)}}{x_t} \ge 0 \right\}$}
\IF{$S_t = \emptyset$}
\STATE{Predict $\widehat y_t \sim \text{Uniform}(\{1,2,\dots,K\})$}
\STATE{Observe feedback $z_t \in \{0,1\}$ \\ \qquad where $z_t = \indicator{\widehat y_t \neq y_t}$}
\IF{$z_t = 1$}
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$}
\ELSE
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ \\ \qquad  for all $i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$}
\STATE{Update $w_{\widehat y_t}^{(t+1)} = w_{\widehat y_t}^{(t)} + x_t$}
\label{line:pos-update}
\ENDIF
\ELSE
\STATE{Predict $\widehat y_t \in S_t$ chosen arbitrarily}
\STATE{Observe feedback $z_t \in \{0,1\}$ \\ \qquad where $z_t = \indicator{\widehat y_t \neq y_t}$}
\IF{$z_t = 1$}
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ \\ \qquad for all $i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$}
\STATE{Update $w_{\widehat y_t}^{(t+1)} = w_{\widehat y_t}^{(t)} - x_t$}
\label{line:neg-update}
\ELSE
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$}
\ENDIF
\ENDIF
\ENDFOR
}
\end{algorithmic}
\end{algorithm}

\begin{theorem}[Mistake upper bound]
\label{theorem:strongly-separable-example-mistake-upper-bound}
Let $(V, \ip{\cdot}{\cdot})$ be an inner product space, $K$ be a positive
integer, $\gamma$ be a positive real number, $R$ be a non-negative real number. If
$(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T)$ is a sequence of labeled examples in
$V \times \{1,2,\dots,K\}$ such that
\begin{enumerate}
  \item the examples are strongly linearly separable with margin $\gamma$ (Definition~\ref{definition:strong-linear-separability}),
  \item $\norm{x_1}, \norm{x_2}, \dots, \norm{x_T} \le R$,
\end{enumerate}
then the
expected number of mistakes
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples}
makes is at most $(K-1) \lfloor 4(R/\gamma)^2 \rfloor$.
\end{theorem}

\begin{proof}
Let $M = \sum_{t=1}^T z_t$ be the number of
mistakes Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} makes.
Let $A
= \sum_{t ~:~ S_t \neq \emptyset} z_t$ be the number of mistakes in the rounds
when $S_t \neq \emptyset$ and let $B = \sum_{t ~:~ S_t = \emptyset} z_t$ be the
number of mistakes in the rounds when $S_t = \emptyset$.
It can be easily seen that $M = A + B$. %We now upper bound $A$ and $B$ separately.

Let $C$ be the number of times line~\ref{line:pos-update} gets executed. Let $U$ be the number of
times line~\ref{line:pos-update} and line~\ref{line:neg-update} get executed. In other words, $U$ is the number of
times the $K$-tuple of vectors $(w_1^{(t)}, w_2^{(t)}, \dots, w_K^{(t)})$ gets
updated. It can be easily seen that $U = A + C$.

The key observation is that $\Exp[B] = (K-1) \Exp[C]$. The equality holds
because if $S_t = \emptyset$, there is $1/K$ probability that the algorithm
guesses the correct label and with probability $(K-1)/K$ algorithm's guess is
incorrect. Putting all the information together, we get that
\begin{align}
\Exp[M]
& = \Exp[A] + \Exp[B] \nonumber \\
& = \Exp[A] + (K-1) \Exp[C] \nonumber \\
& \le (K-1) \Exp[A + C] \nonumber\\
& = (K-1) \Exp[U]  \; .
\label{eqn:mistake-update}
\end{align}

To finish the proof, we need to upper bound the number of updates $U$. We claim
that $U \le \lfloor (R/\gamma)^2 \rfloor$. The proof of this upper bound is
similar to the proof of the mistake bound for \textsc{Multiclass Perceptron}
algorithm. Let $w_1^*, w_2^*, \dots, w_K^* \in V$ be vectors that satisfy
\eqref{equation:strong-linear-separability-1},
\eqref{equation:strong-linear-separability-2} and
\eqref{equation:strong-linear-separability-3}.
The $K$-tuple $(w_1^{(t)}, w_2^{(t)}, \dots, w_K^{(t)})$
changes only if there is an update in round $t$.
We investigate how $\sum_{i=1}^K \norm{w_i^{(t)}}^2$ and
$\sum_{i=1}^K \ip{w_i^*}{w_i^{(t)}}$ change. If there is an update in round $t$,
by lines~\ref{line:pos-update} and~\ref{line:neg-update}, we always have
$ w_{\widehat y_t}^{(t+1)} = w_{\widehat y_t}^{(t)} + (-1)^{z_t} x_t $,
and for all $i \neq \widehat y_t$, $w_{i}^{(t+1)} = w_{i}^{(t)}$.
Therefore,
\begingroup
\allowdisplaybreaks
\begin{align*}
& \sum_{i=1}^K \norm{w_i^{(t+1)}}^2 \\
& = \left( \sum_{i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}} \norm{w_i^{(t)}}^2 \right) + \norm{w_{\widehat y_t}^{(t+1)}}^2 \\
& = \left( \sum_{i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}} \norm{w_i^{(t)}}^2 \right) + \norm{w_{\widehat y_t}^{(t)} + (-1)^{z_t} x_t}^2 \\
& = \left( \sum_{i=1}^K \norm{w_i^{(t)}}^2 \right) + \norm{x_t}^2 + \underbrace{(-1)^{z_t} 2 \ip{w_{\widehat y_t}^{(t)}}{x_t}}_{\le 0} \\
& \le \left( \sum_{i=1}^K \norm{w_i^{(t)}}^2 \right) + \norm{x_t}^2 \\
& \le \left( \sum_{i=1}^K \norm{w_i^{(t)}}^2 \right) + R^2 \; .
\end{align*}
\endgroup
Hence, after $U$ updates,
\begin{equation}
\sum_{i=1}^K \norm{w_i^{(T+1)}}^2 \le R^2 U \; .
\label{eqn:norm-ub}
\end{equation}
Similarly, if there is an update in round $t$, we have
\begin{align*}
& \sum_{i=1}^K \ip{w_i^*}{w_i^{(t)}} \\
& = \left( \sum_{i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}} \ip{w_i^*}{w_i^{(t)}} \right) + \ip{w_{\widehat y_t}^*}{w_{\widehat y_t}^{(t+1)}} \\
& = \left( \sum_{i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}} \ip{w_i^*}{w_i^{(t)}} \right) \\
& \qquad + \ip{w_{\widehat y_t}^*}{w_{\widehat y_t}^{(t)} + (-1)^{z_t} x_t} \\
& = \left( \sum_{i=1}^K \ip{w_i^*}{w_i^{(t)}} \right) + (-1)^{z_t} \ip{w_{\widehat y_t}^*}{x_t} \\
& \ge \left( \sum_{i=1}^K \ip{w_i^*}{w_i^{(t)}} \right) + \frac \gamma 2,
\end{align*}
where the last inequality follows from a case analysis on $z_t$ and
Definition~\ref{definition:strong-linear-separability}: if $z_t = 0$, then
$\widehat y_t = y_t$, by Equation~\eqref{equation:strong-linear-separability-2}, we have
that $\ip{w_{\widehat y_t}^*}{x_t} \geq \frac \gamma 2$;
if $z_t = 1$, then $\widehat y_t \neq y_t$, by Equation~\eqref{equation:strong-linear-separability-3},
we have that $\ip{w_{\widehat y_t}^*}{x_t} \leq -\frac \gamma 2$.

%\eqref{equation:strong-linear-separability-2}
%and \eqref{equation:strong-linear-separability-3} and analysis of the cases $z_t = 0$ and $z_t = 1$.

Thus, after $U$ updates,
\begin{equation}
\sum_{i=1}^K \ip{w_i^*}{w_i^{(T+1)}} \ge \frac {\gamma U} 2 \; .
\label{eqn:norm-lb}
\end{equation}
Applying Cauchy-Schwartz's inequality twice, and using assumption
\eqref{equation:strong-linear-separability-1}, we get that
\begin{align*}
\sum_{i=1}^K \ip{w_i^*}{w_i^{(T+1)}}
& \le \sum_{i=1}^K \norm{w_i^*} \cdot \norm{w_i^{(T+1)}} \\
& \le \sqrt{\sum_{i=1}^K \norm{w_i^*}^2} \sqrt{\sum_{i=1}^K \norm{w_i^{(T+1)}}^2} \\
& \le \sqrt{\sum_{i=1}^K \norm{w_i^{(T+1)}}^2} \; .
\end{align*}
Combining the above inequality with Equations~\eqref{eqn:norm-ub} and~\eqref{eqn:norm-lb}, we get
$$
(\frac{\gamma U}{2})^2 \le \sum_{i=1}^K \norm{w_i^{(T+1)}}^2 \le R^2 U \; .
$$
We conclude that $U \le 4(R/\gamma)^2$. Since $U$ is an integer, $U \le \lfloor 4(R/\gamma)^2 \rfloor$.

Applying Equation~\eqref{eqn:mistake-update}, we get
\[ \Exp[M] \leq (K-1) \Exp[U] \leq (K-1) \lfloor 4(R/\gamma)^2 \rfloor. \qedhere \]
\end{proof}

The upper bound $(K-1) \lfloor 4(R/\gamma)^2 \rfloor$ on the expected number of
mistakes of
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} is
optimal to within constant factor as long as the number of classes $K$ is
at most $O((R/\gamma)^2)$. This is proved in
\autoref{theorem:strongly-separable-example-mistake-lower-bound} below.

\begin{theorem}[Mistake lower bound]
\label{theorem:strongly-separable-example-mistake-lower-bound}
Let $\gamma$ be a positive real number, and $R$ be a non-negative real
number. For any (possibly randomized) algorithm $\calA$ for the \textsc{Online
Multiclass Classification with Bandit Feedback} with $K \le
(R/\gamma)^2$ classes there exists an inner product space $(V,
\ip{\cdot}{\cdot})$, a non-negative integer $T$ and a sequence of labeled
examples $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T)$ in $V \times
\{1,2,\dots,K\}$ that are strongly linearly separable with margin $\gamma$, the
norms satisfy $\norm{x_1}, \norm{x_2}, \dots, \norm{x_T} \le R$ and the expected
number of mistakes $\calA$ makes is at least $\frac{K-1}{2} \left\lfloor
\frac{1}{4} (R/\gamma)^2 \right\rfloor$.
\end{theorem}

\paragraph{Remark.} Irrespective of any conditions on $K$, $R$, and $\gamma$, a trivial lower bound
on the expected number of mistakes of any randomized algorithm is
$\frac{K-1}{2}$. (Adversary chooses the same example over and over.) However, it
is unclear if the trivial lower bound is the best possible if $K$ is large,
i.e., $\omega((R/\gamma)^2)$. We leave this as an open problem.

\begin{proof}
We use probabilistic method. Let $M = \left\lfloor \frac{1}{4} (R/\gamma)^2
\right\rfloor$. Let $V = \R^{M+1}$ equipped with the standard inner product.
Let $e_1, e_2, \dots, e_{M+1}$ be the standard orthonormal basis of $V$. We
define vectors $v_1, v_2, \dots, v_M \in V$ where $v_j = \frac{R}{\sqrt{2}}(e_j
+ e_{M+1})$ for $j=1,2,\dots,M$. Let $\ell_1, \ell_2, \dots, \ell_M$ be chosen
i.i.d. uniformly at random from $\{1,2,\dots,K\}$ and independently of any
randomness used the by algorithm $\calA$. Let $T = M (K - 1)$. We define examples $(x_1,
y_1), (x_2, y_2), \dots, (x_T, y_T)$ as follows. For any $j=1,2,\dots,M$ and any
$h=1,2,\dots,K-1$,
$$
(x_{(j-1)(K-1) + h}, y_{(j-1)(K-1) + h}) = (v_j, \ell_j)
$$


With probability one, the norm of each example is exactly $R$.
We show that with probability
one, the examples are strongly separable with margin $\gamma$. To see
that, consider $w_1^*, w_2^*, \dots, w_K^* \in V$ defined by
$$
w_i^* = \sqrt{2} \frac{\gamma}{R} \left( \sum_{j ~:~ \ell_j = i} e_j \right) - \frac{\sqrt{2}}2 \frac{\gamma}{R} e_{M+1}
$$
for $i=1,2,\dots,K$.

for $i \in \{1,2,\dots,K\}$ and any $j \in
\{1,2,\dots,M\}$, we consider the inner product of $w_i^*$ and $v_j$.
If $i = \ell_j$, $\ip{w_i^*}{v_j} = \gamma - \frac \gamma 2 = \frac \gamma 2$;
otherwise $i \neq \ell_j$, in this case,
$\ip{w_i^*}{v_j} = 0 - \frac \gamma 2 = - \frac \gamma 2$.
This means that $w_1^*, w_2^*, \dots, w_K^*$ satisfy
conditions
\eqref{equation:strong-linear-separability-2} and
\eqref{equation:strong-linear-separability-3}. Condition \eqref{equation:strong-linear-separability-1}
is satisfied since
\begin{multline*}
\sum_{i=1}^K \norm{w_i^*}^2
= 2 \frac{\gamma^2}{R^2} \sum_{j=1}^M \norm{e_j}^2 +  \frac{\gamma^2}{2 R^2} K \norm{e_{M+1}}^2 \\
= 2 \frac{\gamma^2}{R^2} M + \frac{\gamma^2}{2 R^2} K
\le \frac{1}{2} + \frac{1}{2}
= 1 \; .
\end{multline*}

It remains to lower bound the expected number of mistakes of $\calA$. For
any $j \in \{1,2,\dots,M\}$, consider the expected number of mistakes the
algorithm makes in rounds $(K-1)(j-1) + 1, (K-1)(j-1) + 2, \dots, (K-1)j$.

Define a filtration $\cbr{\calB_j}_{j=0}^M$,
where $\calB_j = \sigma((x_1, y_1), \ldots, (x_{(K-1)j}, y_{(K-1)j}))$
for every $j$ in $\{1,2,\dots,M\}$.
By Claim 2 of~\cite{Daniely-Helbertal-2013}, as $\ell_j$
is chosen uniformly from $\cbr{1,\ldots,K}$ and independent of $\calB_{j-1}$,
\[ \Exp \sbr{ \sum_{t=(K-1)(j-1) + 1}^{(K-1)j} z_t \Bigg| \calB_{j-1} } \geq \frac{K-1}2. \]
This implies that
\[ \Exp \sbr{ \sum_{t=(K-1)(j-1) + 1}^{(K-1)j} z_t } \geq \frac{K-1}2. \]
Summing over all $j$ in $\{1,2,\dots,M\}$,
\[ \Exp \sbr{ \sum_{t=1}^{(K-1)M} z_t} \geq \frac{K-1}2 \cdot M = \frac{K-1}2 \left\lfloor \frac 1 4 (R/\gamma)^2 \right\rfloor. \]

%In these rounds,
%the algorithm is guessing the label $\ell_j$. Since $\ell_j$ is chosen uniformly
%at random from the set $\{1,2,\dots,K\}$ and feedback is only binary, the
%expected number of mistakes the algorithm makes in these rounds is at least
%$\frac{K-1}{2}$. Altogether, the algorithm makes at least $\frac{K-1}{2} M$
%mistakes in expectation.

%\chicheng{I think this part need to be formalized, by directly using
%~\cite{Daniely-Helbertal-2013}, Claim 2.}
%Can we assume without loss of generality
%that a deterministic prediction algorithm will have a smaller expected cost?
%If so, define $F(k)$ be the number of mistake that will be made by the algorithm,
%if the adversary commits to a label uniformly at random from $\cbr{1,2,\ldots,k}$,
%and the guessing of the label lasts for $k$ rounds.
%we can construct a recurrence of $F$: $F(k) = F(k-1) + \frac{k-1}{k}$, and $F(1) = 0$.
%Therefore, $F(K) = \Omega(K)$.

Finally, since strong separability and norm condition hold with probability one,
there exists a particular (i.e. deterministic) sequence of examples for which
the algorithm makes at least $\frac{K-1}2 \left\lfloor \frac 1 4 (R/\gamma)^2 \right\rfloor$ mistakes in expectation
over its internal randomization.
\end{proof}

Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples}
can be extended to nonlinear classification using kernels~\cite{Scholkopf-Smola-2002, Shawe-Taylor-Cristianini-2004}.
For each class, as opposed
to explicitly maintaining the predictor's weights for each class, we maintain
the set of examples whose scaled feature transformations are added to the weights in Perceptron updates.
We give a formal description of the kernelized algorithm in Algorithm~\ref{algorithm:kernelized}.

\begin{algorithm}[h]
\caption{\textsc{Kernelized Bandit Algorithm}
\label{algorithm:kernelized}}
\begin{algorithmic}[1]
{
\REQUIRE{Number of classes $K$.}
\REQUIRE{Number of rounds $T$.}
\REQUIRE{Kernel function  $k(\cdot, \cdot)$.}
\STATE{Initialize $J_1^{(1)} = J_2^{(1)} = \dots = J_K^{(1)} = \emptyset$}
\FOR{$t=1,2,\dots,T$}
\STATE{Observe feature vector $x_t$.}
\STATE{Compute $S_t = \left\{ i ~:~ 1 \le i \le K, \ \sum_{(x,y) \in J_i^{(t)}} y k(x, x_t) \ge 0 \right\}$}
\IF{$S_t = \emptyset$}
\STATE{Predict $\widehat y_t \sim \text{Uniform}(\{1,2,\dots,K\})$}
\STATE{Observe feedback $z_t \in \{0,1\}$ \\ \qquad where $z_t = \indicator{\widehat y_t \neq y_t}$}
\IF{$z_t = 1$}
\STATE{Set $J_i^{(t+1)} = J_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$}
\ELSE
\STATE{Set $J_i^{(t+1)} = J_i^{(t)}$ \\ \qquad  for all $i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$}
\STATE{Update $J_{\widehat y_t}^{(t+1)} = J_{\widehat y_t}^{(t)} \cup \cbr{(x_t, +1)}$}
\ENDIF
\ELSE
\STATE{Predict $\widehat y_t \in S_t$ chosen arbitrarily}
\STATE{Observe feedback $z_t \in \{0,1\}$ \\ \qquad where $z_t = \indicator{\widehat y_t \neq y_t}$}
\IF{$z_t = 1$}
\STATE{Set $J_i^{(t+1)} = J_i^{(t)}$ \\ \qquad for all $i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$}
\STATE{Update $J_{\widehat y_t}^{(t+1)} = J_{\widehat y_t}^{(t)} \cup \cbr{(x_t, -1)}$}
\ELSE
\STATE{Set $J_i^{(t+1)} = J_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$}
\ENDIF
\ENDIF
\ENDFOR
}
\end{algorithmic}
\end{algorithm}
