\section{Algorithm for strongly linearly separable data}
\label{section:algorithm-for-strongly-linearly-separable-data}

In this section, we consider the case when the examples are strongly linearly
separable. We present an algorithm for this setting
(Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples})
and give an upper bound on its number of mistakes, stated as
\autoref{theorem:strongly-separable-examples-mistake-upper-bound} below. The
proof of the theorem can be found in
Appendix~\ref{section:proofs-for-stringly-separable-examples} in the
supplementary material.

The idea behind
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} is
to use $K$ copies of the \textsc{Binary Perceptron} algorithm, one copy per class; see e.g.
\citet[Section 3.3.1]{Shalev-Shwartz-2012}.
Upon seeing each example $x_t$, copy $i$ predicts 
whether or not $x_t$ belongs to class $i$.
Multiclass predictions are done by evaluating all $K$ binary predictors and outputting any class with a positive prediction.  If all binary predictions are negative, the algorithm chooses a prediction a prediction uniformly at random from
$\cbr{1,\ldots,K}$.

%\begin{algorithm}[h]
%\caption{\textsc{Bandit Algorithm for Strongly Linearly Separable Examples}
%\label{algorithm:algorithm-for-strongly-linearly-separable-examples}}
%\begin{algorithmic}[1]
%{
%\REQUIRE{Number of classes $K$, number of rounds $T$.}
%\REQUIRE{Inner product space $(V,\ip{\cdot}{\cdot})$.}
%\STATE{Initialize $w_1^{(1)} = w_2^{(1)} = \dots = w_K^{(1)} = 0$} \\
%\FOR{$t=1,2,\dots,T$}
%\STATE{Observe feature vector $x_t \in V$}
%\STATE{Compute $S_t = \left\{ i ~:~ 1 \le i \le K, \ \ip{w_i^{(t)}}{x_t} \ge 0 \right\}$}
%\IF{$S_t = \emptyset$}
%\STATE{Predict $\widehat y_t \sim \text{Uniform}(\{1,2,\dots,K\})$}
%\STATE{Observe feedback $z_t = \indicator{\widehat y_t \neq y_t}$}
%\IF{$z_t = 1$}
%\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$}
%\ELSE
%\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ \\ \qquad  for all $i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$}
%\STATE{Update $w_{\widehat y_t}^{(t+1)} = w_{\widehat y_t}^{(t)} + x_t$}
%\label{line:pos-update}
%\ENDIF
%\ELSE
%\STATE{Predict $\widehat y_t \in S_t$ chosen arbitrarily}
%\STATE{Observe feedback $z_t  = \indicator{\widehat y_t \neq y_t}$}
%\IF{$z_t = 1$}
%\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ \\ \qquad for all $i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$}
%\STATE{Update $w_{\widehat y_t}^{(t+1)} = w_{\widehat y_t}^{(t)} - x_t$}
%\label{line:neg-update}
%\ELSE
%\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$}
%\ENDIF
%\ENDIF
%\ENDFOR
%}
%\end{algorithmic}
%\end{algorithm}

\begin{algorithm}[h]
\SetAlgoLined
\LinesNumbered
\caption{\textsc{Bandit Algorithm for Strongly Linearly Separable Examples}
\label{algorithm:algorithm-for-strongly-linearly-separable-examples}}
\textbf{Require:} Number of classes $K$, number of rounds $T$. \\
\textbf{Require:} Inner product space $(V,\ip{\cdot}{\cdot})$.  \\
\nl Initialize $w_1^{(1)} = w_2^{(1)} = \dots = w_K^{(1)} = 0$\\
\nl \For{$t=1,2,\dots,T$}{
\nl    Observe feature vector $x_t \in V$ \\
\nl    Compute $S_t = \left\{ i ~:~ 1 \le i \le K, \ \ip{w_i^{(t)}}{x_t} \ge 0 \right\}$\\
\nl    \If{$S_t = \emptyset$}{
\nl         Predict $\widehat y_t \sim \text{Uniform}(\{1,2,\dots,K\})$ \\
\nl         Observe feedback $z_t = \indicator{\widehat y_t \neq y_t}$\\
\nl          \If{$z_t = 1$}{
\nl               Set $w_i^{(t+1)} = w_i^{(t)}$, $\forall i \in \{1,2,\dots,K\}$
              }
\nl          \Else{
\nl               Set $w_i^{(t+1)} = w_i^{(t)}$, $\forall i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$ \\
\nl Update $w_{\widehat y_t}^{(t+1)} = w_{\widehat y_t}^{(t)} + x_t$  \label{line:pos-update}
              }
        }
\nl     \Else{
\nl           Predict $\widehat y_t \in S_t$ chosen arbitrarily \\
\nl           Observe feedback $z_t  = \indicator{\widehat y_t \neq y_t}$ \\
\nl            \If{$z_t = 1$}{
\nl                Set $w_i^{(t+1)} = w_i^{(t)}$, $\forall i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$ \\
\nl  Update $w_{\widehat y_t}^{(t+1)} = w_{\widehat y_t}^{(t)} - x_t$   \label{line:neg-update}
               }
\nl           \Else{
\nl                 Set $w_i^{(t+1)} = w_i^{(t)}$, $\forall i \in \{1,2,\dots,K\}$
           }
      }
}
\end{algorithm}

\begin{theorem}[Mistake upper bound]
\label{theorem:strongly-separable-examples-mistake-upper-bound}
Let $(V, \ip{\cdot}{\cdot})$ be an inner product space, $K$ be a positive
integer, $\gamma$ be a positive real number, $R$ be a non-negative real number.
If the examples $(x_1, y_1), \dots, (x_T, y_T) \in V \times \{1,2,\dots,K\}$ are
strongly linearly separable with margin $\gamma$ and $\norm{x_1}, \norm{x_2},
\dots, \norm{x_T} \le R$ then the expected number of mistakes
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples}
makes is at most $(K-1) \lfloor 4(R/\gamma)^2 \rfloor$.
\end{theorem}

The upper bound $(K-1) \lfloor 4(R/\gamma)^2 \rfloor$ on the expected number of
mistakes of
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} is
optimal up to a constant factor, as long as the number of classes $K$ is at most
$O((R/\gamma)^2)$. This lower bound is stated as
\autoref{theorem:strongly-separable-examples-mistake-lower-bound} below. The
proof of the theorem can be found in
Appendix~\ref{section:proofs-for-stringly-separable-examples} in the
supplementary material.

\begin{theorem}[Mistake lower bound]
\label{theorem:strongly-separable-examples-mistake-lower-bound}
Let $\gamma$ be a positive real number, $R$ be a non-negative real number and
let $K \le (R/\gamma)^2$ be a positive integer. Any (possibly randomized)
algorithm makes at least $((K-1)/2)\left\lfloor (R/\gamma)^2/4 \right\rfloor$
mistakes in expectation on some sequence of labeled examples $(x_1, y_1),
(x_2, y_2), \dots, (x_T, y_T) \in V \times \{1,2,\dots,K\}$ for some inner
product space $(V, \ip{\cdot}{\cdot})$ such that the examples are strongly
linearly separable with margin $\gamma$ and satisfy $\norm{x_1}, \norm{x_2},
\dots, \norm{x_T} \le R$.
\end{theorem}

\paragraph{Remark.}
\citet{Daniely-Helbertal-2013} provide a lower bound under the assumption of weak linear separability, which does not immediately imply a lower bound under the stronger notion.
If $\gamma \le R$ then, irrespective of any other conditions on $K$, $R$, and
$\gamma$, a trivial lower bound on the expected number of mistakes of any
randomized algorithm is $(K-1)/2$. To see this, note that the adversary can
choose an example $(R e_1, y)$, where $e_1$ is some arbitrary unit vector in $V$
and $y$ is a label chosen uniformly from $\cbr{1,2,\dots,K}$, and show this
example $K$ times. The sequence of examples trivially satisfies the strong
linear separability condition, and the $(K-1)/2$ expected mistake lower bound
follows from ~\citep[][Claim 2]{Daniely-Helbertal-2013}. %However, it is unclear
%if the trivial lower bound is the best possible if $K$ is large, i.e.,
%$\omega((R/\gamma)^2)$. We leave this as an open problem.

Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} can
be extended to nonlinear classification using \emph{positive definite kernels}
(or \emph{kernels}, for short), which are functions of the form $k: X \times X
\to \R$ for some set $X$ such that the matrix
$\left(k(x_i,x_j)\right)_{i,j=1}^m$ is a symmetric positive semidefinite for any
positive integer $m$ and $x_1, x_2, \dots, x_m \in X$ \citep[Definition
2.5]{Scholkopf-Smola-2002}.\footnote{Every kernel has an associated feature map
$\phi:X \to V$ for some inner product space $(V,\ip{\cdot}{\cdot})$ such that
$k(x,x') = \ip{\phi(x)}{\phi(x')}$.} As opposed to explicitly maintaining the
weight vector for each class, the algorithm maintains the set of example-scalar
pairs corresponding to the updates of the non-kernelized algorithm. As a direct
consequence of
Theorem~\ref{theorem:strongly-separable-examples-mistake-upper-bound} we get a
mistake bound for the kernelized algorithm.

\begin{theorem}[Mistake upper bound for kernelized algorithm]
\label{theorem:kernelized-upper-bound}
Let $X$ be a non-empty set, let $(V, \ip{\cdot}{\cdot})$ be an inner product
space. Let $\phi:X \to V$ be a feature map and let $k:X \times X \to \R$,
$k(x,x') = \ip{\phi(x)}{\phi(x')}$ be the associated positive definite
kernel. Let $K$ be a positive integer, $\gamma$ be a positive real number, $R$
be a non-negative real number. If $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T) \in
X \times \{1,2,\dots,K\}$ are labeled examples such that:
\begin{enumerate}
\item the mapped examples $(\phi(x_1), y_1)$, $\dots$, $(\phi(x_T), y_T)$
are strongly linearly separable with margin $\gamma$,
\item $k(x_1, x_1), k(x_2, x_2), \dots, k(x_T,x_T) \le R^2$,
\end{enumerate}
then the expected number of mistakes Algorithm~\ref{algorithm:kernelized} makes
is at most $(K-1) \lfloor 4(R/\gamma)^2 \rfloor$.
\end{theorem}

\begin{algorithm}[h]
\caption{\textsc{Kernelized Bandit Algorithm}
\label{algorithm:kernelized}}
\textbf{Require:} Number of classes $K$, number of rounds $T$.\\
\textbf{Require:} Kernel function  $k(\cdot, \cdot)$. \\
Initialize $J_1^{(1)} = J_2^{(1)} = \dots = J_K^{(1)} = \emptyset$ \\
\For{$t=1,2,\dots,T$} {
     Observe feature vector $x_t$.\\
     Compute \\ $S_t = \left\{ i ~:~ 1 \le i \le K, \ \sum_{(x,y) \in J_i^{(t)}} y k(x, x_t) \ge 0 \right\}$ \\
     \If{$S_t = \emptyset$}{
           Predict $\widehat y_t \sim \text{Uniform}(\{1,2,\dots,K\})$ \\
           Observe feedback $z_t = \indicator{\widehat y_t \neq y_t}$ \\
           \If{$z_t = 1$}{
                Set $J_i^{(t+1)} = J_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$
           }
           \Else{
                Set $J_i^{(t+1)} = J_i^{(t)}$, $\forall i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$ \\
                Update $J_{\widehat y_t}^{(t+1)} = J_{\widehat y_t}^{(t)} \cup \cbr{(x_t, +1)}$
           }
      }
      \Else{
           Predict $\widehat y_t \in S_t$ chosen arbitrarily \\
           Observe feedback $z_t =  \indicator{\widehat y_t \neq y_t}$ \\
           \If{$z_t = 1$}{
                Set $J_i^{(t+1)} = J_i^{(t)}$, $\forall i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$\\
                Update $J_{\widehat y_t}^{(t+1)} = J_{\widehat y_t}^{(t)} \cup \cbr{(x_t, -1)}$
           }
           \Else {
                Set $J_i^{(t+1)} = J_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$
           }
      }
}

\end{algorithm}
