\section{Algorithm for strongly linearly separable data}
\label{section:algorithm-for-strongly-linearly-separable-data}

In this section, we consider the case when the examples are strongly linearly
separable. We present an algorithm for this setting
(Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples})
and give an upper bound on its number of mistakes, stated as
\autoref{theorem:strongly-separable-examples-mistake-upper-bound} below. The
proof of the theorem can be found in
Appendix~\ref{section:proofs-for-stringly-separable-examples} in the
supplementary material.

The idea behind
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} is
to use $K$ copies of the \textsc{Binary Perceptron} algorithm; see e.g.
\citet[Section 3.3.1]{Shalev-Shwartz-2012}. Each copy corresponds to one of the
classes. At each timestep $t$, upon seeing example $x_t$, copy $i$ makes a
binary prediction on whether $x_t$ belongs to class $i$ or not. If at least one
copy makes a positive prediction, the algorithm chooses its prediction as one of
the classes corresponding to a positive prediction. If all copies make negative
predictions, the algorithm makes a prediction uniformly at random from
$\cbr{1,\ldots,K}$.

\begin{algorithm}[h]
\caption{\textsc{Bandit Algorithm for Strongly Linearly Separable Examples}
\label{algorithm:algorithm-for-strongly-linearly-separable-examples}}
\begin{algorithmic}[1]
{
\REQUIRE{Number of classes $K$, number of rounds $T$.}
\REQUIRE{Inner product space $(V,\ip{\cdot}{\cdot})$.}
\STATE{Initialize $w_1^{(1)} = w_2^{(1)} = \dots = w_K^{(1)} = 0$}
\FOR{$t=1,2,\dots,T$}
\STATE{Observe feature vector $x_t \in V$}
\STATE{Compute $S_t = \left\{ i ~:~ 1 \le i \le K, \ \ip{w_i^{(t)}}{x_t} \ge 0 \right\}$}
\IF{$S_t = \emptyset$}
\STATE{Predict $\widehat y_t \sim \text{Uniform}(\{1,2,\dots,K\})$}
\STATE{Observe feedback $z_t \in \{0,1\}$ \\ \qquad where $z_t = \indicator{\widehat y_t \neq y_t}$}
\IF{$z_t = 1$}
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$}
\ELSE
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ \\ \qquad  for all $i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$}
\STATE{Update $w_{\widehat y_t}^{(t+1)} = w_{\widehat y_t}^{(t)} + x_t$}
\label{line:pos-update}
\ENDIF
\ELSE
\STATE{Predict $\widehat y_t \in S_t$ chosen arbitrarily}
\STATE{Observe feedback $z_t \in \{0,1\}$ \\ \qquad where $z_t = \indicator{\widehat y_t \neq y_t}$}
\IF{$z_t = 1$}
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ \\ \qquad for all $i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$}
\STATE{Update $w_{\widehat y_t}^{(t+1)} = w_{\widehat y_t}^{(t)} - x_t$}
\label{line:neg-update}
\ELSE
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$}
\ENDIF
\ENDIF
\ENDFOR
}
\end{algorithmic}
\end{algorithm}

\begin{theorem}[Mistake upper bound]
\label{theorem:strongly-separable-examples-mistake-upper-bound}
Let $(V, \ip{\cdot}{\cdot})$ be an inner product space, $K$ be a positive
integer, $\gamma$ be a positive real number, $R$ be a non-negative real number.
If the examples $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T) \in V \times
\{1,2,\dots,K\}$ are strongly linearly separable with margin $\gamma$ and for
all $t \in \cbr{1,\ldots,T}$ satisfy $\norm{x_t} \le R$ then the expected number
of mistakes
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples}
makes is at most $(K-1) \lfloor 4(R/\gamma)^2 \rfloor$.
\end{theorem}

The upper bound $(K-1) \lfloor 4(R/\gamma)^2 \rfloor$ on the expected number of
mistakes of
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} is
optimal up to a constant factor, as long as the number of classes $K$ is at most
$O((R/\gamma)^2)$. This lower bound is stated as
\autoref{theorem:strongly-separable-examples-mistake-lower-bound} below. The
proof of the theorem can be found in
Appendix~\ref{section:proofs-for-stringly-separable-examples} in the
supplementary material.

\begin{theorem}[Mistake lower bound]
\label{theorem:strongly-separable-examples-mistake-lower-bound}
Let $\gamma$ be a positive real number, and $R$ be a non-negative real
number. For any (possibly randomized) algorithm $\calA$ for the \textsc{Online
Multiclass Classification with Bandit Feedback} problem with $K \le
(R/\gamma)^2$ classes, there exists an inner product space $(V,
\ip{\cdot}{\cdot})$, a non-negative integer $T$ and a sequence of labeled
examples $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T)$ in $V \times
\{1,2,\dots,K\}$ that are strongly linearly separable with margin $\gamma$, the
norms satisfy $\norm{x_1}, \norm{x_2}, \dots, \norm{x_T} \le R$ and the expected
number of mistakes $\calA$ makes is at least $\frac{K-1}{2} \left\lfloor
\frac{1}{4} (R/\gamma)^2 \right\rfloor$.
\end{theorem}

\paragraph{Remark.} Irrespective of any conditions on $K$, $R$, and $\gamma$, a
trivial lower bound on the expected number of mistakes of any randomized
algorithm is $\frac{K-1}{2}$. To see this, note that the adversary can choose an
example $(R e_1, y)$, where $y$ is a label chosen uniformly from
$\cbr{1,2,\dots,K}$, and show this example $K$ times. The sequence of examples
trivially satisfies the strong linear separability condition, and the
$\frac{K-1}{2}$ expected mistake lower bound follows from ~\citep[][Claim 2]{Daniely-Helbertal-2013}. However, it is unclear if the trivial lower
bound is the best possible if $K$ is large, i.e., $\omega((R/\gamma)^2)$. We
leave this as an open problem.

Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} can
be extended to nonlinear classification using
kernels~\cite{Scholkopf-Smola-2002, Shawe-Taylor-Cristianini-2004}. The formal
description of the kernelized algorithm is given in
Algorithm~\ref{algorithm:kernelized}. As opposed to explicitly maintaining the
weight vector for each class, the algorithm maintains the set of example-scalar
pairs corresponding to the updates of the non-kernelized algorithm. As a direct
consequence of
Theorem~\ref{theorem:strongly-separable-examples-mistake-upper-bound} we get a
mistake bound for the kernelized algorithm.

\begin{theorem}[Mistake upper bound for kernelized algorithm]
\label{theorem:kernelized-upper-bound}
Let $X$ be a non-empty set, let $(V, \ip{\cdot}{\cdot})$ be an inner product
space. Let $\phi:X \to V$ be a feature map and let $k:X \times X \to \R$,
$k(x,x') = \ip{\phi(x)}{\phi(x')}$ the associated positive positive definite
kernel. Let $K$ be a positive integer, $\gamma$ be a positive real number, $R$
be a non-negative real number. If $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T) \in
X \times \{1,2,\dots,K\}$ are labeled examples such that:
\begin{enumerate}
\item the mapped examples $(\phi(x_1), y_1)$, $(\phi(x_2), y_2)$, $\dots$, $(\phi(x_T), y_T)$
are strongly linearly separable with margin $\gamma$,
\item for all $t$ in $\cbr{1,\ldots,T}$, $k(x_t,x_t) \le R^2$,
\end{enumerate}
then the expected number of mistakes Algorithm~\ref{algorithm:kernelized} makes
is at most $(K-1) \lfloor 4(R/\gamma)^2 \rfloor$.
\end{theorem}

\begin{algorithm}[h]
\caption{\textsc{Kernelized Bandit Algorithm}
\label{algorithm:kernelized}}
\begin{algorithmic}[1]
{
\REQUIRE{Number of classes $K$, number of rounds $T$.}
\REQUIRE{Kernel function  $k(\cdot, \cdot)$.}
\STATE{Initialize $J_1^{(1)} = J_2^{(1)} = \dots = J_K^{(1)} = \emptyset$}
\FOR{$t=1,2,\dots,T$}
\STATE{Observe feature vector $x_t$.}
\STATE{Compute \\ $S_t = \left\{ i ~:~ 1 \le i \le K, \ \sum_{(x,y) \in J_i^{(t)}} y k(x, x_t) \ge 0 \right\}$}
\IF{$S_t = \emptyset$}
\STATE{Predict $\widehat y_t \sim \text{Uniform}(\{1,2,\dots,K\})$}
\STATE{Observe feedback $z_t \in \{0,1\}$ \\ \qquad where $z_t = \indicator{\widehat y_t \neq y_t}$}
\IF{$z_t = 1$}
\STATE{Set $J_i^{(t+1)} = J_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$}
\ELSE
\STATE{Set $J_i^{(t+1)} = J_i^{(t)}$ \\ \qquad  for all $i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$}
\STATE{Update $J_{\widehat y_t}^{(t+1)} = J_{\widehat y_t}^{(t)} \cup \cbr{(x_t, +1)}$}
\ENDIF
\ELSE
\STATE{Predict $\widehat y_t \in S_t$ chosen arbitrarily}
\STATE{Observe feedback $z_t \in \{0,1\}$ \\ \qquad where $z_t = \indicator{\widehat y_t \neq y_t}$}
\IF{$z_t = 1$}
\STATE{Set $J_i^{(t+1)} = J_i^{(t)}$ \\ \qquad for all $i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$}
\STATE{Update $J_{\widehat y_t}^{(t+1)} = J_{\widehat y_t}^{(t)} \cup \cbr{(x_t, -1)}$}
\ELSE
\STATE{Set $J_i^{(t+1)} = J_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$}
\ENDIF
\ENDIF
\ENDFOR
}
\end{algorithmic}
\end{algorithm}
