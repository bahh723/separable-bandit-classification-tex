We thank all reviewers for their helpful comments.  We will incorporate all
comments in the final version of the paper.

-----
Reviewer #2

Regarding Appendix G, we agree that it is not related to the main story of the
paper. However, it is an important example of an "ignorant" algorithm and the
lower bound that we prove in Appendix I applies to it.

In Appendix F, Banditron's decision boundary is plotted with the version with
exploration rate of 0.02. This version explores the most, and gives the most
accurate decision boundary. We will add more explanation on this in the final
version.

-----
Reviewer #4

(1) The problem was motivated in the Banditron paper (Kakade et al. 2008). One
application area is web content optimization and web advertising.

(2) The phenomenon of finite number of mistakes is explained by Theorem 2 and
Corollary 6.

-----
Reviewer #3

The lower bound for strongly separable case (Theorem 3) implies the same lower
bound for the weakly separable case. This lower bound is actually tight, since
there exists an (inefficient) algorithm that makes at most O(K / \gamma^2)
mistakes due to Daniely and Herbertal, 2013 (see their Theorem 2).  We are not
aware of any mistake lower bound for computationally efficient algorithms under
the weakly separable assumption. We will add a discussion in the paper.

-----
Reviewer #5

i. There has been substantial amount of work on the non-separable case
as discussed in Section 2 (lines 128--137). Our paper focuses
on the remaining open case when the data is linearly separable; there
was no known computationally efficient algorithm with finite mistake bound before
our work.

ii. "Efficient" means that the run-time is polynomial in K, d, 1/\gamma, T. We will
define this explicitly in the paper.  Since J_i^{(t)} only includes the samples
on which the i-th binary perceptron makes mistakes, its size is upper bounded by t.
This implies that the algorithm runs in polynomial time.

iii. Our work indeed only improves Kakade et al. and Daniely & Helbertal's
algorithms in computational efficiency, but not in mistake bound.
 - Kakade et al.'s algorithm is computationally inefficient because it requires
enumerating over a grid of O(1/\gamma)^kd linear classifiers.
 - Daniely & Helbertal's algorithm is computationally inefficient because it involves a key step
of computing (margin-based) Littlestone dimensions of subsets of multiclass
linear classifiers, which is not known how to do efficiently.

iv. We will add error bars to the plots. The focus of this submission is
largely theoretical and the goal of the experiments is to verify that the
algorithms work as expected.

v. See the answer for Reviewer #3.
