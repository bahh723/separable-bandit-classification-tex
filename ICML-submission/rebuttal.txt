We thank all reviewers for their helpful comments.  We will incorporate all
comments in the final version of the paper.

-----
Reviewer #2

Regarding Appendix G, we agree that it is not related to the main story of the
paper. However, it is an important example of an "ignorant" algorithm and the
lower bound that we prove in Appendix I applies to it.

In Appendix F, the Banditron's boundary is plotted with the version with
exploration rate of 0.02. This version explores the most, and gives the most
accurate decision boundary. We would add more explanation on this in the final
version.

-----
Reviewer #4

(1) The problem was motivated in the Banditron paper (Kakade et al. 2008). One
application area is web content optimization and web advertising.

(2) The phenomenon of finite number of mistakes is explained by Theorem 2 and
Corollary 6.

-----
Reviewer #3

The lower bound for strongly separable case (Theorem 3) implies the same lower
bound for the weakly separable case. This lower bound is actually tight, since
there exists an (inefficient) algorithm that makes at most O(K / \gamma^2)
mistakes due to Daniely and Herbertal, 2013 (see their Theorem 2).  We are not
aware of any mistake lower bound for computationaly efficient algorithms under
the weakly separable assumption. We will add a discussion in the paper.

-----
Reviewer #5

i. There has been substanstial amount of work on the non-separable case
as discussed in Section 2 (lines 128--137). Our paper focuses
on the remaining open case when the data is linearly separable; there
was non known computationaly efficient algorithm with finite mistake bound.

ii. "Efficient" means that run-time is polynomial in K, d, 1/\gamma, T. We will
define this explicitly in the paper.  Since J_i^{(t)} only includes the samples
on which the i-th binary perceptron makes mistakes, in the separable case,
their number will be finite based on perceptron's guarantee. The expected size of
J_i^{(t)} is upper bounded by it min(t, (1/gamma')^2).

iii. Our work indeed only improves Kakade et al. and Daniely & Helbert et al.'s
algorithms in efficiency, but not in mistake bound.

Daniely and Herbertal's algorithm is inefficient because it involves a key step
of computing (margin-based) Littlestone dimensions of subsets of multiclass
linear classifiers, which is not known how to do efficiently.

iv. We will add error bars to the plots. The focus of this submission is
largely theoretical and the goal of the experiments is to verift that the
algorithms work as expected.

v. See the answer for Reviewer #3.
