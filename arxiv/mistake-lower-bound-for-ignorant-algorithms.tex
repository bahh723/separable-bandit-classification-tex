\section{Mistake lower bound for ignorant algorithms}
\label{section:mistake-lower-bound-for-ignorant-algorithms}

In this section, we prove a mistake lower bound for a family of algorithms
called \textit{ignorant algorithms}. Ignorant algorithms ignore the examples on
which they make mistakes. This assumption seems strong, but as we will explain
below, it is actually natural, and several recently proposed bandit linear
classification algorithms that achieve $\sqrt{T}$ regret bounds belong to this
family, e.g., SOBA~\citep{Beygelzimer-Orabona-Zhang-2017},
OBAMA~\citep{Foster-Kale-Luo-Mohri-Sridharan-2018}. Also,
\textsc{Nearest-Neighbor Algorithm} (Algorithm~\ref{algorithm:nearest-neighbor})
presented in Appendix~\ref{section:nearest-neighbor-algorithm} is an ignorant
algorithm.

Under the assumption that the examples lie in in the unit ball of $\R^d$ and are
weakly linearly separable with margin $\gamma$, we show that any ignorant
algorithm must make at least $\Omega \left( \left(\frac{1}{160
\gamma}\right)^{(d-2)/4} \right)$ mistakes in the worst case. In other words, an
algorithm that achieves a better mistake bound cannot ignore examples on which
it makes a mistake and it must make a meaningful update on such examples.

To formally define ignorant algorithms, we define the conditional distribution
from which an algorithm draws its predictions. Formally, given an algorithm
$\calA$ and an adversarial strategy, we define
\[
p_t(y|x) = \\
\Pr[y_t = y ~|~ (x_1, y_1), (x_2, y_2) \dots, (x_{t-1}, y_{t-1}), x_t = x] \; .
\]
In other words, in any round $t$, conditioned on the past $t-1$ rounds, the
algorithm $\calA$ chooses $y_t$ from probability distribution $p_t(\cdot|x_t)$.
Formally, $p_t$ is a function $p:\{1,2,\dots,K\} \times \R^d \to [0,1]$
such that $\sum_{y=1}^K p_t(y|x) = 1$ for any $x \in \R^d$.

\begin{definition}[Ignorant algorithm]
An algorithm $\calA$ for \textsc{Online Multiclass Linear Classification with
Bandit Feedback} is called \emph{ignorant} if for every $t=1,2,\dots,T$,
$p_t$ is determined solely by the sequence
$(x_{a_1}, y_{a_1})$,$(x_{a_2}, y_{a_2})$, $\dots$, $(x_{a_n}, y_{a_n})$
of labeled examples
from the rounds $1 \le a_1 < a_2 < \dots < a_n < t$ in which
the algorithm makes a correct prediction.
\end{definition}

An equivalent definition of an ignorant algorithm is that the memory state of
the algorithm does not change after it makes a mistake. Equivalently,
the memory state of an ignorant algorithm is completely determined
by the sequence of labeled examples on which it made correct prediction.

To explain the definition, consider an ignorant algorithm $\calA$. Suppose that
on a sequence of examples $(x_1, y_1)$, $(x_2, y_2)$, $\dots$, $(x_{t-1}, y_{t-1})$
generated by some adversary the algorthm $\calA$ makes correct predictions in
rounds $a_1, a_2, \dots, a_n$ where $1 \le a_1 < a_2 < \dots < a_n < t$ and
errors on rounds $\{1,2,\dots,t-1\} \setminus \{a_1, a_2, \dots, a_n\}$. Suppose
that on another sequence of examples $(x_1', y_1'), (x_2', y_2'), \dots,
(x_{s-1}', y_{s-1}')$ generated by another adversary the algorithm $\calA$ makes
correct predictions in rounds $b_1, b_2, \dots, b_n$ where $1 \le b_1 < b_2 <
\dots < b_n < s$ and errors on rounds $\{1,2,\dots,s-1\} \setminus \{b_1, b_2,
\dots, b_n\}$. Futhermore, suppose
\begin{align*}
(x_{a_1}, y_{a_1}) &= (x'_{b_1}, y'_{b_1}) \; , \\
(x_{a_2}, y_{a_2}) &= (x'_{b_2}, y'_{b_2}) \; , \\
\vdots \\
(x_{a_n}, y_{a_n}) &= (x'_{b_2}, y'_{b_n}) \; .
\end{align*}
Then, as $\calA$ is ignorant,
\[
\Pr[y_t = y ~|~ (x_1, y_1), (x_2, y_2) \dots, (x_{t-1}, y_{t-1}), x_t = x] =
\Pr[y_t' = y ~|~ (x_1', y_1'), (x_2', y_2') \dots, (x_{t-1}', y_{t-1}'), x_t' = x] \; .
\]
Note that the sequences $(x_1, y_1)$, $(x_2, y_2)$, $\dots$, $(x_{t-1},
y_{t-1})$ and $(x_1', y_1')$, $(x_2', y_2')$, $\dots$, $(x_{s-1}', y_{s-1}')$
might have different lengths and and $\calA$ might error in different sets of
rounds. As a special case, if an ignorant algorithm makes a mistake in round $t$
then $p_{t+1}=p_t$.

Our main result is the following lower bound on the expected number of mistakes
for ignorant algorithms.

\begin{theorem}[Mistake lower bound for ignorant algorithms]
\label{theorem:ignorant-lower-bound}
Let $\gamma \in (0,1)$ and let $d$ be a positive integer. Suppose $\calA$ is an
ignorant algorithm for \textsc{Online Multiclass Linear Classification with
Bandit Feedback}. There exists $T$ and an adversary that sequentially chooses
labeled examples $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T) \in \R^d\times
\{1,2\}$ such that the examples are strongly linearly separable with magin
$\gamma$ and $\norm{x_1}, \norm{x_2}, \dots, \norm{x_T} \le 1$, and the expected
number of mistakes made by $\calA$ is at least
$$
\frac{1}{10} \left(\frac{1}{160\gamma}\right)^{\frac{d-2}{4}} \; .
$$
\end{theorem}

Before proving the theorem, we need the following lemma.

\begin{lemma}
\label{lemma:embed_d_gamma}
Let $\gamma \in (0,\frac{1}{160})$, let $d$ be a positive integer and let $N = (\frac{1}{2\sqrt{40\gamma}})^{d-2}$.
There exist vectors $u_1, u_2, \dots, u_N$, $v_1, v_2, \dots, v_N$ in $\R^d$ such that for all $i, j \in \{1,2,\dots,N\}$,
\begin{align*}
\norm{u_i} & \le 1 \; , \\
\norm{v_j} & \le 1 \; , \\
\ip{u_i}{v_j} & \ge \gamma, \quad \text{if $i=j$,} \\
\ip{u_i}{v_j} & \le -\gamma, \quad \text{if $i \neq j$.}
\end{align*}
\end{lemma}

\begin{proof}
By Lemma 6 of~\citet{Long-1995}, there exists vectors $z_1, z_2, \dots, z_N \in
\R^{d-1}$ such that $\norm{z_1} = \norm{z_2} = \dots = \norm{z_N} = 1$ and the
angle between the vectors is $\measuredangle(z_i, z_j) \ge \sqrt{40 \gamma}$ for
$i \neq j$, $i,j \in \{1,2,\dots,N\}$. Since $\cos\theta \le 1-\theta^2/5$ for
any $\theta \in [-\pi,\pi]$, this implies that
\begin{align*}
\ip{z_i}{z_j} &= 1, \quad \text{if $i = j$,} \\
\ip{z_i}{z_j} &\le 1 - 8\gamma, \quad \text{if $i \neq j$.}
\end{align*}

Define $v_i = (\frac{1}{2} z_i, \frac{1}{2})$, and $u_i = (\frac{1}{2} z_i,
-\frac{1}{2}(1-4\gamma))$ for all $i \in \{1,2,\dots,N\}$. It can be easily
checked that for all $i$, $\norm{v_i} \le 1$ and $\norm{u_i} \le 1$.
Additionally,
$$
\ip{u_i}{v_j} = \frac{1}{4} \ip{z_i}{z_j} - \frac {1-4\gamma} 4 \; .
$$
Thus,
\begin{align*}
\ip{u_i}{v_j} &\ge \gamma, \quad \text{if $i=j$,} \\
\ip{u_i}{v_j} &\le -\gamma, \quad \text{if $i \neq j$.}
\end{align*}
\end{proof}

\begin{proof}[Proof of \autoref{theorem:ignorant-lower-bound}]
We consider the strategy for the adversary described in
Algorithm~\ref{algorithm:adversary-strategy}.

\begin{algorithm}
\caption{\textsc{Adversary's strategy}}
\label{algorithm:adversary-strategy}
\textbf{Define} $T=N$ and $v_1, v_2, \dots, v_N$ as in Lemma~\ref{lemma:embed_d_gamma}.\\
\textbf{Define} $q_0=\frac{1}{\sqrt{T}}$. \\
\textbf{Initialize} $\textsc{phase}= 1$. \\
\For{$t=1,2,\dots,T$}{
    \If{$\textsc{phase}=1$}{
       \If{$p_t(1|v_t) < 1-q_0$}{
          $(x_t, y_t)\leftarrow (v_t, 1)$
        }
       \Else{
          $(x_t, y_t)\leftarrow (v_t, 2)$ \\
          $\textsc{phase}\leftarrow 2$
       }
    }
    \Else{
         $(x_t, y_t)\leftarrow (x_{t-1}, y_{t-1})$
    }
}
\end{algorithm}

Let $\tau$ be the time step $t$ in which the adversary sets $\textsc{phase}\leftarrow 2$.
If the adversary never sets $\textsc{phase}\leftarrow 2$, we define $\tau = T + 1$.
Then,
\begin{align*}
 \Exp \left[\sum_{t=1}^T \indicator{\widehat y_t\neq y_t}\right]
 \ge \Exp\left[\sum_{t=1}^{\tau - 1} \indicator{\widehat y_t\neq y_t}\right]
+ \Exp\left[\sum_{t=\tau}^T \indicator{\widehat y_t\neq y_t}\right] \; .
\end{align*}
We upper bound each of last two terms separately.

In rounds $1,2,\dots,\tau-1$, the algorithm predicts the incorrect class $2$
with probability at least $q_0$. Thus,
\begin{equation}
\Exp\left[\sum_{t=1}^{\tau - 1} \indicator{\widehat y_t\neq y_t}\right] = q_0 \Exp[(\tau - 1)] \; .
\label{eqn:err-phase1}
\end{equation}
In rounds $\tau, \tau+1, \dots, T$, all the examples are the same and are equal
to $(v_\tau, 2)$. Let $s$ be the first time step $t$ such that $t \ge \tau$
and the algorithm makes a correct prediction. If the algorithm makes mistakes
in all rounds $\tau, \tau+1, \dots, T$, we define $s = T+1$.
By definition the algorithm makes mistakes in rounds $\tau, \tau+1, \dots, s-1$.
Therefore,
\begin{equation}
\Exp\left[\sum_{t=\tau}^T \indicator{\widehat y_t\neq y_t}\right] \ge \Exp[s-\tau].
\label{eqn:err-phase2}
\end{equation}
%Since the algorithm is ignorant, conditioned on $\tau$, $s-\tau+1$ has the same distribution as
%$\min(X, T-\tau+2)$, where $X$ is drawn from a geometric distribution with parameter
%$p_\tau(2|v_\tau)$.
Since the algorithm is ignorant, conditioned on $\tau$ and $q \triangleq p_\tau(2|v_\tau)$, $s-\tau$ follows a truncated geometric distribution with parameter $q$
(i.e., $s-\tau$ is $0$ with probability $q$, $1$ with probability $(1-q)q$, $2$ with probability $(1-q)^2q, \ldots$). Its conditional expectation can be calculated as follows:
\begin{align*}
\Exp[s-\tau~|~\tau, q]
&= \sum_{i=1}^{T+1-\tau} i \times \Pr[s-\tau = i |~\tau, q ] \\
&= \sum_{j=1}^{T+1-\tau}\Pr[s-\tau\geq j | ~\tau, q] = \sum_{j=1}^{T+1-\tau}(1-q)^j \geq  \sum_{j=1}^{T+1-\tau}(1-q_0)^j = \frac{1-q_0}{q_0}\left( 1-(1-q_0)^{T-\tau+1} \right).
\end{align*}

Therefore, by the tower property of conditional expectation,
\[ \Exp[s-\tau~|~\tau] = \Exp\left[ \Exp\left[s-\tau~\middle|~\tau, q\right]~ \middle| ~\tau\right] \geq \frac{1-q_0}{q_0}\left( 1-(1-q_0)^{T-\tau+1}\right). \]

%= \sum_{i=1}^{T+1-\tau}\sum_{j=1}^i \Pr[s-\tau=i] = \sum_{j=1}^{T+1-\tau}\sum_{i=j}^{T+1-\tau} \Pr[s-\tau=i] \\
%By Lemma~\ref{lem:truncated-geom} below,
%\[
%  \Exp\left[s-\tau+1 \mid \tau, p_\tau(2|v_\tau) \right] = F(p_\tau(2|v_\tau), T-\tau+2) \geq F(q_0, T-\tau+2).
%\]
%Therefore,
%$$
%\Exp[s-\tau ~|~ \tau]
%\ge
%F(q_0, T-\tau+2) - 1
%=
%\frac{1-q_0}{q_0}\left(1-(1-q_0)^{T-\tau+1}\right) \; .
%$$



%the random variable $s-\tau+1$
%has a truncated geometric distribution with parameter $p_\tau(2|v_\tau) \le q_0$.
%Specifically, conditioned on $\tau$,




Combining this fact with Equations~\eqref{eqn:err-phase1} and~\eqref{eqn:err-phase2}, we have that
\begin{align*}
 \Exp \left[\sum_{t=1}^T \indicator{\widehat y_t\neq y_t}\right]
& \ge q_0 \Exp[\tau - 1] + \Exp \left[\frac{1-q_0}{q_0}\left(1-(1-q_0)^{T-\tau+1}\right)\right] \\
& =  \Exp \left[  q_0 (\tau - 1) + \frac{1-q_0}{q_0}\left(1-(1-q_0)^{T-\tau+1}\right)  \right] \; .
\end{align*}

%\Exp \left[~\middle|~ \tau \right]

We lower bound the last expression by considering two cases for $\tau$.
If $\tau \ge \frac{1}{2}T + 1$, then the last expression is lower bounded by
$\frac{1}{2}q_0 T = \frac{1}{2} \sqrt{T}$. If
$\tau < \frac{1}{2}T+1$, it is lower bounded by
\begin{align*}
& \frac{1-q_0}{q_0}\left(1-(1-q_0)^{\frac{1}{2}T}\right) \\
& = \frac{1-q_0}{q_0}\left(1-(1-q_0)^{\frac{1}{2q_0^2}}\right) \\
& \ge \frac{1-\frac{1}{\sqrt{2}}}{q_0}\left(1-\frac{1}{\sqrt{e}}\right) \\
& \ge \frac{1}{10} \sqrt{T} \; .
\end{align*}

Observe that in phase 1, the labels are equal to $1$ and in phase 2 the labels
are equal to $2$. Note that $(x_\tau, y_\tau)=(x_{\tau+1}, y_{\tau+1})= \dots =
(x_T, y_T) = (v_\tau, 2)$. Consider the vectors $u_1, u_2, \dots, u_N$ as
defined in Lemma~\ref{lemma:embed_d_gamma}. We claim that $w_1=-u_\tau/2$ and
$w_2=u_\tau/2$ satisfy the conditions of strong linear separability.

Clearly $\norm{w_1}^2 + \norm{w_2}^2 \le (\norm{w_1} + \norm{w_2})^2 \le
(\frac{1}{2} + \frac{1}{2})^2 \le 1$. By Lemma~\ref{lemma:embed_d_gamma}, we
have $\ip{w_2/2}{x_t} = \ip{u_\tau/2}{v_\tau} \ge \gamma/2, \forall t \ge \tau$ and
$\ip{w_2/2}{x_t} = \ip{u_\tau/2}{v_t} \le - \gamma/2$ for all $t < \tau$. Similarly,
$\ip{w_1/2}{x_t} \le -\gamma/2$ for all $t \ge \tau$ and $\ip{w_1/2}{x_t} \ge
\gamma/2$ for all $t < \tau$. Thus, the examples are strongly linearly
separable with margin $\gamma$.
\end{proof}

%\begin{lemma}
%For $p \in (0,1)$ and $N \geq 1$, denote by $F(p, N) = \frac{1}{p}(1 - (1-p)^N)$.
%\begin{enumerate}
%  \item $F(p, N)$ is monotonically decreasing in $p$.
%  \item If $X \sim \Geom(p)$, then $\Exp[\min(X, N)] = F(p, N)$.
%\end{enumerate}
%\label{lem:truncated-geom}
%\end{lemma}
%\begin{proof}
%We show the two items respectively.
%\begin{enumerate}
%\item By calculus,
%\[ \frac{\partial F(p,n)}{\partial p} = \frac{(1-p)^{N-1} (1 + (N-1)p) - 1}{p^2}. \]
%Now, note that for any $p \in (0,1)$,
%\[ (1 + (N-1)p) \leq \frac{1}{(1 - p)^{N-1}} \]
%We therefore get $\frac{\partial F(p,n)}{\partial p} < 0$ for all $p$, showing the first item.

%\item We have the following decomposition on $\Exp[X]$:
%\[ \Exp[X] = \Exp[X \one[X \leq N]] + \Prob(X \geq N+1) \Exp[X|X \geq N+1] \]
%By the memoryless property of geometric distribution, $\Exp[X|X \geq N+1] = N + \Exp[X] = N + \frac 1 p$.
%Therefore,
%\[ \Exp[X] = \Exp[X \one[X \leq N]] + \Prob(X \geq N+1) \cdot N + \Prob(X \geq N+1) \cdot \frac 1 p. \]
%At the same time, observe that $\Exp[X \one[X \leq N]] + \Prob(X \geq N+1) \cdot N = \Exp[\min(X, N)]$.
%Therefore,
%\[ \Exp[\min(X, N)] = \frac 1 p \cdot (1 - \Prob(X \geq N+1)) = \frac{1}{p}(1 - (1-p)^N) = F(p, N). \qedhere \]
%\end{enumerate}
%\end{proof}
