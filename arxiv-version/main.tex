\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fullpage}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{natbib}


\title{Online Multiclass Linear Classification with Bandit Feedback}

\author{
Alina Beygelzimer \and
D\'avid P\'al \and
Bal\'azs Sz\"or\'enyi \and
Devanathan Thiruvenkatachari \and
Chen-Yu Wei \and
Chicheng Zhang
}

\begin{document}

\maketitle

\begin{abstract}
TODO
\end{abstract}

\section{Introduction}
\label{section:introduction}

We study the online multiclass classification problem with bandit feedback. The
problem was initially proposed by \cite{Kakade-Shalev-Shwartz-Tewari-2008}. The
corresponding problem with full information feedback is solved by the classical
\textsc{Multiclass Perceptron} algorithm and its variants (see e.g.
\cite{Crammer-Dekel-Keshet-Shalev-Shwartz-Singer-2006}). The problem is quite
natural and potential applications include online advertising and online content
optimization.

In this paper, we focus on the special case when the examples are \emph{linearly
separable with a margin}. In the full-information feedback setting, it is well
know that if the examples lie in the ball of radius $R$ centered at the origin
and is linearly separable with a margin $\gamma$ then the \textsc{Multiclass
Perceptron} algorithm makes at most $(R/\gamma)^2$ mistakes. This result is very
satisfactory since the upper bound on the number of mistakes is
information-theoretically optimal and at the same time the \textsc{Multiclass
Perceptron} algorithm has low time and memory complexity.

The bandit feedback setting, however, is much more challenging. We start by
dissecting the notion of linear separability. We define two notions of linear
separability for multiclass classification. the ``standard'' linear separability
and \emph{strong linear separability}. For binary classification, these two
definitions are identical. However, they differ for multiclass classification.
For multiclass classification with $K$ clases the standard notion of linear
separability means that the examples from each class lie in an intersection of
$K-1$ halfspaces and the examples outside the class lie in the complement of the
intersection of the halfspaces. Strong linear separability means that examples
from each class can be separated from the remaining examples by a \emph{single}
hyperplane. We will give a more precise definition
in Section~\ref{section:notions-of-linear-separability}.

We provide a simple and efficient algorithm for the bandit feedback setting if
the examples are strongly linearly separable. The algorithm runs $K$ copies a
\textsc{Binary Perceptron} algorithm.

For the standard notion of linear separability, we propose three algorithms with
various time-complexity-versus-mistake-bound trade-offs. Two of these algorithms
map the examples into a high-dimensional vector space induced by a kernel. In
this space the examples become \emph{strongly linearly separable}. This allows
us to run a kernelized version of the algorithm for the strongly linearly
separable case.

The two kernel-based algorithms differ by the choice of the kernel. The crux of
the problem is the analysis of the approximation properties of these kernels and
the way margin in original space under standard linear separability notion gets
transformed into a margin in the high-dimensional space under the strong
separability notion. This problem is related to the problem of learning
intersection of halfspaces and has been studied previously by
\cite{Klivans-Servedio-2008}.

The third algorithm is based on the obvious idea that two points that are close
enough must have the same label. Here, close enough means that they cannot be
separated from each other by a linear separator with a margin.

All three algorithms run in time that is polynomial in the dimension of the
feature vectors, the number of classes and the number of rounds. At the same
time, all three algorithms make number of mistakes that does \emph{not} depend
on the number of rounds. Instead, their number of mistakes depends only on the
margin, number of classes, dimension of the feature vectors, and the radius of
the ball in which the data lie.

Finally, we study two questions related to the computational and
information-theoretic hardness of the problem. Any algorithm for the bandit
setting collects information in the form of so called \emph{strongly labeled}
and \emph{weakly labeled} examples. Strongly-labeled examples are those for
which we know the class label. Weakly labeled example is an example for which we
know that class label can be anything except for a particular one class. First,
we show the offline problem of determining if a linear multiclass separator
exists for a set of strongly- and weakly-labeled examples is NP-complete.
Second, we show that any algorithm that uses only strongly-labeled examples and
ignores weakly labeled examples makes at least $\Omega(???)$ mistakes.


\section{Related work}
\label{section:related-work}

TODO


\section{Notions of linear separability}
\label{section:notions-of-linear-separability}

TODO


\section{Algorithm for strongly linearly separable data}

TODO


\section{Converting linear separability to strong linear separability}

TODO


\section{Nearest neighbor algorithm}

TODO


\section{NP-hardness of the weak labeling problem}

TODO


\section{Mistake lower bound for ignorant algorithms}

TODO



\bibliographystyle{plainnat}
\bibliography{biblio}

\end{document}
