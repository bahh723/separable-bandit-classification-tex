\documentclass[12pt]{article}

% LaTeX packages
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{fullpage}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{natbib}
\usepackage{thmtools}
\usepackage{tikz}

% TikZ packages
\usetikzlibrary{arrows}
\usetikzlibrary{backgrounds}
\usetikzlibrary{calc}
\usetikzlibrary{cd}  % commutative diagrams
\usetikzlibrary{intersections}
\usetikzlibrary{patterns}
\usetikzlibrary{shapes}
\usetikzlibrary{through}

% Math environments
\newtheorem{definition}{Definition}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{corollary}[definition]{Corollary}

% Math symbols and commands
\newcommand{\R}{\mathbb{R}}  % set of real numbers
\newcommand{\indicator}[1]{\mathbf{1}\left[#1 \right]} % indicator
\newcommand{\ip}[2]{\left\langle #1, #2 \right\rangle} % inner product
\newcommand{\norm}[1]{\left\| #1 \right\|}  % norm of a vector or a matrix
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\Exp}{\mathbf{E}}  % expected value


\title{Online Multiclass Linear Classification with Bandit Feedback}

\author{
Alina Beygelzimer \and
D\'avid P\'al \and
Bal\'azs Sz\"or\'enyi \and
Devanathan Thiruvenkatachari \and
Chen-Yu Wei \and
Chicheng Zhang
}

\begin{document}

\maketitle

\begin{abstract}
TODO
\end{abstract}

\section{Introduction}
\label{section:introduction}

We design new algorithms for the problem of \textsc{Online Multiclass
Classification with Bandit Feedback}. In this problem, in each round, the
algorithm receives a feature vector and the algorithm has to predict one of $K$
possible classes. Immediately after the algorithm makes its prediction, it
receives feedback whether the prediction was correct or not. However, in
contrast with the standard, so called full information setting, the algorithm
does \emph{not} receive the correct label. The feedback is only binary. To allow
for meaningful randomized strategies, we assume that the correct label in a
particular round is chosen prior to algorithm's prediction in that
round.\footnote{The reader can assume throughout the paper a simpler model of
the so called \emph{oblivious adversary} where the whole sequence of labeled
examples $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T)$ is chosen before the first
round of the game.} The protocol of the problem is stated below.

\begin{algorithm}[h]
\caption{\textsc{Online Multiclass Classification with Bandit Feedback}
\label{algorithm:game-protocol}}
\begin{algorithmic}[1]
{
\REQUIRE{Number of classes $K$. Number of rounds $T$.}
\FOR{$t=1,2,\dots,T$}
\STATE{Observe feature vector $x_t$}
\STATE{Environment \emph{secretly} chooses correct class label $y_t \in \{1,2,\dots,K\}$}
\STATE{Predict class label $\widehat y_t \in \{1,2,\dots,K\}$}
\STATE{Observe feedback $z_t \in \{0,1\}$ where $z_t = \begin{cases}
0 & \text{if $\widehat y_t = y_t$,} \\
1 & \text{if $\widehat y_t \neq y_t$.}
\end{cases}$
}
\ENDFOR
}
\end{algorithmic}
\end{algorithm}

The goal is to design an algorithm that makes small number of mistakes
$\sum_{t=1}^T z_t = \sum_{t=1}^T \indicator{\widehat y_t \neq y_t}$. The problem
was initially proposed by \cite{Kakade-Shalev-Shwartz-Tewari-2008}. The
corresponding problem with full information feedback is solved by the classical
\textsc{Multiclass Perceptron} algorithm and its variants (see e.g.
\cite{Crammer-Dekel-Keshet-Shalev-Shwartz-Singer-2006}). The problem is quite
natural and potential applications include online advertising and online content
optimization where the feedback corresponds to a user's click.

In this paper, we focus on the special case when the examples lie in $\R^d$ and
are \emph{linearly separable with a margin}. In the full-information feedback
setting, it is well know that if the examples have norm at most $R$ and are
linearly separable with a margin $\gamma$ then the \textsc{Multiclass
Perceptron} algorithm makes at most $\lfloor 2(R/\gamma)^2 \rfloor$ mistakes.
This result is very satisfactory since the upper bound on the number of mistakes
is information-theoretically optimal and at the same time the \textsc{Multiclass
Perceptron} algorithm has low time and memory complexity.

The bandit feedback setting, however, is much more challenging. To understand
the differences, we dissect the notion of linear separability in the multiclass
setting. Beside the standard notion of linear separability, we define also
\emph{strong linear separability}. For binary classification, these two
definitions are identical. However, they differ for multiclass classification.
For multiclass classification with $K$ classes the standard notion of linear
separability means that the examples from each class lie in an intersection of
$K-1$ halfspaces and the examples outside of the class lie in the complement of
the intersection of the halfspaces. Strong linear separability means that
examples from each class is separated from the remaining examples by a
\emph{single} hyperplane. We state the definitions precisely in
Section~\ref{section:notions-of-linear-separability}.

For the case when the examples are strongly linearly separable, we design a
simple and efficient algorithm with expected number of mistakes at most $(K-1)
\lfloor (R/\gamma)^2 \rfloor$. The algorithm can be viewed as running $K$ copies
of the \textsc{Binary Perceptron} algorithm, one copy for each class.
Intuitively speaking, the factor $K-1$ is the price we pay for the bandit
feedback, or more precisely, the lack of full-information feedback. We also
prove that any (possibly randomized) algorithm makes $\Omega(K (R/\gamma)^2))$
mistakes in the worst case.

For the standard notion of linear separability, we propose three algorithms with
various time-complexity-versus-mistake-bound trade-offs. Two of these algorithms
map the examples into a high-dimensional inner product space induced by a
kernel. In this space the examples become \emph{strongly} linearly separable.
This allows us to run a kernelized version of the algorithm for the strongly
linearly separable case.

The two kernel-based algorithms differ by the choice of the kernel. The crux of
the problem is the analysis of the approximation properties of these kernels and
the way margin in original space under standard linear separability notion gets
transformed into a margin in the high-dimensional space under the strong
separability notion. This problem is related to the problem of learning
intersection of halfspaces and has been studied previously by
\cite{Klivans-Servedio-2008}.

The third algorithm is based on the obvious idea that two points that are close
enough must have the same label. Here, close enough means that they cannot be
separated from each other by a linear separator with a margin.

All three algorithms run in time that is polynomial in the dimension of the
feature vectors, the number of classes and the number of rounds. At the same
time, all three algorithms make number of mistakes that does \emph{not} depend
on the number of rounds. Instead, their number of mistakes depends only on the
margin, number of classes, dimension of the feature vectors, and the radius of
the ball in which the data lie.

Finally, we study two questions related to the computational and
information-theoretic hardness of the problem. Any algorithm for the bandit
setting collects information in the form of so called \emph{strongly labeled}
and \emph{weakly labeled} examples. Strongly-labeled examples are those for
which we know the class label. Weakly labeled example is an example for which we
know that class label can be anything except for a particular one class. First,
we show the offline problem of determining if a linear multiclass separator
exists for a set of strongly- and weakly-labeled examples is NP-complete. This
result is a simple consequence of the fact that learning intersection of
halfspaces is NP-complete~\citep{Blum-Rivest-1993}. Second, we show that any
algorithm that uses only strongly-labeled examples and ignores weakly labeled
examples makes at least $\Omega(???)$ mistakes.


\section{Related work}
\label{section:related-work}

\cite{Abernethy-Rakhlin-2009}

\cite{Hazan-Kale-2011}

\cite{Foster-Krishnamurthy-2018}


TODO


\section{Notions of linear separability}
\label{section:notions-of-linear-separability}

We define two notions of linear separability for multiclass classification. The
first notion is the standard notion of linear separability used in the proof of
the mistake bound for the \textsc{Multiclass Perceptron} algorithm. The second
notion is stronger, i.e. more restrictive. However, it is more suitable for the
bandit setting, since it allows for a simple and efficient algorithm; see
Section~\ref{section:algorithm-for-strongly-linearly-separable-data}.

\begin{definition}[Linear separability]
Let $(V,\ip{\cdot}{\cdot})$ be an inner product space, let $K$ be a positive
integer, and let $\gamma$ be a positive real number. Let $(x_1, y_1), (x_2,
y_2), \dots, (x_T, y_T)$ be labeled examples in $V \times \{1,2,\dots,K\}$.

The examples are said to be \emph{linearly separable with a
margin $\gamma$} if there exist vectors $w_1, w_2, \dots, w_K \in V$ such
that
\begin{align}
\label{equation:linear-separability-1}
\sum_{i=1}^K \norm{w_i}^2 & \le 1 \; , \\
\label{equation:linear-separability-2}
\forall t \in \{1,2,\dots,T\} \quad \forall i \in \{1,2,\dots, K\} \setminus \{y_t\} \qquad \qquad \ip{x_t}{w_{y_t}} & \ge \ip{x_t}{w_i} + \gamma \; .
\end{align}

The examples are said to be \emph{strongly linearly separable with a
margin $\gamma$} if there exist vectors $w_1, w_2, \dots, w_K \in V$ such
that
\begin{align}
\label{equation:strong-linear-separability-1}
\sum_{i=1}^K \norm{w_i}^2 & \le 1 \; , \\
\label{equation:strong-linear-separability-2}
\forall t \in \{1,2,\dots,T\} \qquad \qquad \ip{x_t}{w_{y_t}} &\ge \gamma \; , \\
\label{equation:strong-linear-separability-3}
\forall t \in \{1,2,\dots,T\} \qquad \forall i \in \{1,2,\dots, K\} \setminus \{y_t\} \qquad \qquad \ip{x_t}{w_i} & \le - \gamma \; .
\end{align}
\end{definition}

The notion of linear separability with a margin is standard. It used in the
full-information setting to upper bound the number of mistakes of the
\textsc{Multiclass Perceptron} algorithm. For completeness, we state the
algorithm in Appendix~\ref{section:multiclass-perceptron-proofs}. It is a
folklore result that if a set of labeled examples is separable with a margin
$\gamma$ and the norm of the examples is bounded by $R$ then \textsc{Multiclass
Perceptron} algorithm makes at most $\left\lfloor 2(R/\gamma)^2 \right \rfloor$
mistakes. Another folklore result is that \textsc{Multiclass Perceptron} is
essentially optimal in the sense that any deterministic algorithm must make
$\left\lfloor (R/\gamma)^2 \right \rfloor$ mistakes in the worst case. For
completeness, we give proofs of both of these results in
Section~\ref{section:multiclass-perceptron-proofs}.

The notion of strong linear separability is new.

It is easy to see that if labeled examples are strongly linearly
separable with margin $\gamma$ then they are linearly separable with margin
$2\gamma$. Indeed, if $w_1, w_2, \dots, w_K \in V$
satisfy \eqref{equation:strong-linear-separability-1},
\eqref{equation:strong-linear-separability-2},
\eqref{equation:strong-linear-separability-3} then they satisfy
\eqref{equation:linear-separability-1} and
\eqref{equation:linear-separability-2} with margin $\gamma' = 2\gamma$.

Similarly, if $K=2$ and labeled examples are linearly separable with a margin
$\gamma$ then they are strongly linearly separable with margin $\gamma/2$.
Indeed, if $w_1, w_2$ satisfy \eqref{equation:linear-separability-1} and
\eqref{equation:linear-separability-2} then $w_1' = \frac{w_1 - w_2}{2}$, $w_2' =
\frac{w_2 - w_1}{2}$ and $\gamma'=\gamma/2$ satisfy
\eqref{equation:strong-linear-separability-1},
\eqref{equation:strong-linear-separability-2},
\eqref{equation:strong-linear-separability-3}. The last two conditions are easy
to see. The first condition follows from $\norm{w_i'}^2 \le (\frac{1}{2}
\norm{w_1} + \frac{1}{2} \norm{w_2})^2 \le \frac{1}{2}\norm{w_1}^2 +
\frac{1}{2}\norm{w_2}^2 \le \frac{1}{2}$ for $i=1,2$.

However, for $K \ge 3$ there exists a set of labeled examples in an inner product
space that is linearly separable with a positive margin $\gamma$ but it is not
strongly linearly separable with any positive margin $\gamma$. Such set of labeled
examples is shown in
Figure~\ref{figure:linearly-separable-examples-with-margin}.

\begin{figure}
\begin{center}
\input{figures/linearly-separable-examples-with-margin}
\end{center}
\caption[]{The figure shows labeled examples in $\R^2$. The examples belong to
$K=3$ classes colored white, gray and black respectively. Each class lies in a
$120^\circ$ wedge. In other words, each class lies in an intersection of two
halfspaces.

While the examples are linearly separable with a positive margin $\gamma$, they
are \emph{not} strongly linearly separable with any positive margin $\gamma$.
}
\label{figure:linearly-separable-examples-with-margin}
\end{figure}


\section{Algorithm for strongly linearly separable data}
\label{section:algorithm-for-strongly-linearly-separable-data}

In this section, we present an algorithm for \textsc{Online Multiclass
Classification with Bandit Feedback} and prove an upper bound on the number of
mistakes under the assumption that the examples are strongly linearly separable
with a margin. The algorithm is stated as
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} and
the mistake upper bound is stated as
\autoref{theorem:strongly-separable-example-mistake-upper-bound}.

The idea behind the algorithm is to use $K$ copies of the \textsc{Binary
Perceptron} algorithm. Each copy corresponds to a class. In each round, each
copy makes a binary decision whether the feature vector belongs their respective
class or not. If at least one copy makes a positive prediction, the algorithm
predicts accordingly. If multiple copies make positive predictions, the
algorithm chooses one of them at random. If all copies make negative
predictions, the algorithms makes a blind guess.

\begin{algorithm}[h]
\caption{\textsc{Bandit Algorithm for Strongly Linearly Separable Examples}
\label{algorithm:algorithm-for-strongly-linearly-separable-examples}}
\begin{algorithmic}[1]
{
\REQUIRE{Number of classes $K$. Number of rounds $T$. Inner product space $(V,\ip{\cdot}{\cdot})$.}
\STATE{Initialize $w_1^{(1)} = w_2^{(1)} = \dots = w_K^{(1)} = 0$}
\FOR{$t=1,2,\dots,T$}
\STATE{Observe feature vector $x_t \in V$}
\STATE{Compute $S_t = \left\{ i ~:~ 1 \le i \le K, \ \ip{w_i^{(t)}}{x_t} \ge 0 \right\}$}
\IF{$S_t = \emptyset$}
\STATE{Predict $\widehat y_t \sim \text{Uniform}(\{1,2,\dots,K\})$}
\ELSE
\STATE{Predict $\widehat y_t \in S_t$ chosen arbitrarily}
\ENDIF
\STATE{Observe feedback $z_t \in \{0,1\}$ where $z_t = \indicator{\widehat y_t \neq y_t}$}
\IF{$S_t = \emptyset$}
\IF{$z_t = 1$}
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$}
\ELSE
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$}
\STATE{Update $w_{\widehat y_t}^{(t+1)} = w_{\widehat y_t}^{(t)} + x_t$}
\ENDIF
\ELSE
\IF{$z_t = 1$}
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$}
\STATE{Update $w_{\widehat y_t}^{(t+1)} = w_{\widehat y_t}^{(t)} - x_t$}
\ELSE
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$}
\ENDIF
\ENDIF
\ENDFOR
}
\end{algorithmic}
\end{algorithm}

\begin{theorem}[Mistake upper bound]
\label{theorem:strongly-separable-example-mistake-upper-bound}
Let $(V, \ip{\cdot}{\cdot})$ be any inner product space, let $K$ be a positive
integer, $\gamma$ a positive real number, $R$ be a non-negative real number. If
$(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T)$ is a sequence of labeled examples in
$V \times \{1,2,\dots,K\}$ such that the examples are strongly separable with
margin $\gamma$ and $\norm{x_1}, \norm{x_2}, \dots, \norm{x_T} \le R$ then the
expected number of mistakes
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples}
makes is at most $(K-1) \lfloor (R/\gamma)^2 \rfloor$ mistakes.
\end{theorem}

\begin{proof}
Let $M = \sum_{t=1}^T z_t$ be the number of mistakes the algorithm makes. Let $A
= \sum_{t ~:~ S_t \neq \emptyset} z_t$ be the number of mistakes in the rounds
when $S_t \neq \emptyset$ and let $B = \sum_{t ~:~ S_t = \emptyset} z_t$ be the
number of mistakes in the rounds when $S_t = \emptyset$. Obviously, $M = A + B$.
We upper bound $A$ and $B$ separately.

Let $C$ be the number of times line 16 gets executed. Let $U$ be the number of
times line 16 and line 21 get executed. In other words, $U$ is the number of
times the $K$-tuple of vectors $(w_1^{(t)}, w_2^{(t)}, \dots, w_K^{(t)})$ gets
updated. Clearly, $U = A + C$.

The key observation is that $\Exp[B] = (K-1) \Exp[C]$ since if $S_t =
\emptyset$, there is $1/K$ probability that the algorithm guesses the correct
label and with probability $(K-1)/K$ algorithm's guess is incorrect.
Putting all the information together, we get that
\begin{align*}
\Exp[M]
& = \Exp[A] + \Exp[B] \\
& = \Exp[A] + (K-1) \Exp[C]  \\
& \le (K-1) \Exp[A + C] \\
& = (K-1) \Exp[U]  \; .
\end{align*}

To finish the proof, we need to upper bound the number of updates, $U$. We claim
that $U \le \lfloor (R/\gamma)^2 \rfloor$. The proof of this upper bound is
similar to the proof of the mistake bound for \textsc{Multiclass Perceptron}
algorithm. Let $w_1^*, w_2^*, \dots, w_K^* \in V$ be vectors that satisfy
\eqref{equation:strong-linear-separability-1},
\eqref{equation:strong-linear-separability-2} and
\eqref{equation:strong-linear-separability-3}.
The $K$-tuple $(w_1^{(t)}, w_2^{(t)}, \dots, w_K^{(t)})$
changes only if there is an update in round $t$.
We investigate how $\sum_{i=1}^K \norm{w_i^{(t)}}^2$ and
$\sum_{i=1}^K \ip{w_i^*}{w_i^{(t)}}$ change. If there is an update in round $t$,
we have
\begin{align*}
\sum_{i=1}^K \norm{w_i^{(t+1)}}^2
& = \left( \sum_{i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}} \norm{w_i^{(t)}}^2 \right) + \norm{w_{\widehat y_t}^{(t+1)}}^2 \\
& = \left( \sum_{i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}} \norm{w_i^{(t)}}^2 \right) + \norm{w_{\widehat y_t}^{(t)} + (-1)^{z_t} x_t}^2 \\
& = \left( \sum_{i=1}^K \norm{w_i^{(t)}}^2 \right) + \norm{x_t}^2 + \underbrace{(-1)^{z_t} 2 \ip{w_{\widehat y_t}^{(t)}}{x_t}}_{\le 0} \\
& \le \left( \sum_{i=1}^K \norm{w_i^{(t)}}^2 \right) + \norm{x_t}^2 \\
& \le \left( \sum_{i=1}^K \norm{w_i^{(t)}}^2 \right) + R^2 \; .
\end{align*}
Therefore, after $U$ updates,
$$
\sum_{i=1}^K \norm{w_i^{(T+1)}}^2 \le R^2 U \; .
$$
Similarly, if there is an update in round $t$, we have
\begin{align*}
\sum_{i=1}^K \ip{w_i^*}{w_i^{(t)}}
& = \left( \sum_{i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}} \ip{w_i^*}{w_i^{(t)}} \right) + \ip{w_{\widehat y_t}^*}{w_{\widehat y_t}^{(t+1)}} \\
& = \left( \sum_{i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}} \ip{w_i^*}{w_i^{(t)}} \right) + \ip{w_{\widehat y_t}^*}{w_{\widehat y_t}^{(t)} + (-1)^{z_t} x_t} \\
& = \left( \sum_{i=1}^K \ip{w_i^*}{w_i^{(t)}} \right) + (-1)^{z_t} \ip{w_{\widehat y_t}^*}{x_t} \\
& \ge \left( \sum_{i=1}^K \ip{w_i^*}{w_i^{(t)}} \right) + \gamma
\end{align*}
where the last inequality follows from \eqref{equation:strong-linear-separability-2}
and \eqref{equation:strong-linear-separability-3} and case analysis whether $z_t = 0$ or $z_t = 1$.
Thus, after $U$ updates,
$$
\sum_{i=1}^K \ip{w_i^*}{w_i^{(T+1)}} \ge \gamma U \; .
$$
Applying Cauchy-Schwartz's inequality twice and assumption
\eqref{equation:strong-linear-separability-1}, we get that
$$
\sum_{i=1}^K \ip{w_i^*}{w_i^{(T+1)}}
\le \sum_{i=1}^K \norm{w_i^*} \cdot \norm{w_i^{(T+1)}}
\le \sqrt{\sum_{i=1}^K \norm{w_i^*}^2} \sqrt{\sum_{i=1}^K \norm{w_i^{(T+1)}}^2}
\le \sqrt{\sum_{i=1}^K \norm{w_i^{(T+1)}}^2} \; .
$$
Combining the upper and lower bounds on $\sum_{i=1}^K \norm{w_i^{(T+1)}}^2$ inequalities, we get
$$
(\gamma U)^2 \le \sum_{i=1}^K \norm{w_i^{(T+1)}}^2 \le R^2 U \; .
$$
We conclude that $U \le (R/\gamma)^2$. Since $U$ is an integer, $U \le \lfloor (R/\gamma)^2 \rfloor$.
\end{proof}

\begin{theorem}[Mistake lower bound]
\label{theorem:strongly-separable-example-mistake-lower-bound}
Let $\gamma$ be a positive real number and let $R$ be a non-negative real
number. For any algorithm for the \textsc{Online Multiclass Classification with
Bandit Feedback} with $K \le \frac{1}{4}(R/\gamma)^2$ classes there exists an inner
product space $(V, \ip{\cdot}{\cdot})$, a non-negative integer $T$ and a
sequence of labeled examples $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T)$
in $V \times \{1,2,\dots,K\}$ that are strongly linearly separable with
margin $\gamma$, the norms satisfy $\norm{x_1}, \norm{x_2}, \dots, \norm{x_T}
\le R$ and the expected number of mistakes the algorithm makes is at least $\frac{K-1}{2}
\lfloor \frac{1}{16} (R/\gamma)^2 \rfloor$.
\end{theorem}

\begin{proof}
We use probabilistic method. Let $M = \lfloor \frac{1}{16} (R/\gamma)^2 \rfloor$.
Let $V = \R^{M+1}$ equipped with the standard inner product.  Let $e_1, e_2,
\dots, e_{M+1}$ be the standard orthonormal basis of $V$. We define
vectors $v_1, v_2, \dots, v_M \in V$ where $v_i = \frac{R}{\sqrt{2}}(e_i +
e_{M+1})$ for $i=1,2,\dots,M$. Let $\ell_1, \ell_2, \dots, \ell_M$ be chosen
i.i.d. uniformly at random from $\{1,2,\dots,K\}$ and independently of any
randomness used the by the algorithm. Let $T = M K$. We define examples $(x_1,
y_1), (x_2, y_2), \dots, (x_T, y_T)$ as follows:
$$
\begin{array}{lclclclcl}
(x_1, y_1) & = & (x_2, y_2) & = & \cdots & = & (x_K, y_K) & = & (v_1, \ell_1) \; , \\
(x_{K+1},y_{K+1}) & = & (x_{K+2}, y_{K+2}) & = & \cdots & = & (x_{2K}, y_{2K}) & = & (v_2, \ell_2) \; , \\
& \vdots & \\
(x_{(M-1)K+1}, y_{(M-1)K+1})  & = & (x_{(M-1)K+2}, y_{(M-1)K+2}) & = & \cdots & = & (x_{MK}, y_{MK}) & = & (v_M, \ell_M) \; .
\end{array}
$$

With probability one, norm of each example is $R$. We show that with probability
one, the examples are strongly separable with margin $\gamma$. In order to see
that consider $w_1^*, w_2^*, \dots, w_K^* \in V$ defined by
$$
w_i^* = 2 \sqrt{2} \frac{\gamma}{R} \left( \sum_{j ~:~ \ell_j = i} e_j \right) - \sqrt{2} \frac{\gamma}{R} e_{M+1} \qquad \quad \text{for $i=1,2,\dots,K$.}
$$
Since for any $i \in \{1,2,\dots,K\}$ and any $j \in \{1,2,\dots,M\}$,
$$
\ip{w_i^*}{v_j} =
\begin{cases}
\gamma & \text{if $\ell_j = i$,} \\
- \gamma & \text{if $\ell_j \neq i$,}
\end{cases}
$$
it means that $w_1^*, w_2^*, \dots, w_K^*$ satisfy
\eqref{equation:strong-linear-separability-2} and
\eqref{equation:strong-linear-separability-3}. Condition \eqref{equation:strong-linear-separability-1}
is satisfied since
$$
\sum_{i=1}^K \norm{w_i^*}^2
= 8 \frac{\gamma^2}{R^2} \sum_{j=1}^M \norm{e_j}^2 + 2 \frac{\gamma^2}{R^2} K \norm{e_{M+1}}^2
= 8 \frac{\gamma^2}{R^2} M + 2 \frac{\gamma^2}{R^2} K
\le \frac{1}{2} + \frac{1}{2}
= 1 \; .
$$

It remains to lower bound the expected number of mistakes of the algorithm. For
any $j \in \{1,2,\dots,M\}$, consider the expected number of mistakes the
algorithm makes in rounds $K(j-1) + 1, K(j-1) + 2, \dots, Kj$. In these rounds,
the algorithm is guessing the label $\ell_j$. Since $\ell_j$ is chosen uniformly
at random from the set $\{1,2,\dots,K\}$ and feedback is only binary, the
expected number of mistakes the algorithm makes in these rounds is at least
$\frac{K-1}{2}$. Altogether, the algorithm makes at least $\frac{K-1}{2} M$
mistakes in expectation.

Finally, since strong separability and norm condition hold with probability one,
there exists a particular (i.e. deterministic) sequence of examples for which
the algorithm makes at least $\frac{K-1}{2} M$ mistakes.
\end{proof}

\section{Converting linear separability to strong linear separability}
\label{section:converting-linear-separability-to-strong-linear-separability}

In this section, we show how construct a mapping $\phi$ of points in the unit
ball $B_d$ of $\R^d$ into a high dimensional inner product space that has the
property if any set of labeled examples in $B_d$ is linearly separable with a
margin $\gamma$, applying the mapping $\phi$ makes the examples \emph{strongly}
linearly separable with a margin $\gamma'$ and their norms are bounded by $R'$.
The parameters $\gamma'$ and $R'$ are functions of the old margin $\gamma$ and
the number of classes $K$, and the are specified in the theorems below.

Equipped with the mapping $\phi$, we can utilize
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} and
\autoref{theorem:strongly-separable-example-mistake-upper-bound} from the
previous section and we obtain an algorithm for linearly separable examples and
an upper bound on its number of mistakes. As a computational speed up, instead
of working with the mapped examples $\phi(x_1), \phi(x_2), \dots$ explicitly, we
can use the kernelized version of
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples}
that uses a kernel function $k(x,x') = \ip{\phi(x)}{\phi(x')}$.

We construct several different mappings $\phi$. The parameters $R'$ and
$\gamma'$ for the different mappings are essentially the same. However, one
mapping has the advantage of having a kernel $k(x,x')$ that is faster to
evaluate. Another mapping has the advantage that one does \emph{not} need to
know the initial margin $\gamma$. In other words, the same mapping works for all
possible values of $\gamma > 0$. The idea behind all the mappings is that
polynomials are universal function approximators, which is a well known fact
expressed by the classical Stone-Weierstrass theorem~\citep[page 159]{Rudin-1976}.

\subsection{Polynomials}
TODO

\subsection{Rational Kernel}
TODO

\section{Nearest neighbor algorithm}

TODO


\section{NP-hardness of the weak labeling problem}

Any algorithm for the bandit setting collects information in the form of so
called \emph{strongly labeled} and \emph{weakly labeled} examples.
Strongly-labeled examples are those for which we know the class label. Weakly
labeled example is an example for which we know that class label can be anything
except for a particular one class.

A natural strategy for each round is to find vectors $w_1, w_2, \dots, w_K$ that
linearly separate the examples seen in the previous rounds and use the vectors
to predict the label in the next round. More precisely, we want to find both the
vectors $w_1, w_2, \dots, w_K$ and label for each example consistent with its
weak and/or strong labels such that $w_1, w_2, \dots, w_K$ linearly separate the
labeled examples. We show this problem is NP-hard even for $K=3$.

Clearly, the problem is at least as hard as the decision version of the problem
where the goal is to determine if such vectors and labeling exist. We show that
this problem is NP-complete.

Formally, the input to the decision problem are positive integers $d, T, K$ and
a sequence $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T) \in \{0,1\}^d \times
\{1,2,\dots,K, \overline{1}, \overline{2}, \dots, \overline{K}\}$ of strongly
and weakly labeled examples. The numbers $1,2,\dots,K$ correspond to strong
labels and the symbols $\overline{1}, \overline{2}, \dots, \overline{K}$
correspond to weak labels. We adopt the convention that $\overline{\overline{i}}
= i$ for any positive integer $i$. The goal is to determine whether or not there
exist vectors $w_1, w_2, \dots, w_K \in \R^d$ that satisfy the following
conditions for all $t=1,2,\dots,T$,
\begin{align*}
y_t \in \{1,2,\dots,K\} \qquad & \Longrightarrow \qquad & \forall i \in \{1,2,\dots,K\} \setminus \{y_t\} \qquad \ip{w_{y_t}}{x_t} & > \ip{w_i}{x_t} \; , \\
y_t \in \{\overline{1}, \overline{2},\dots, \overline{K}\} \qquad & \Longrightarrow & \qquad \exists i \in \{1,2,\dots,K\} \qquad \ip{w_i}{x_t} & > \ip{w_{\overline{y_t}}}{x_t} \; .
\end{align*}

TODO

\section{Mistake lower bound for ignorant algorithms}

TODO

\bibliographystyle{plainnat}
\bibliography{biblio}

\appendix

\section{Multiclass Perceptron}
\label{section:multiclass-perceptron-proofs}

\textsc{Multiclass Perceptron} is an algorithm for \textsc{Online Multiclass
Classification}. Both the protocol for the problem and the algorithm are stated
below. The algorithm assumes that the feature vectors come from an inner product
space $(V, \ip{\cdot}{\cdot})$.

Two results are folklore. The first result is
\autoref{theorem:mutliclass-perceptron-mistake-upper-bound} which states that if
examples are linearly separable with margin $\gamma$ and examples have norm
at most $R$ then the algorithm makes at most $\lfloor 2 (R/\gamma)^2 \rfloor$
mistakes. The second result is
\autoref{theorem:online-multiclass-classification-mistake-lower-bound} which
states that under the same assumptions as in
\autoref{theorem:online-multiclass-classification-mistake-lower-bound}
\emph{any} deterministic algorithm for \textsc{Online Multiclass Classification}
must make at least $\lfloor (R/\gamma)^2 \rfloor$ mistakes in the worst case.

\begin{algorithm}[h]
\caption{\textsc{Online Multiclass Classification}
\label{algorithm:mutliclass-classification}}
\begin{algorithmic}[1]
{
\REQUIRE{Number of classes $K$. Number of rounds $T$.}
\FOR{$t=1,2,\dots,T$}
\STATE{Observe feature vector $x_t$}
\STATE{Predict $\widehat y_t \in \{1,2,\dots,K\}$}
\STATE{Observe $y_t \in \{1,2,\dots,K\}$}
\ENDFOR
}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{\textsc{Multiclass Perceptron}
\label{algorithm:mutliclass-perceptron}}
\begin{algorithmic}[1]
{
\REQUIRE{Number of classes $K$. Number of rounds $T$. Inner product space $(V,\ip{\cdot}{\cdot})$.}
\STATE{Initialize $w_1^{(1)} = w_2^{(1)} = \dots = w_K^{(1)} = 0$}
\FOR{$t=1,2,\dots,T$}
\STATE{Observe feature vector $x_t \in V$}
\STATE{Predict $\widehat y_t = \argmax_{i \in \{1,2,\dots,K\}} \ip{w_t^{(i)}}{x_t}$}
\STATE{Observe $y_t \in \{1,2,\dots,K\}$}
\IF{$\widehat y_t \neq y_t$}
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\} \setminus \{y_t, \widehat y_t\}$}
\STATE{Update $w_{y_t}^{(t+1)} = w_{y_t}^{(t)} + x_t$}
\STATE{Update $w_{\widehat y_t}^{(t+1)} = w_{\widehat y_t}^{(t)} - x_t$}
\ELSE
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$}
\ENDIF
\ENDFOR
}
\end{algorithmic}
\end{algorithm}

\begin{theorem}[Mistake upper bound]
\label{theorem:mutliclass-perceptron-mistake-upper-bound}
Let $(V, \ip{\cdot}{\cdot})$ be an inner product space, let $K$ be a positive
integer, let $\gamma$ be a positive real number and let $R$ be a non-negative real
number. If $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T)$ is a sequence of labeled
examples in $V \times \{1,2,\dots,K\}$ that are linearly separable with margin
$\gamma$ and $\norm{x_1}, \norm{x_2}, \dots, \norm{x_T} \le R$
then \textsc{Multiclass Perceptron} algorithm makes at most $\lfloor
2(R/\gamma)^2 \rfloor$ mistakes.
\end{theorem}

\begin{proof}
Let $M = \sum_{t=1}^T \indicator{\widehat y_t \neq y_t}$ be the number of
mistakes the algorithm makes. Since the $K$-tuple $(w_1^{(t)}, w_2^{(t)}, \dots,
w_K^{(t)})$ changes only if a mistake is made, we can upper bound $\sum_{i=1}^K
\norm{w_i^{(t)}}^2$ in terms of number of mistakes.
If a mistake happens in round $t$,
\begin{align*}
\sum_{i=1}^K \norm{w_i^{(t+1)}}^2
& = \left(\sum_{i \in \{1,2,\dots,K\} \setminus \{y_t, \widehat y_t\} } \norm{w_i^{(t)}}^2 \right) + \norm{w_{y_t}^{(t)} + x_t}^2 + \norm{w_{\widehat y_t}^{(t)} - x_t}^2 \\
& = \left(\sum_{i \in \{1,2,\dots,K\} \setminus \{y_t, \widehat y_t\} } \norm{w_i^{(t)}}^2 \right) + \norm{w_{y_t}^{(t)}}^2 + \norm{w_{\widehat y_t}^{(t)}}^2 + 2 \norm{x_t}^2 + 2 \ip{w_{y_t}^{(t)} - w_{\widehat y_t}^{(t)}}{x_t} \\
& = \left(\sum_{i=1}^K \norm{w_i^{(t)}}^2 \right) + 2 \norm{x_t}^2 + 2 \ip{w_{y_t}^{(t)} - w_{\widehat y_t}^{(t)}}{x_t} \\
& \le \left(\sum_{i=1}^K \norm{w_i^{(t)}}^2 \right) + 2 \norm{x_t}^2 \\
& \le \left(\sum_{i=1}^K \norm{w_i^{(t)}}^2 \right) + 2 R^2 \; .
\end{align*}
So each time a mistake happens, $\sum_{i=1}^K \norm{w_i^{(t)}}^2$ increases by at most $2R^2$. Thus,
$$
\sum_{i=1}^K \norm{w_i^{(T+1)}}^2 \le 2R^2 M \; .
$$
Let $w_1^*, w_2^*, \dots, w_K^* \in V$ be vectors satisfying the
\eqref{equation:linear-separability-1} and
\eqref{equation:linear-separability-2}. We lower bound $\sum_{i=1}^K \ip{w_i^*}{w_i^{(t)}}$. This quantity changes
only when a mistakes happens. If mistake happens in round $t$, we have
\begin{align*}
\sum_{i=1}^K \ip{w_i^*}{w_i^{(t+1)}}
& = \left( \sum_{i \in \{1,2,\dots,K\} \setminus \{y_t, \widehat y_t\}} \ip{w_i^*}{w_i^{(t)}} \right) + \ip{w_{y_t}^*}{w_{y_t}^{(t)} + x_t} + \ip{w_{\widehat y_t}^*}{w_{\widehat y_t}^{(t)} - x_t} \\
& = \left( \sum_{i=1}^K \ip{w_i^*}{w_i^{(t)}} \right) + \ip{w_{y_t}^* - w_{\widehat y_t}^*}{x_t} \\
& \ge  \left( \sum_{i=1}^K \ip{w_i^*}{w_i^{(t)}} \right) + \gamma \; .
\end{align*}
Thus, after $M$ mistakes,
$$
\sum_{i=1}^K \ip{w_i^*}{w_i^{(T+1)}} \ge \gamma M \; .
$$
We upper bound the left hand side by using Cauchy-Schwartz inequality twice and
the condition \eqref{equation:linear-separability-1} on $w_1^*, w_2^*, \dots,
w_K^*$. We have
$$
\sum_{i=1}^K \ip{w_i^*}{w_i^{(T+1)}}
\le \sum_{i=1}^K \norm{w_i^*} \cdot \norm{w_i^{(T+1)}}
\le \sqrt{\sum_{i=1}^K \norm{w_i^*}^2} \sqrt{\sum_{i=1}^K \norm{w_i^{(T+1)}}^2}
\le \sqrt{\sum_{i=1}^K \norm{w_i^{(T+1)}}^2} \; .
$$
Combining all inequalities, we get
$$
(\gamma M)^2 \le \sum_{i=1}^K \norm{w_i^{(T+1)}}^2 \le 2R^2 M \; .
$$
We conclude that $M \le 2(R/\gamma)^2$. Since $M$ is an integer, $M \le \lfloor 2(R/\gamma)^2 \rfloor$.
\end{proof}


\begin{theorem}[Mistake lower bound]
\label{theorem:online-multiclass-classification-mistake-lower-bound}
Let $K$ be a positive integer, let $\gamma$ be a positive real number and let
$R$ be a non-negative real number. For any deterministic algorithm for the
\textsc{Online Multiclass Classification} problem there exists an
inner product space $(V, \ip{\cdot}{\cdot})$, a non-negative integer $T$ and a
sequence of labeled examples $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T)$
examples in $V \times \{1,2,\dots,K\}$ that are linearly separable with margin
$\gamma$, the norms satisfy $\norm{x_1}, \norm{x_2}, \dots, \norm{x_T} \le R$
and the algorithm makes at least $\lfloor (R/\gamma)^2 \rfloor$ mistakes.
\end{theorem}

\begin{proof}
Let $T = \lfloor (R/\gamma)^2 \rfloor$ and let $x_1, x_2, \dots, x_T$ be the
orthogonal vectors such that $\norm{x_t} = R$ for all $t=1,2,\dots,T$. For
example, we can take $V = \R^T$ and $x_t = R e_t$ where $e_t$ is $t$-th element
of the standard orthonormal basis of $\R^T$.

Since the algorithm is deterministic, we can construct the sequence of labels
$y_1, y_2, \dots, y_T$ adaptively based on the predictions $\widehat y_1,
\widehat y_2, \dots, \widehat y_T$ of the algorithm. We define $y_t$ to be any
element of $\{1,2,\dots,K\}$ not equal to $\widehat y_t$. This way the algorithm
makes a mistake in every round $t=1,2,\dots,T$.

It remains to show that the examples we have constructed are linearly separable
with margin $\gamma$. To prove that we demonstrate vectors $w_1, w_2, \dots, w_K$
satisfying conditions \eqref{equation:linear-separability-1} and
\eqref{equation:linear-separability-2}. We define
$$
w_i = \frac{1}{R\sqrt{T}} \sum_{\substack{t : 1 \le t \le T \\ y_t = i}} x_t \qquad \qquad \text{for $i=1,2,\dots,K$.}
$$
Let $a_i = |\{ t ~:~ 1 \le t \le T, \ y_t = i \}|$ be the number of occurrences of label $i$.
It is easy to see that
$$
\norm{w_i}^2 = \frac{1}{R^2 T} \sum_{\substack{t : 1 \le t \le T \\ y_t = i}} \norm{x_t}^2 = \frac{a_i}{T} \qquad \qquad \text{for $i=1,2,\dots,K$.}
$$
Since $\sum_{i=1}^K a_i = T$, the condition
\eqref{equation:linear-separability-1} holds. To verify condition
\eqref{equation:linear-separability-2} consider any labeled example $(x_t,
y_t)$. Then, by definition of $w_{y_t}$ we have
$$
\ip{w_{y_t}}{x_t}
= \frac{1}{R\sqrt{T}} \sum_{\substack{s : 1 \le s \le T \\ y_s = y_t}} \ip{x_s}{x_t}
= \frac{1}{R\sqrt{T}} \norm{x_t}^2
= \frac{R}{\sqrt{T}}
\ge \frac{R}{R/\gamma}
= \gamma \; .
$$
and for any $i \in \{1,2,\dots,K\} \setminus \{y_t\}$ we have
$\ip{w_i}{x_t} = 0$. Therefore, condition \eqref{equation:linear-separability-2} holds.
\end{proof}

\end{document}
