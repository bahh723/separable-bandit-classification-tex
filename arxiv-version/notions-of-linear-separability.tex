\section{Notions of linear separability}
\label{section:notions-of-linear-separability}

We define two notions of linear separability for multiclass classification. The
first notion is the standard notion of linear separability used in the proof of
the mistake bound for the \textsc{Multiclass Perceptron} algorithm. The second
notion is stronger, i.e. more restrictive. However, it is more suitable for the
bandit setting, since it allows for a simple and efficient algorithm; see
Section~\ref{section:algorithm-for-strongly-linearly-separable-data}.

\begin{definition}[Linear separability]
Let $(V,\ip{\cdot}{\cdot})$ be an inner product space, let $K$ be a positive
integer, and let $\gamma$ be a positive real number. Let $(x_1, y_1), (x_2,
y_2), \dots, (x_T, y_T)$ be labeled examples in $V \times \{1,2,\dots,K\}$.

The examples are said to be \emph{linearly separable with a
margin $\gamma$} if there exist vectors $w_1, w_2, \dots, w_K \in V$ such
that
\begin{align}
\label{equation:linear-separability-1}
\sum_{i=1}^K \norm{w_i}^2 & \le 1 \; , \\
\label{equation:linear-separability-2}
\forall t \in \{1,2,\dots,T\} \quad \forall i \in \{1,2,\dots, K\} \setminus \{y_t\} \qquad \qquad \ip{x_t}{w_{y_t}} & \ge \ip{x_t}{w_i} + \gamma \; .
\end{align}

The examples are said to be \emph{strongly linearly separable with a
margin $\gamma$} if there exist vectors $w_1, w_2, \dots, w_K \in V$ such
that
\begin{align}
\label{equation:strong-linear-separability-1}
\sum_{i=1}^K \norm{w_i}^2 & \le 1 \; , \\
\label{equation:strong-linear-separability-2}
\forall t \in \{1,2,\dots,T\} \qquad \qquad \ip{x_t}{w_{y_t}} &\ge \gamma \; , \\
\label{equation:strong-linear-separability-3}
\forall t \in \{1,2,\dots,T\} \qquad \forall i \in \{1,2,\dots, K\} \setminus \{y_t\} \qquad \qquad \ip{x_t}{w_i} & \le - \gamma \; .
\end{align}
\end{definition}

The notion of linear separability with a margin is standard. It used in the
full-information setting to upper bound the number of mistakes of the
\textsc{Multiclass Perceptron} algorithm. For completeness, we state the
algorithm in Appendix~\ref{section:multiclass-perceptron-proofs}. It is a
folklore result that if a set of labeled examples is separable with a margin
$\gamma$ and the norm of the examples is bounded by $R$ then \textsc{Multiclass
Perceptron} algorithm makes at most $\left\lfloor 2(R/\gamma)^2 \right \rfloor$
mistakes. Another folklore result is that \textsc{Multiclass Perceptron} is
essentially optimal in the sense that any deterministic algorithm must make
$\left\lfloor (R/\gamma)^2 \right \rfloor$ mistakes in the worst case. For
completeness, we give proofs of both of these results in
Appendix~\ref{section:multiclass-perceptron-proofs}.

The notion of strong linear separability is new.

It is easy to see that if labeled examples are strongly linearly
separable with margin $\gamma$ then they are linearly separable with margin
$2\gamma$. Indeed, if $w_1, w_2, \dots, w_K \in V$
satisfy \eqref{equation:strong-linear-separability-1},
\eqref{equation:strong-linear-separability-2},
\eqref{equation:strong-linear-separability-3} then they satisfy
\eqref{equation:linear-separability-1} and
\eqref{equation:linear-separability-2} with margin $\gamma' = 2\gamma$.

Similarly, if $K=2$ and labeled examples are linearly separable with a margin
$\gamma$ then they are strongly linearly separable with margin $\gamma/2$.
Indeed, if $w_1, w_2$ satisfy \eqref{equation:linear-separability-1} and
\eqref{equation:linear-separability-2} then $w_1' = \frac{w_1 - w_2}{2}$, $w_2' =
\frac{w_2 - w_1}{2}$ and $\gamma'=\gamma/2$ satisfy
\eqref{equation:strong-linear-separability-1},
\eqref{equation:strong-linear-separability-2},
\eqref{equation:strong-linear-separability-3}. The last two conditions are easy
to see. The first condition follows from $\norm{w_i'}^2 \le (\frac{1}{2}
\norm{w_1} + \frac{1}{2} \norm{w_2})^2 \le \frac{1}{2}\norm{w_1}^2 +
\frac{1}{2}\norm{w_2}^2 \le \frac{1}{2}$ for $i=1,2$.

However, for any $K \ge 3$ and any inner product space of dimension at least
$2$, there exists a set of labeled examples that is linearly separable with a
positive margin $\gamma$ but it is not strongly linearly separable with any
positive margin $\gamma$. An example of such set of labeled examples is shown in
Figure~\ref{figure:linearly-separable-examples-with-margin}.

\begin{figure}
\begin{center}
\input{figures/linearly-separable-examples-with-margin}
\end{center}
\caption[]{The figure shows labeled examples in $\R^2$. The examples belong to
$K=3$ classes colored white, gray and black respectively. Each class lies in a
$120^\circ$ wedge. In other words, each class lies in an intersection of two
halfspaces.

While the examples are linearly separable with a positive margin $\gamma$, they
are \emph{not} strongly linearly separable with any positive margin $\gamma$.
}
\label{figure:linearly-separable-examples-with-margin}
\end{figure}
