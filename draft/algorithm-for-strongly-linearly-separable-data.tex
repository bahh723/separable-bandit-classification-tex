\section{Algorithm for strongly linearly separable data}
\label{section:algorithm-for-strongly-linearly-separable-data}

In this section, we present an algorithm for \textsc{Online Multiclass
Classification with Bandit Feedback} and prove an upper bound on the number of
mistakes under the assumption that the examples are strongly linearly separable
with a margin. The algorithm is stated as
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} and
the mistake upper bound is stated as
\autoref{theorem:strongly-separable-example-mistake-upper-bound}.

The idea behind the algorithm is to use $K$ copies of the \textsc{Binary
Perceptron} algorithm. Each copy corresponds to a class. In each round, each
copy makes a binary decision whether the feature vector belongs its respective
class or not. If at least one copy makes a positive prediction, the algorithm
predicts accordingly. If multiple copies make positive predictions, the
algorithm chooses one of them arbitrarily. If all copies make negative
predictions, the algorithms makes a uniformly random guess.

\begin{algorithm}[h]
\caption{\textsc{Bandit Algorithm for Strongly Linearly Separable Examples}
\label{algorithm:algorithm-for-strongly-linearly-separable-examples}}
\begin{algorithmic}[1]
{
\REQUIRE{Number of classes $K$. Number of rounds $T$. Inner product space $(V,\ip{\cdot}{\cdot})$.}
\STATE{Initialize $w_1^{(1)} = w_2^{(1)} = \dots = w_K^{(1)} = 0$}
\FOR{$t=1,2,\dots,T$}
\STATE{Observe feature vector $x_t \in V$}
\STATE{Compute $S_t = \left\{ i ~:~ 1 \le i \le K, \ \ip{w_i^{(t)}}{x_t} \ge 0 \right\}$}
\IF{$S_t = \emptyset$}
\STATE{Predict $\widehat y_t \sim \text{Uniform}(\{1,2,\dots,K\})$}
\STATE{Observe feedback $z_t \in \{0,1\}$ where $z_t = \indicator{\widehat y_t \neq y_t}$}
\IF{$z_t = 1$}
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$}
\ELSE
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$}
\STATE{Update $w_{\widehat y_t}^{(t+1)} = w_{\widehat y_t}^{(t)} + x_t$}
\ENDIF
\ELSE
\STATE{Predict $\widehat y_t \in S_t$ chosen arbitrarily}
\STATE{Observe feedback $z_t \in \{0,1\}$ where $z_t = \indicator{\widehat y_t \neq y_t}$}
\IF{$z_t = 1$}
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}$}
\STATE{Update $w_{\widehat y_t}^{(t+1)} = w_{\widehat y_t}^{(t)} - x_t$}
\ELSE
\STATE{Set $w_i^{(t+1)} = w_i^{(t)}$ for all $i \in \{1,2,\dots,K\}$}
\ENDIF
\ENDIF
\ENDFOR
}
\end{algorithmic}
\end{algorithm}

\begin{theorem}[Mistake upper bound]
\label{theorem:strongly-separable-example-mistake-upper-bound}
Let $(V, \ip{\cdot}{\cdot})$ be any inner product space, let $K$ be a positive
integer, let $\gamma$ a positive real number, let $R$ be a non-negative real number. If
$(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T)$ is a sequence of labeled examples in
$V \times \{1,2,\dots,K\}$ such that the examples are strongly separable with
margin $\gamma$ and $\norm{x_1}, \norm{x_2}, \dots, \norm{x_T} \le R$ then the
expected number of mistakes
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples}
makes is at most $(K-1) \lfloor (R/\gamma)^2 \rfloor$.
\end{theorem}

\begin{proof}
Let $M = \sum_{t=1}^T z_t$ be the number of mistakes the algorithm makes. Let $A
= \sum_{t ~:~ S_t \neq \emptyset} z_t$ be the number of mistakes in the rounds
when $S_t \neq \emptyset$ and let $B = \sum_{t ~:~ S_t = \emptyset} z_t$ be the
number of mistakes in the rounds when $S_t = \emptyset$. Obviously, $M = A + B$.
We upper bound $A$ and $B$ separately.

Let $C$ be the number of times line 12 gets executed. Let $U$ be the number of
times line 12 and line 19 get executed. In other words, $U$ is the number of
times the $K$-tuple of vectors $(w_1^{(t)}, w_2^{(t)}, \dots, w_K^{(t)})$ gets
updated. Clearly, $U = A + C$.

The key observation is that $\Exp[B] = (K-1) \Exp[C]$. The equality holds
because if $S_t = \emptyset$, there is $1/K$ probability that the algorithm
guesses the correct label and with probability $(K-1)/K$ algorithm's guess is
incorrect. Putting all the information together, we get that
\begin{align*}
\Exp[M]
& = \Exp[A] + \Exp[B] \\
& = \Exp[A] + (K-1) \Exp[C]  \\
& \le (K-1) \Exp[A + C] \\
& = (K-1) \Exp[U]  \; .
\end{align*}

To finish the proof, we need to upper bound the number of updates, $U$. We claim
that $U \le \lfloor (R/\gamma)^2 \rfloor$. The proof of this upper bound is
similar to the proof of the mistake bound for \textsc{Multiclass Perceptron}
algorithm. Let $w_1^*, w_2^*, \dots, w_K^* \in V$ be vectors that satisfy
\eqref{equation:strong-linear-separability-1},
\eqref{equation:strong-linear-separability-2} and
\eqref{equation:strong-linear-separability-3}.
The $K$-tuple $(w_1^{(t)}, w_2^{(t)}, \dots, w_K^{(t)})$
changes only if there is an update in round $t$.
We investigate how $\sum_{i=1}^K \norm{w_i^{(t)}}^2$ and
$\sum_{i=1}^K \ip{w_i^*}{w_i^{(t)}}$ change. If there is an update in round $t$,
we have
\begingroup
\allowdisplaybreaks
\begin{align*}
\sum_{i=1}^K \norm{w_i^{(t+1)}}^2
& = \left( \sum_{i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}} \norm{w_i^{(t)}}^2 \right) + \norm{w_{\widehat y_t}^{(t+1)}}^2 \\
& = \left( \sum_{i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}} \norm{w_i^{(t)}}^2 \right) + \norm{w_{\widehat y_t}^{(t)} + (-1)^{z_t} x_t}^2 \\
& = \left( \sum_{i=1}^K \norm{w_i^{(t)}}^2 \right) + \norm{x_t}^2 + \underbrace{(-1)^{z_t} 2 \ip{w_{\widehat y_t}^{(t)}}{x_t}}_{\le 0} \\
& \le \left( \sum_{i=1}^K \norm{w_i^{(t)}}^2 \right) + \norm{x_t}^2 \\
& \le \left( \sum_{i=1}^K \norm{w_i^{(t)}}^2 \right) + R^2 \; .
\end{align*}
\endgroup
Therefore, after $U$ updates,
$$
\sum_{i=1}^K \norm{w_i^{(T+1)}}^2 \le R^2 U \; .
$$
Similarly, if there is an update in round $t$, we have
\begin{align*}
\sum_{i=1}^K \ip{w_i^*}{w_i^{(t)}}
& = \left( \sum_{i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}} \ip{w_i^*}{w_i^{(t)}} \right) + \ip{w_{\widehat y_t}^*}{w_{\widehat y_t}^{(t+1)}} \\
& = \left( \sum_{i \in \{1,2,\dots,K\} \setminus \{\widehat y_t\}} \ip{w_i^*}{w_i^{(t)}} \right) + \ip{w_{\widehat y_t}^*}{w_{\widehat y_t}^{(t)} + (-1)^{z_t} x_t} \\
& = \left( \sum_{i=1}^K \ip{w_i^*}{w_i^{(t)}} \right) + (-1)^{z_t} \ip{w_{\widehat y_t}^*}{x_t} \\
& \ge \left( \sum_{i=1}^K \ip{w_i^*}{w_i^{(t)}} \right) + \gamma
\end{align*}
where the last inequality follows from \eqref{equation:strong-linear-separability-2}
and \eqref{equation:strong-linear-separability-3} and analysis of the cases $z_t = 0$ and $z_t = 1$.
Thus, after $U$ updates,
$$
\sum_{i=1}^K \ip{w_i^*}{w_i^{(T+1)}} \ge \gamma U \; .
$$
Applying Cauchy-Schwartz's inequality twice and assumption
\eqref{equation:strong-linear-separability-1}, we get that
$$
\sum_{i=1}^K \ip{w_i^*}{w_i^{(T+1)}}
\le \sum_{i=1}^K \norm{w_i^*} \cdot \norm{w_i^{(T+1)}}
\le \sqrt{\sum_{i=1}^K \norm{w_i^*}^2} \sqrt{\sum_{i=1}^K \norm{w_i^{(T+1)}}^2}
\le \sqrt{\sum_{i=1}^K \norm{w_i^{(T+1)}}^2} \; .
$$
Combining the upper and lower bounds on $\sum_{i=1}^K \norm{w_i^{(T+1)}}^2$ inequalities, we get
$$
(\gamma U)^2 \le \sum_{i=1}^K \norm{w_i^{(T+1)}}^2 \le R^2 U \; .
$$
We conclude that $U \le (R/\gamma)^2$. Since $U$ is an integer, $U \le \lfloor (R/\gamma)^2 \rfloor$.
\end{proof}

The upper bound $(K-1) \lfloor (R/\gamma)^2 \rfloor$ on the expected number of
mistakes of
Algorithm~\ref{algorithm:algorithm-for-strongly-linearly-separable-examples} is
optimal to within constant factor as long as the number of classes $K$ is
at most $O((R/\gamma)^2)$. This is proved in
\autoref{theorem:strongly-separable-example-mistake-lower-bound} below.

Irrespective of any conditions on $K$, $R$, and $\gamma$, a trivial lower bound
on the expected number of mistakes of any randomized algorithm is
$\frac{K-1}{2}$. (Adversary chooses the same example over and over.) However, it
is unclear if the trivial lower bound is the best possible if $K$ is large,
i.e., $\omega((R/\gamma)^2)$. We leave this as an open problem.

\begin{theorem}[Mistake lower bound]
\label{theorem:strongly-separable-example-mistake-lower-bound}
Let $\gamma$ be a positive real number and let $R$ be a non-negative real
number. For any (possibly randomized) algorithm for the \textsc{Online
Multiclass Classification with Bandit Feedback} with $K \le
\frac{1}{4}(R/\gamma)^2$ classes there exists an inner product space $(V,
\ip{\cdot}{\cdot})$, a non-negative integer $T$ and a sequence of labeled
examples $(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T)$ in $V \times
\{1,2,\dots,K\}$ that are strongly linearly separable with margin $\gamma$, the
norms satisfy $\norm{x_1}, \norm{x_2}, \dots, \norm{x_T} \le R$ and the expected
number of mistakes the algorithm makes is at least $\frac{K-1}{2} \left\lfloor
\frac{1}{16} (R/\gamma)^2 \right\rfloor$.
\end{theorem}

\begin{proof}
We use probabilistic method. Let $M = \left\lfloor \frac{1}{16} (R/\gamma)^2 \right\rfloor$.
Let $V = \R^{M+1}$ equipped with the standard inner product.  Let $e_1, e_2,
\dots, e_{M+1}$ be the standard orthonormal basis of $V$. We define
vectors $v_1, v_2, \dots, v_M \in V$ where $v_i = \frac{R}{\sqrt{2}}(e_i +
e_{M+1})$ for $i=1,2,\dots,M$. Let $\ell_1, \ell_2, \dots, \ell_M$ be chosen
i.i.d. uniformly at random from $\{1,2,\dots,K\}$ and independently of any
randomness used the by the algorithm. Let $T = M K$. We define examples $(x_1,
y_1), (x_2, y_2), \dots, (x_T, y_T)$ as follows:
$$
\begin{array}{lclclclcl}
(x_1, y_1) & = & (x_2, y_2) & = & \cdots & = & (x_K, y_K) & = & (v_1, \ell_1) \; , \\
(x_{K+1},y_{K+1}) & = & (x_{K+2}, y_{K+2}) & = & \cdots & = & (x_{2K}, y_{2K}) & = & (v_2, \ell_2) \; , \\
& \vdots & \\
(x_{(M-1)K+1}, y_{(M-1)K+1})  & = & (x_{(M-1)K+2}, y_{(M-1)K+2}) & = & \cdots & = & (x_{MK}, y_{MK}) & = & (v_M, \ell_M) \; .
\end{array}
$$

With probability one, norm of each example is $R$. We show that with probability
one, the examples are strongly separable with margin $\gamma$. In order to see
that consider $w_1^*, w_2^*, \dots, w_K^* \in V$ defined by
$$
w_i^* = 2 \sqrt{2} \frac{\gamma}{R} \left( \sum_{j ~:~ \ell_j = i} e_j \right) - \sqrt{2} \frac{\gamma}{R} e_{M+1} \qquad \quad \text{for $i=1,2,\dots,K$.}
$$
Since for any $i \in \{1,2,\dots,K\}$ and any $j \in \{1,2,\dots,M\}$,
$$
\ip{w_i^*}{v_j} =
\begin{cases}
\gamma & \text{if $\ell_j = i$,} \\
- \gamma & \text{if $\ell_j \neq i$,}
\end{cases}
$$
it means that $w_1^*, w_2^*, \dots, w_K^*$ satisfy
\eqref{equation:strong-linear-separability-2} and
\eqref{equation:strong-linear-separability-3}. Condition \eqref{equation:strong-linear-separability-1}
is satisfied since
$$
\sum_{i=1}^K \norm{w_i^*}^2
= 8 \frac{\gamma^2}{R^2} \sum_{j=1}^M \norm{e_j}^2 + 2 \frac{\gamma^2}{R^2} K \norm{e_{M+1}}^2
= 8 \frac{\gamma^2}{R^2} M + 2 \frac{\gamma^2}{R^2} K
\le \frac{1}{2} + \frac{1}{2}
= 1 \; .
$$

It remains to lower bound the expected number of mistakes of the algorithm. For
any $j \in \{1,2,\dots,M\}$, consider the expected number of mistakes the
algorithm makes in rounds $K(j-1) + 1, K(j-1) + 2, \dots, Kj$. In these rounds,
the algorithm is guessing the label $\ell_j$. Since $\ell_j$ is chosen uniformly
at random from the set $\{1,2,\dots,K\}$ and feedback is only binary, the
expected number of mistakes the algorithm makes in these rounds is at least
$\frac{K-1}{2}$. Altogether, the algorithm makes at least $\frac{K-1}{2} M$
mistakes in expectation.

\chicheng{I think this part need to be formalized. Can we assume without loss of generality
that a deterministic prediction algorithm will have a smaller expected cost?
If so, define $F(k)$ be the number of mistake that will be made by the algorithm,
if the adversary commits to a label uniformly at random from $\cbr{1,2,\ldots,k}$,
and the guessing of the label lasts for $k$ rounds.
we can construct a recurrence of $F$: $F(k) = F(k-1) + \frac{k-1}{k}$, and $F(1) = 0$.
Therefore, $F(K) = \Omega(K)$.}

Finally, since strong separability and norm condition hold with probability one,
there exists a particular (i.e. deterministic) sequence of examples for which
the algorithm makes at least $\frac{K-1}{2} M$ mistakes in expectation
over its internal randomization.
\end{proof}
