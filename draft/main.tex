\documentclass[12pt]{article}

% LaTeX packages
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[singlelinecheck=false]{caption}
\usepackage{fullpage}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{natbib}
\usepackage{thmtools}
\usepackage{tikz}
\usepackage{color}
\usepackage{comment}
\usepackage{commath}
\usepackage{dsfont}
\usepackage{framed}

% TikZ packages
\usetikzlibrary{arrows}
\usetikzlibrary{angles}
\usetikzlibrary{backgrounds}
\usetikzlibrary{calc}
\usetikzlibrary{cd}  % commutative diagrams
\usetikzlibrary{intersections}
\usetikzlibrary{patterns}
\usetikzlibrary{shapes}
\usetikzlibrary{through}

% Math environments
\newtheorem{definition}{Definition}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{corollary}[definition]{Corollary}

% Math symbols and commands
\newcommand{\calF}{\mathcal{F}}
\newcommand{\one}{\mathds{1}}
\newcommand{\R}{\mathbb{R}}  % set of real numbers
\newcommand{\indicator}[1]{\mathbf{1}\left[#1 \right]} % indicator
\newcommand{\ip}[2]{\left\langle #1, #2 \right\rangle} % inner product
\newcommand{\e}{\mathbf{e}}
\newcommand{\zero}{\mathbf{0}}
%\newcommand{\norm}[1]{\left\| #1 \right\|}  % norm of a vector or a matrix
%\DeclareMathOperator*{\deg}{deg}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\Exp}{\mathbf{E}}  % expected value
\DeclareMathOperator{\diff}{d \!} % differential
\DeclareMathOperator*{\polylog}{polylog}


\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{darkred}{rgb}{0.7,0,0}
\definecolor{teal}{rgb}{0.3,0.8,0.8}
\definecolor{orange}{rgb}{1.0,0.5,0.0}
\definecolor{purple}{rgb}{0.8,0.0,0.8}
\definecolor{blue}{rgb}{0.0,0.0,1.0}
\newcommand{\kibitz}[2]{{\textcolor{#1}{\textsf{\footnotesize #2}}}}
\newcommand{\alina}[1]{\kibitz{darkred}{[AB: #1]}}
\newcommand{\david}[1]{\kibitz{darkgreen}{[DP: #1]}}
\newcommand{\balazs}[1]{\kibitz{purple}{[BS: #1]}}
\newcommand{\deva}[1]{\kibitz{teal}{[DT: #1]}}
\newcommand{\chenyu}[1]{\kibitz{orange}{[CW: #1]}}
\newcommand{\chicheng}[1]{\kibitz{blue}{[CZ: #1]}}


\title{Online Multiclass Linear Classification with Bandit Feedback}

\author{
Alina Beygelzimer \and
D\'avid P\'al \and
Bal\'azs Sz\"or\'enyi \and
Devanathan Thiruvenkatachari \and
Chen-Yu Wei \and
Chicheng Zhang
}

\begin{document}

\maketitle

\begin{abstract}
We study the problem of efficient online multiclass linear classification with
bandit feedback, where all examples belong to one of $K$ classes and lie in the
$d$-dimensional Euclidean space. Previous works have left open the challenge of
designing efficient algorithms with finite mistake bounds when the data is
linearly separable by a margin $\gamma$. In this work, we take a first step
towards this problem. We consider two notions of linear separability: strong
linear separability and weak linear separability.

\begin{enumerate}
\item Under the strong linear separability condition, we design an efficient
algorithm that achieves a near-optimal mistake bound of
$O\left(\frac{K}{\gamma^2} \right)$.

\item Under the more challenging weak linear separability condition, we design
an efficient algorithm with a mistake bound of $2^{\widetilde{O}(\min(K \log^2
\frac{1}{\gamma}, \log K \sqrt{\frac 1 \gamma}))}$ \footnote{We use the notation
$\widetilde{O}(f(\cdot)) = O(f(\cdot) \polylog(f(\cdot)))$.}. Our algorithm is
based on a infinite-dimensional feature mapping that transforms weak linear
separable examples to strong linear separable examples, which in turn is
inspired by \cite{Klivans-Servedio-2008}'s work on learning intersection of
halfspaces.
\end{enumerate}
\end{abstract}

\input{introduction}
\input{related-work}
\input{notions-of-linear-separability}
\input{algorithm-for-strongly-linearly-separable-data}
\input{from-linear-separability-to-strong-linear-separability}
\input{proofs-of-polynomial-approximation-theorems}
\input{nearest-neighbor-algorithm}
\input{np-hardness-of-the-weak-labeling-problem}
\input{mistake-lower-bound-for-ignorant-algorithms}

\bibliographystyle{plainnat}
\bibliography{biblio}

\appendix

\input{appendix-multiclass-perceptron}

\end{document}
